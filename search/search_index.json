{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to ChemGraphBuilder","text":"<p>The <code>chemgraphbuilder</code> package is a specialized tool designed for researchers in bioinformatics, cheminformatics, and computational biology. It enables the construction of detailed knowledge graphs that map the complex interactions between chemical compounds, genes, proteins, and bioassays. These knowledge graphs are critical for visualizing and understanding the multifaceted relationships in biochemical networks.</p> <p>At its core, <code>chemgraphbuilder</code> integrates data from PubChem and utilize Neo4j, creating nodes that represent key entities such as compounds, genes, proteins, and bioassays. It further enriches these graphs by incorporating various relationships, including:</p> <ul> <li>BioAssay-Compound Relationships: Capturing the interactions where bioassays evaluate the effects of specific compounds. This is crucial for understanding the efficacy and mechanism of action of pharmaceuticals.</li> <li>BioAssay-Gene Relationships: Documenting instances where bioassays study specific genes, providing valuable data on gene function and regulation.</li> <li>Gene-Protein Relationships: Mapping the connections where genes encode proteins, providing insights into genetic regulation and protein function.</li> <li>Compound-Gene Interactions: Detailing how compounds interact with genes, categorized into various interaction types such as Inhibitors, Inactivators, Activators, Inducers, Substrates, Ligands, and Inactive compounds. This classification aids in understanding gene regulation, drug mechanisms, and potential therapeutic applications.</li> <li>Compound Similarities: Highlighting chemical similarities between compounds, which can suggest similar biological activities or shared molecular properties.</li> <li>Compound Co-occurrence: Identifying instances where compounds or genes co-occur in scientific literature, indicating potential interactions or co-regulation.</li> </ul> <p>This comprehensive approach allows researchers to explore and analyze data across multiple levels of biological organization, from molecular interactions to broader biological pathways. The insights gained from these knowledge graphs can drive innovation in drug discovery, toxicology, and personalized medicine by revealing new drug targets, understanding adverse drug reactions, and exploring the molecular basis of diseases.</p> <p>The <code>chemgraphbuilder</code> package is versatile and can be utilized both in Python code and via the command line interface. For practical examples and use cases, please refer to the \"Usage Examples\" section available in the menu on the left sidebar. For comprehensive documentation of the main classes and their functionalities, visit the Documentation Page.</p>"},{"location":"#installation","title":"Installation:","text":"<p>You can visit this page to get the installation command: PyPI Project Page</p> <pre><code>!pip install -i https://test.pypi.org/simple/ chemgraphbuilder==1.14.0\n</code></pre>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>chemgraphbuilder/                 # Main source code directory for the chemgraphbuilder package.\nbuild/\n    lib/chemgraphbuilder/         # Build artifacts and main package files.\nchemgraphbuilder.egg-info/        # Package metadata and distribution information.\ndist/                             # Distribution packages (.tar.gz, .whl).\ndocs/                             # Documentation files for the project.\n    index.md                      # Documentation homepage.\n    ...                           # Other markdown pages, images, and files.\nexamples/                         # Example scripts and usage demonstrations.\n__pycache__/                      # Compiled Python bytecode files.\n.gitignore                        # Specifies files and directories to be ignored by Git.\nLICENSE                           # License file detailing the terms of use.\nREADME.md                         # Project overview and instructions.\nmkdocs.yml                        # Configuration file for MkDocs.\nrequirements.txt                  # List of Python dependencies for the project.\nsetup.py                          # Setup script for packaging and distribution.\n</code></pre>"},{"location":"#gallery","title":"Gallery","text":"<p>Welcome to the gallery. Here are some screenshots from the Knowledge Graph built using this package:</p> Previous Next"},{"location":"1.node_collector_processor/","title":"Step 1: Nodes Collector Processor","text":"<p>You can use the <code>NodesCollectorProcessor</code> class in both Python and the command line:</p>"},{"location":"1.node_collector_processor/#python-script-example","title":"Python Script Example","text":"<p>You can use the <code>NodesCollectorProcessor</code> class within a Python script as follows:</p> <pre><code>from chemgraphbuilder.node_collector_processor import NodesCollectorProcessor\nfrom chemgraphbuilder.setup_data_folder import SetupDataFolder\n\n\nnode_type = \"Compound\"  # Change to \"BioAssay\", \"Gene\", or \"Protein\" as needed\nenzyme_list = ['CYP2D6', 'CYP3A4']\n\n# Initialize and setup the data directory before collecting any data\nsetup_folder = SetupDataFolder()\nsetup_folder.setup()\n\n# Initialize the collector\ncollector = NodesCollectorProcessor(node_type=node_type, enzyme_list=enzyme_list, start_chunk=0)\n\n# Collect and process the data\ncollector.collect_and_process_data()\n\n# Close the connection\ncollector.close()\n</code></pre>"},{"location":"1.node_collector_processor/#command-line-interface-cli-example","title":"Command Line Interface (CLI) Example","text":"<p>You can use the <code>NodesCollectorProcessor</code> class from the command line by executing the script with the necessary arguments:</p> <pre><code>setup-data-folder\ncollect-process-nodes --node_type Compound --enzyme_list CYP2D6,CYP3A4 --start_chunk 0 # the default start-chunk is 0\n</code></pre> <p>The <code>node_type</code> argument can be one of <code>\"Compound\"</code>, <code>\"BioAssay\"</code>, <code>\"Gene\"</code>, or <code>\"Protein\"</code>, depending on the type of data you want to collect. The <code>enzyme_list</code> should be a comma-separated string of enzyme names. The <code>start_chunk</code> argument is optional and is used when collecting data for the <code>Compound</code> type because the compound data is downloaded as chunks. If you weren't able to download all the data at once, you can continue the download starting from the desired chunk.</p>"},{"location":"2.relationship_collector_processor/","title":"Relationships Collector and Processor","text":"<p>This module collects and processes relationship data for different types of relationships using the <code>RelationshipPropertiesExtractor</code> and <code>RelationshipDataProcessor</code> classes.</p>"},{"location":"2.relationship_collector_processor/#available-relationship-types","title":"Available Relationship Types","text":"<ul> <li>Assay_Compound</li> <li>Assay_Enzyme</li> <li>Gene_Enzyme</li> <li>Compound_Gene</li> <li>Compound_Similarity</li> <li>Compound_Cooccurrence</li> <li>Compound_Transformation</li> </ul>"},{"location":"2.relationship_collector_processor/#usage-examples","title":"Usage Examples","text":""},{"location":"2.relationship_collector_processor/#python","title":"Python","text":"<p>To use the <code>RelationshipsCollectorProcessor</code> class in Python, follow the example below:</p> <pre><code>from relationship_collector_processor import RelationshipsCollectorProcessor\n\n# Set the relationship type\nrelationship_type = \"Assay_Compound\"  # Change to the desired relationship type\n\n# Initialize the collector\ncollector = RelationshipsCollectorProcessor(relationship_type=relationship_type, start_chunk=0)\n\n# Collect and process the relationship data\ncollector.collect_relationship_data()\n</code></pre>"},{"location":"2.relationship_collector_processor/#command-line","title":"Command Line","text":"<p>To collect and process relationship data from the command line, use the following command:</p> <pre><code># Collect and process the relationship data for the specified relationship type\ncollect-process-relationships --relationship_type Assay_Compound --start_chunk 0\n</code></pre> <p>Replace <code>Assay_Compound</code> with any of the available relationship types to collect different data. If your data is downloaded as chunks and you were not able to complete the  download at once, you can use the <code>start_chunk</code> argument to specify the starting  chunk index and continue the download.</p>"},{"location":"3.graph_nodes_loader/","title":"The <code>GraphNodesLoader</code> class is usable from Python code, and the command line interface.","text":""},{"location":"3.graph_nodes_loader/#usage-in-different-contexts","title":"Usage in Different Contexts","text":""},{"location":"3.graph_nodes_loader/#available-relationships","title":"Available Relationships","text":"<ul> <li><code>Compound</code></li> <li><code>BioAssay</code></li> <li><code>Gene</code></li> <li><code>Protein</code></li> </ul>"},{"location":"3.graph_nodes_loader/#1-python-code","title":"1. Python Code","text":"<pre><code>from chemgraphbuilder.graph_nodes_loader import GraphNodesLoader\n\n# User-provided connection details and label\nuri = \"bolt://localhost:7687\"\nusername = \"neo4j\"\npassword = \"password\"\nlabel = \"Compound\"  # you can add other labels such as (BioAssay, Gene, Protein)\n\ngraph_nodes_loader = GraphNodesLoader(uri, username, password)\ngraph_nodes_loader.load_data_for_node_type(label)\n</code></pre>"},{"location":"3.graph_nodes_loader/#2-command-line","title":"2. Command Line","text":"<p>After installing the package using <code>pip install .</code> or <code>pip install chemgraphbuilder</code>, you can use the command-line interface:</p> <pre><code>load-graph-nodes --uri bolt://localhost:7687 --username neo4j --password password --label Compound\n</code></pre>"},{"location":"4.graph_relationships_loader/","title":"The <code>GraphRelationshipsLoader</code> class is usable from Python code, and the command line interface.","text":""},{"location":"4.graph_relationships_loader/#usage-in-different-contexts","title":"Usage in Different Contexts","text":""},{"location":"4.graph_relationships_loader/#available-relationships","title":"Available Relationships","text":"<ul> <li><code>Assay_Gene</code></li> <li><code>Assay_Compound</code></li> <li><code>Compound_Gene</code></li> <li><code>Compound_Transformation</code></li> <li><code>Gene_Enzyme</code></li> <li><code>Compound_Similarities</code></li> <li><code>Cpd_Cpd_CoOccurence</code></li> <li><code>Cpd_Gene_CoOccurence</code></li> </ul>"},{"location":"4.graph_relationships_loader/#1-python-code","title":"1. Python Code","text":"<pre><code>from chemgraphbuilder.graph_relationships_loader import GraphRelationshipsLoader\n\nuri = \"bolt://localhost:7687\"\nusername = \"neo4j\"\npassword = \"password\"\n\nloader = GraphRelationshipsLoader(uri, username, password)\nloader.add_relationships(\"Assay_Gene\")\nloader.close()\n</code></pre>"},{"location":"4.graph_relationships_loader/#2-command-line","title":"2. Command Line","text":"<p>After installing the package using <code>pip install .</code> or <code>pip install chemgraphbuilder</code>, you can use the command-line interface:</p> <pre><code>load-graph_relationships --uri bolt://localhost:7687 --username neo4j --password password --relationship_type Assay_Gene\n\n</code></pre>"},{"location":"about/","title":"About","text":""},{"location":"about/#about-the-chemgraphbuilder-package","title":"About the <code>chemgraphbuilder</code> Package","text":"<p>The <code>chemgraphbuilder</code> package is a powerful tool designed to facilitate the construction of knowledge graphs that represent complex chemical-gene interactions. This package is ideal for researchers and data scientists working in the fields of bioinformatics, cheminformatics, and computational biology. By providing a structured framework for integrating data on compounds, genes, proteins, and bioassays, <code>chemgraphbuilder</code> helps uncover the intricate relationships between these entities.</p> <p>Key features include:</p> <ul> <li>Node Representation: Incorporates diverse nodes such as compounds, genes, proteins, and bioassays.</li> <li>Comprehensive Relationships: Maps out various interactions, including gene-protein relationships, bioassay-gene relationships, bioassay-compound relationships, compound similarities, compound co-occurrences in literature, and more nuanced interactions like inhibitor, activator, ligand, and other roles between compounds and genes.</li> <li>Data Integration: The knowledge graph schema is designed to support the integration of additional data sources, enhancing the depth and accuracy of the knowledge graph.</li> <li>Command Line and Programmatic Access: Provides flexibility in usage, allowing for integration into larger workflows or standalone analyses.</li> </ul>"},{"location":"about/#about-the-author","title":"About the Author","text":"<p>Asmaa Ali Abdelwahab is a research data scientisy and bioinformatician with extensive experience in data science, particularly in the realm of computational drug design and toxicology. Her work is focused on leveraging artificial intelligence and bioinformatics to have valuable contributions in precision medicine and precision toxicology.</p> <p>With a commitment to open science, Asmaa is passionate about making her work accessible to the broader scientific community, as exemplified by the <code>chemgraphbuilder</code> package. This tool is part of her broader effort to democratize access to advanced data analysis tools in the life sciences, promoting greater collaboration and innovation in the field.</p> <p>Author LinkedIn Profile: https://www.linkedin.com/in/asmaa-a-abdelwahab/.</p> <p>ChemGraphBuilder GitHub: https://github.com/asmaa-a-abdelwahab/ChemGraphBuilder.</p>"},{"location":"documentation/","title":"Documentation of the main classes","text":""},{"location":"documentation/#1-setup-data-folder","title":"1. Setup Data Folder","text":"<p>Module to set up a data directory with a predefined structure.</p> <p>This module provides the DataFolderSetup class, which creates a directory structure for a data folder. The structure includes nodes and relationships folders with specified subfolders.</p> <p>Classes:</p> Name Description <code>DataFolderSetup</code> <p>Class to set up a data directory with a predefined structure.</p> <p>Functions:</p> Name Description <code>main</code> <p>Main function to set up the data directory.</p>"},{"location":"documentation/#chemgraphbuilder.setup_data_folder.SetupDataFolder","title":"<code>SetupDataFolder</code>","text":"<p>Class to set up a data directory with a predefined structure.</p> <p>Attributes:</p> Name Type Description <code>data_folder</code> <code>str</code> <p>The name of the data folder.</p> <code>base_path</code> <code>str</code> <p>The base path for the data directory.</p> <code>structure</code> <code>dict</code> <p>The structure of directories to create.</p> Source code in <code>chemgraphbuilder/setup_data_folder.py</code> <pre><code>class SetupDataFolder:\n    \"\"\"\n    Class to set up a data directory with a predefined structure.\n\n    Attributes:\n        data_folder (str): The name of the data folder.\n        base_path (str): The base path for the data directory.\n        structure (dict): The structure of directories to create.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initializes the DataFolderSetup with the data folder name and directory structure.\n        \"\"\"\n        self.data_folder = \"Data\"\n        self.base_path = os.path.join(os.getcwd(), self.data_folder)\n        self.structure = {\n            \"Nodes\": [\"Compound_Properties\"],\n            \"Relationships\": [\n                \"Assay_Compound_Relationship\",\n                \"Compound_Similarities\",\n                \"Cpd_Cpd_CoOccurrence\",\n                \"Cpd_Gene_CoOccurrence\"\n            ]\n        }\n\n    @staticmethod\n    def create_folder(path):\n        \"\"\"\n        Creates a folder if it does not already exist.\n\n        Args:\n            path (str): The path of the folder to create.\n        \"\"\"\n        if not os.path.exists(path):\n            os.makedirs(path)\n            print(f\"Created folder: {path}\")\n        else:\n            print(f\"Folder already exists: {path}\")\n\n    def setup(self):\n        \"\"\"\n        Sets up the data directory structure based on the predefined structure.\n        \"\"\"\n        # Create the base data directory\n        self.create_folder(self.base_path)\n\n        # Create the 'Nodes' directory and its subdirectories\n        nodes_path = os.path.join(self.base_path, \"Nodes\")\n        self.create_folder(nodes_path)\n        for folder in self.structure[\"Nodes\"]:\n            self.create_folder(os.path.join(nodes_path, folder))\n\n        # Create the 'Relationships' directory and its subdirectories\n        relationships_path = os.path.join(self.base_path, \"Relationships\")\n        self.create_folder(relationships_path)\n        for folder in self.structure[\"Relationships\"]:\n            self.create_folder(os.path.join(relationships_path, folder))\n</code></pre>"},{"location":"documentation/#chemgraphbuilder.setup_data_folder.SetupDataFolder.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the DataFolderSetup with the data folder name and directory structure.</p> Source code in <code>chemgraphbuilder/setup_data_folder.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initializes the DataFolderSetup with the data folder name and directory structure.\n    \"\"\"\n    self.data_folder = \"Data\"\n    self.base_path = os.path.join(os.getcwd(), self.data_folder)\n    self.structure = {\n        \"Nodes\": [\"Compound_Properties\"],\n        \"Relationships\": [\n            \"Assay_Compound_Relationship\",\n            \"Compound_Similarities\",\n            \"Cpd_Cpd_CoOccurrence\",\n            \"Cpd_Gene_CoOccurrence\"\n        ]\n    }\n</code></pre>"},{"location":"documentation/#chemgraphbuilder.setup_data_folder.SetupDataFolder.create_folder","title":"<code>create_folder(path)</code>  <code>staticmethod</code>","text":"<p>Creates a folder if it does not already exist.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path of the folder to create.</p> required Source code in <code>chemgraphbuilder/setup_data_folder.py</code> <pre><code>@staticmethod\ndef create_folder(path):\n    \"\"\"\n    Creates a folder if it does not already exist.\n\n    Args:\n        path (str): The path of the folder to create.\n    \"\"\"\n    if not os.path.exists(path):\n        os.makedirs(path)\n        print(f\"Created folder: {path}\")\n    else:\n        print(f\"Folder already exists: {path}\")\n</code></pre>"},{"location":"documentation/#chemgraphbuilder.setup_data_folder.SetupDataFolder.setup","title":"<code>setup()</code>","text":"<p>Sets up the data directory structure based on the predefined structure.</p> Source code in <code>chemgraphbuilder/setup_data_folder.py</code> <pre><code>def setup(self):\n    \"\"\"\n    Sets up the data directory structure based on the predefined structure.\n    \"\"\"\n    # Create the base data directory\n    self.create_folder(self.base_path)\n\n    # Create the 'Nodes' directory and its subdirectories\n    nodes_path = os.path.join(self.base_path, \"Nodes\")\n    self.create_folder(nodes_path)\n    for folder in self.structure[\"Nodes\"]:\n        self.create_folder(os.path.join(nodes_path, folder))\n\n    # Create the 'Relationships' directory and its subdirectories\n    relationships_path = os.path.join(self.base_path, \"Relationships\")\n    self.create_folder(relationships_path)\n    for folder in self.structure[\"Relationships\"]:\n        self.create_folder(os.path.join(relationships_path, folder))\n</code></pre>"},{"location":"documentation/#chemgraphbuilder.setup_data_folder.main","title":"<code>main()</code>","text":"<p>Main function to set up the data directory.</p> Source code in <code>chemgraphbuilder/setup_data_folder.py</code> <pre><code>def main():\n    \"\"\"\n    Main function to set up the data directory.\n    \"\"\"\n    data_folder_setup = SetupDataFolder()\n    data_folder_setup.setup()\n</code></pre>"},{"location":"documentation/#2-neo4j-driver","title":"2. Neo4j Driver","text":"<p>Module for managing connections to a Neo4j database.</p> <p>This module provides classes and methods to establish and manage connections with a Neo4j database, including custom error handling.</p>"},{"location":"documentation/#chemgraphbuilder.neo4jdriver.Neo4jBase","title":"<code>Neo4jBase</code>","text":"<p>Base class to manage connections with the Neo4j database.</p> <p>Attributes: - uri: The connection URI for the Neo4j database. - user: The username to use for authentication. - driver: The driver object used to interact with the Neo4j database.</p> <p>Methods: - connect_to_neo4j: Establish a connection to the Neo4j database. - close: Close the connection to the Neo4j database.</p> Source code in <code>chemgraphbuilder/neo4jdriver.py</code> <pre><code>class Neo4jBase:\n    \"\"\"\n    Base class to manage connections with the Neo4j database.\n\n    Attributes:\n    - uri: The connection URI for the Neo4j database.\n    - user: The username to use for authentication.\n    - driver: The driver object used to interact with the Neo4j database.\n\n    Methods:\n    - connect_to_neo4j: Establish a connection to the Neo4j database.\n    - close: Close the connection to the Neo4j database.\n    \"\"\"\n\n    def __init__(self, logger=None, uri=\"tcp://5.tcp.eu.ngrok.io:12445\", user=\"neo4j\"):\n        self.uri = uri\n        self.user = user\n        self.driver = None\n\n        # Set up logging configuration\n        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n        self.logger = logger or logging.getLogger(__name__)\n\n    def connect_to_neo4j(self):\n        \"\"\"Establish a connection to the Neo4j database using provided URI and username.\"\"\"\n        password = os.getenv(\"NEO4J_PASSWORD\")  # Check if password is set in environment variables\n        if not password:\n            password = getpass.getpass(prompt=\"Enter Neo4j password: \")\n\n        try:\n            self.driver = GraphDatabase.driver(self.uri, auth=(self.user, password))\n            self.logger.info(\"Successfully connected to the Neo4j database.\")\n        except Exception as e:\n            self.logger.error(\"Failed to connect to the Neo4j database: %s\", e)\n            raise Neo4jConnectionError(\"Failed to connect to the Neo4j database.\") from e\n\n    def close(self):\n        \"\"\"Close the connection to the Neo4j database.\"\"\"\n        if self.driver:\n            self.driver.close()\n            self.logger.info(\"Neo4j connection closed successfully.\")\n</code></pre>"},{"location":"documentation/#chemgraphbuilder.neo4jdriver.Neo4jBase.close","title":"<code>close()</code>","text":"<p>Close the connection to the Neo4j database.</p> Source code in <code>chemgraphbuilder/neo4jdriver.py</code> <pre><code>def close(self):\n    \"\"\"Close the connection to the Neo4j database.\"\"\"\n    if self.driver:\n        self.driver.close()\n        self.logger.info(\"Neo4j connection closed successfully.\")\n</code></pre>"},{"location":"documentation/#chemgraphbuilder.neo4jdriver.Neo4jBase.connect_to_neo4j","title":"<code>connect_to_neo4j()</code>","text":"<p>Establish a connection to the Neo4j database using provided URI and username.</p> Source code in <code>chemgraphbuilder/neo4jdriver.py</code> <pre><code>def connect_to_neo4j(self):\n    \"\"\"Establish a connection to the Neo4j database using provided URI and username.\"\"\"\n    password = os.getenv(\"NEO4J_PASSWORD\")  # Check if password is set in environment variables\n    if not password:\n        password = getpass.getpass(prompt=\"Enter Neo4j password: \")\n\n    try:\n        self.driver = GraphDatabase.driver(self.uri, auth=(self.user, password))\n        self.logger.info(\"Successfully connected to the Neo4j database.\")\n    except Exception as e:\n        self.logger.error(\"Failed to connect to the Neo4j database: %s\", e)\n        raise Neo4jConnectionError(\"Failed to connect to the Neo4j database.\") from e\n</code></pre>"},{"location":"documentation/#chemgraphbuilder.neo4jdriver.Neo4jConnectionError","title":"<code>Neo4jConnectionError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Custom exception for Neo4j connection errors.</p> Source code in <code>chemgraphbuilder/neo4jdriver.py</code> <pre><code>class Neo4jConnectionError(Exception):\n    \"\"\"Custom exception for Neo4j connection errors.\"\"\"\n</code></pre>"},{"location":"documentation/#3-node-properties-extractor","title":"3. Node Properties Extractor","text":"<p>This module defines the <code>NodePropertiesExtractor</code> class, responsible for extracting data from the PubChem database to build knowledge graphs in Neo4j. The class focuses on nodes representing chemical entities and their relationships, allowing users to query chemical data and construct a graph-based representation of chemical compounds, their assays, related genes, and proteins.</p> <p>The primary functionality revolves around fetching detailed information about specified enzymes from PubChem, including assay data, gene properties, protein properties, and compound properties. It processes this data into a structured format suitable for knowledge graph construction, specifically tailored for use with Neo4j databases.</p> <p>Classes:</p> Name Description <code>- NodePropertiesExtractor</code> <p>A class to extract data from PubChem to build knowledge graphs in Neo4j.</p> Usage Example <p>enzyme_list = ['CYP2D6', 'CYP3A4'] extractor = NodePropertiesExtractor(enzyme_list) df = extractor.run() This example initiates the extractor with a list of enzymes, fetches their data from PubChem, processes it, and potentially prepares it for knowledge graph construction in Neo4j.</p> Note <p>To fully utilize this class, ensure you have network access to the PubChem API for data retrieval and a Neo4j database instance for knowledge graph construction. The class methods facilitate data extraction and processing, but integrating the output into Neo4j requires additional steps outside the scope of this class.</p>"},{"location":"documentation/#chemgraphbuilder.node_properties_extractor.NodePropertiesExtractor","title":"<code>NodePropertiesExtractor</code>","text":"<p>Extracts data from PubChem to build knowledge graphs in Neo4j, focusing on nodes representing chemical entities and their relationships. This class serves as a bridge between the PubChem database and Neo4j, allowing users to query chemical data and construct a graph-based representation of chemical compounds, their assays, related genes, and proteins.</p> <p>The primary functionality revolves around fetching detailed information about specified enzymes from PubChem, including assay data, gene properties, protein properties, and compound properties. It processes this data into a structured format suitable for knowledge graph construction, specifically tailored for use with Neo4j databases.</p> <p>Attributes:</p> Name Type Description <code>enzyme_list</code> <code>list of str</code> <p>Enzymes to query in the PubChem database.</p> <code>_base_url</code> <code>str</code> <p>Base URL for the PubChem API requests.</p> <code>_sep</code> <code>str</code> <p>Delimiter for parsing CSV data from PubChem.</p> <code>_enzyme_count</code> <code>int</code> <p>Number of enzymes in the enzyme_list, calculated at</p> <p>Parameters:</p> Name Type Description Default <code>enzyme_list</code> <code>list of str</code> <p>List of enzyme names for which assay data</p> required <code>base_url</code> <code>str</code> <p>Base URL for PubChem API requests. Defaults to</p> <code>'https://pubchem.ncbi.nlm.nih.gov/rest/pug/assay/target/genesymbol'</code> <code>sep</code> <code>str</code> <p>Separator used for parsing CSV data returned</p> <code>','</code> Usage Example <p>enzyme_list = ['CYP2D6', 'CYP3A4'] extractor = NodePropertiesExtractor(enzyme_list) df = extractor.run() This example initiates the extractor with a list of enzymes, fetches their data from PubChem, processes it, and potentially prepares it for knowledge graph construction in Neo4j.</p> Note <p>To fully utilize this class, ensure you have network access to the PubChem API for data retrieval and a Neo4j database instance for knowledge graph construction. The class methods facilitate data extraction and processing, but integrating the output into Neo4j requires additional steps outside the scope of this class.</p> Source code in <code>chemgraphbuilder/node_properties_extractor.py</code> <pre><code>class NodePropertiesExtractor:\n    \"\"\"\n    Extracts data from PubChem to build knowledge graphs in Neo4j,\n    focusing on nodes representing chemical entities and their relationships.\n    This class serves as a bridge between the PubChem database and Neo4j,\n    allowing users to query chemical data and construct a graph-based\n    representation of chemical compounds, their assays, related genes, and proteins.\n\n    The primary functionality revolves around fetching detailed information\n    about specified enzymes from PubChem, including assay data, gene properties,\n    protein properties, and compound properties. It processes this data into\n    a structured format suitable for knowledge graph construction, specifically\n    tailored for use with Neo4j databases.\n\n    Attributes:\n        enzyme_list (list of str): Enzymes to query in the PubChem database.\n        _base_url (str): Base URL for the PubChem API requests.\n        _sep (str): Delimiter for parsing CSV data from PubChem.\n        _enzyme_count (int): Number of enzymes in the enzyme_list, calculated at\n        initialization.\n\n    Parameters:\n        enzyme_list (list of str): List of enzyme names for which assay data\n        will be fetched from PubChem.\n        base_url (str, optional): Base URL for PubChem API requests. Defaults to\n        the assay target genesymbol endpoint.\n        sep (str, optional): Separator used for parsing CSV data returned\n        by PubChem. Defaults to ','.\n\n    Usage Example:\n        &gt;&gt;&gt; enzyme_list = ['CYP2D6', 'CYP3A4']\n        &gt;&gt;&gt; extractor = NodePropertiesExtractor(enzyme_list)\n        &gt;&gt;&gt; df = extractor.run()\n        This example initiates the extractor with a list of enzymes, fetches\n        their data from PubChem, processes it, and potentially prepares it for\n        knowledge graph construction in Neo4j.\n\n    Note:\n        To fully utilize this class, ensure you have network access to the\n        PubChem API for data retrieval and a Neo4j database instance for\n        knowledge graph construction. The class methods facilitate data extraction\n        and processing, but integrating the output into Neo4j requires additional\n        steps outside the scope of this class.\n    \"\"\"\n\n    _REQUEST_TIMEOUT = 30  # in seconds\n    _CONCURRENT_REQUEST_LIMIT = 2\n    _RETRY_ATTEMPTS = 3  # number of times to retry a failed request\n\n    def __init__(self, enzyme_list,\n                 base_url=\"https://pubchem.ncbi.nlm.nih.gov/rest/pug/assay/target/genesymbol\",\n                 sep=\",\"):\n        \"\"\"\n        Initializes a NodePropertiesExtractor instance, setting up the base URL\n        for API requests, the separator for CSV parsing, and the list of enzymes\n        to query from the PubChem database.\n\n        Parameters:\n            enzyme_list (list of str): A list of enzyme names for which to fetch\n            assay data.\n            base_url (str, optional): The base URL for PubChem API requests.\n            Default is set to the assay target genesymbol endpoint.\n            sep (str, optional): The delimiter to use for parsing CSV files\n            returned by PubChem. Defaults to ','.\n\n        Attributes:\n            _base_url (str): Stores the base URL for API requests.\n            _sep (str): Stores the delimiter for parsing CSV data.\n            enzyme_list (list of str): Stores the list of enzyme names provided\n            during initialization.\n            _enzyme_count (int): The number of enzymes in the enzyme_list.\n        \"\"\"\n        self._base_url = base_url\n        self._sep = sep\n        self.enzyme_list = enzyme_list\n        self._enzyme_count = len(enzyme_list)\n\n    def _make_request(self, url):\n        \"\"\"\n        Sends an HTTP GET request to a specified URL with built-in retry logic.\n        If the request fails, it retries the request up to a predefined number\n        of attempts with exponential backoff to handle potential temporary network\n        or server issues.\n\n        The method attempts to gracefully handle server-side errors\n        (HTTP 4XX/5XX responses) by raising an exception if the response status\n        code indicates an error. For client-side errors (e.g., connectivity issues),\n        it logs a warning and retries the request.\n\n        Parameters:\n            url (str): The complete URL to which the HTTP GET request is sent.\n\n        Returns:\n            requests.Response: The response object from the server if the request\n            is successfully completed.\n\n        Raises:\n            requests.RequestException: If the request fails to complete\n            successfully after the maximum number of retry attempts.\n        \"\"\"\n        for attempt in range(self._RETRY_ATTEMPTS):\n            try:\n                response = requests.get(url, timeout=self._REQUEST_TIMEOUT)\n                response.raise_for_status()  # Checks for HTTP errors\n                return response\n            except requests.RequestException as e:\n                logging.warning(\"Attempt %s of %s failed for URL: %s. Error: %s\",\n                                attempt + 1, self._RETRY_ATTEMPTS, url, e)\n                if attempt + 1 == self._RETRY_ATTEMPTS:\n                    raise  # All attempts failed; re-raise the last exception\n                time.sleep(2 ** attempt)  # Exponential backoff\n\n    def get_enzyme_assays(self, enzyme):\n        \"\"\"\n        Fetches assay data for a specified enzyme from the PubChem database and\n        returns it as a pandas DataFrame.\n\n        This method constructs a URL to query the PubChem database for concise\n        assay data related to the given enzyme. It processes the CSV response\n        into a DataFrame, which includes various assay data points provided by PubChem.\n\n        Parameters:\n            enzyme (str): The name of the enzyme for which assay data is\n            requested. This name is used in the API query.\n\n        Returns:\n            pd.DataFrame: A DataFrame containing the assay data fetched from\n            PubChem for the specified enzyme. The DataFrame includes columns\n            based on the CSV response from PubChem, such as assay ID, results,\n            and conditions. Returns None if no data is available or if an error\n            occurs during data fetching or processing.\n\n        Raises:\n            requests.RequestException: If an error occurs during the HTTP\n            request to the PubChem API.\n            pd.errors.EmptyDataError: If the response from PubChem contains no data.\n\n        Example:\n            &gt;&gt;&gt; extractor = NodePropertiesExtractor(['enzyme'])\n            &gt;&gt;&gt; enzyme_assays_df = extractor.get_enzyme_assays('enzyme')\n            &gt;&gt;&gt; print(enzyme_assays_df.head())\n        \"\"\"\n        assays_url = f\"{self._base_url}/{enzyme.lower()}/concise/CSV\"\n        logging.info(f\"Fetching assays for enzyme: {enzyme}\")\n\n        response = self._make_request(assays_url)\n\n        assays_csv_string = response.text\n        assays_csv_string_io = StringIO(assays_csv_string)\n        try:\n            assays_df = pd.read_csv(assays_csv_string_io,\n                                    sep=self._sep,\n                                    low_memory=False)\n            logging.info(\"Assays DataFrame for enzyme %s has shape: %s\",\n                         enzyme, assays_df.shape)\n            return assays_df\n        except pd.errors.EmptyDataError:\n            logging.warning(\"No data available for enzyme %s.\", enzyme)\n            return None\n\n    def _process_enzymes(self, enzyme_list):\n        \"\"\"\n        Iterates over a list of enzyme names, fetching assay data for each enzyme\n        and aggregating the results into a list of DataFrames.\n\n        This method calls `get_enzyme_assays` for each enzyme in the provided\n        list, collecting the assay data (if available) into a list of pandas\n        DataFrames. This list can then be used for further processing or analysis.\n\n        Parameters:\n            enzyme_list (list of str): A list containing the names of enzymes\n            for which to fetch assay data.\n\n        Returns:\n            list of pd.DataFrame: A list containing a pandas DataFrame for each\n            enzyme for which assay data was successfully fetched and processed.\n            Each DataFrame includes the assay data from PubChem for that enzyme.\n            If no data is available for an enzyme, it is omitted from the list.\n        \"\"\"\n        df_list = [self.get_enzyme_assays(enzyme) for enzyme in enzyme_list]\n        return [df for df in df_list if df is not None]\n\n    def _concatenate_data(self, df_list):\n        \"\"\"\n        Concatenates a list of pandas DataFrames into a single DataFrame.\n        This method is useful for aggregating\n        data fetched from multiple sources or APIs into a unified structure.\n        If the list is empty, it returns None to indicate that no data was\n        aggregated.\n\n        Parameters:\n            df_list (List[pd.DataFrame]): A list of pandas DataFrames to\n            concatenate. These DataFrames should have the same structure\n            (columns) to ensure proper concatenation.\n\n        Returns:\n            pd.DataFrame or None: A single concatenated DataFrame comprising all\n            rows from the input DataFrames, indexed continuously. Returns None\n            if the input list is empty, indicating there is no data to concatenate.\n        \"\"\"\n        if df_list:\n            return pd.concat(df_list, ignore_index=True)\n        return None\n\n    def run(self):\n        \"\"\"\n        Orchestrates the process of fetching, filtering, and aggregating assay\n        data from PubChem for a predefined list of enzymes.\n\n        This method iteratively queries PubChem for assay data corresponding\n        to each enzyme specified in the `enzyme_list` attribute during class\n        initialization. It performs the following steps for each enzyme:\n        1. Constructs a query URL and fetches assay data from PubChem.\n        2. Filters the fetched data based on predefined criteria\n        (e.g., containing specific substrings in the assay name).\n        3. Aggregates the filtered data into a single pandas DataFrame.\n        4. Identifies enzymes for which data could not be fetched or were\n        excluded based on filtering criteria, logging their names.\n\n        The final aggregated DataFrame, containing assay data for all successfully\n        processed enzymes, is then saved to a CSV file. This method facilitates\n        the extraction and preprocessing of chemical assay data for further\n        analysis or integration into knowledge graphs.\n\n        Note:\n            - This method relies on the successful response from PubChem\n            for each enzyme query.\n            - Enzymes with no available data or failing to meet the filtering\n            criteria are excluded from the final DataFrame.\n            - The output CSV file is saved in the current working directory\n            with the name 'Data/AllDataConnected.csv'.\n\n        Returns:\n            pd.DataFrame: A DataFrame containing the aggregated and filtered\n            assay data for the specified enzymes. Columns in the DataFrame\n            correspond to the assay data fields returned by PubChem, subject to\n            the filtering criteria applied within this method.\n\n        Raises:\n            requests.RequestException: If there is an issue with fetching data\n            from PubChem, such as a network problem or an invalid response.\n\n        Example:\n            Assuming `enzyme_list` was set to ['CYP2D6', 'CYP3A4'] during\n            class initialization:\n\n            &gt;&gt;&gt; extractor = NodePropertiesExtractor(['CYP2D6', 'CYP3A4'])\n            &gt;&gt;&gt; extractor.create_data_directories()\n            &gt;&gt;&gt; result_df = extractor.run()\n            &gt;&gt;&gt; print(result_df.head())\n\n            This will fetch and process assay data for 'CYP2D6' and 'CYP3A4',\n            returning a DataFrame with the processed data.\n        \"\"\"\n\n        # Initialize an empty list to store enzymes with successful responses\n        enzymes_with_response = []\n\n        # Keep a copy of the original list to identify removed enzymes later\n        original_enzyme_list = self.enzyme_list.copy()\n\n        for enzyme in self.enzyme_list:\n            # Formulate the URL\n            url = f\"{self._base_url}/{enzyme}/concise/CSV\"\n\n            try:\n                response = requests.get(url)\n                # Check for a successful response (status code 200)\n                if response.status_code == 200:\n                    enzymes_with_response.append(enzyme)  # Keep the enzyme in the new list\n            except requests.RequestException:\n                # If there's an exception, skip adding the enzyme to the new list\n                pass\n\n        # Update the enzyme list with only the enzymes that had a successful response\n        self.enzyme_list = enzymes_with_response\n\n        # Identify and print the removed enzymes\n        removed_enzymes = [enzyme for enzyme in original_enzyme_list if enzyme not in enzymes_with_response]\n        if removed_enzymes:\n            logging.info(\"These enzymes were removed because their names aren't correct: %s\",\n                         \", \".join(removed_enzymes))\n\n        df_list = self._process_enzymes(self.enzyme_list)\n        df = self._concatenate_data(df_list)\n        substrings_to_filter = ['CYP', 'Cytochrome']\n        pattern = '|'.join(substrings_to_filter)\n        df = df[df['Assay Name'].str.contains(pattern, case=False, na=False)]\n        df.to_csv('Data/AllDataConnected.csv', index=False)\n        return df\n\n    def _fetch_gene_details(self, gene_id):\n        \"\"\"\n        Fetches gene details in parallel using the PubChem API.\n\n        Args:\n            gene_id (int): The gene ID for fetching details.\n\n        Returns:\n            tuple: Contains gene ID, symbol, taxonomy, taxonomy ID, and synonyms.\n        \"\"\"\n        BASE_URL = \"https://pubchem.ncbi.nlm.nih.gov/rest/pug\"\n        url = f\"{BASE_URL}/gene/geneid/{int(gene_id)}/summary/JSON\"\n        try:\n            response = self._make_request(url)\n            data = response.json()\n\n            # Extracting the necessary details\n            symbol = data['GeneSummaries']['GeneSummary'][0].get('Symbol', None)\n            taxonomy = data['GeneSummaries']['GeneSummary'][0].get('Taxonomy', None)\n            taxonomy_id = data['GeneSummaries']['GeneSummary'][0].get('TaxonomyID', None)\n            synonyms = data['GeneSummaries']['GeneSummary'][0].get('Synonym', None)\n            # print(type(synonyms))\n            return gene_id, symbol, taxonomy, taxonomy_id, synonyms\n        except Exception as e:\n            logging.error(f\"Error fetching details for gene_id {gene_id}: {e}\")\n            return gene_id, None, None, None, None\n\n    def extract_gene_properties(self, main_data):\n        \"\"\"\n        Extracts and processes gene properties from a given data source,\n        specifically targeting genes relevant to the study (e.g., CYP enzymes)\n        and records their details in a structured DataFrame.\n\n        This method reads gene data from a CSV file specified by `main_data`,\n        queries the PubChem database for additional properties of each unique\n        gene ID found in the file, and compiles these properties into a new\n        DataFrame. It focuses on fetching details like gene symbols, taxonomy,\n        taxonomy IDs, and synonyms for each gene. The final DataFrame is filtered\n        to include only genes of particular interest (e.g., certain CYP enzymes)\n        and saved to a separate CSV file for further analysis or use.\n\n        Parameters:\n            main_data (str): Path to a CSV file containing main data was which\n            generated after running `extractor.run()`.\n\n        Returns:\n            pd.DataFrame: A DataFrame containing the compiled gene properties,\n            including GeneID, Symbol, Taxonomy, Taxonomy ID, and Synonyms,\n            filtered to include only specified genes of interest. This DataFrame\n            is also saved to 'Data/Nodes/Gene_Properties.csv'.\n\n        Raises:\n            Exception: If there's an issue reading the initial CSV file or\n            fetching gene details from PubChem, details of the exception are\n            logged, and the method proceeds to process the next gene ID.\n\n        Example:\n            &gt;&gt;&gt; extractor = NodePropertiesExtractor(['CYP2D6', 'CYP3A4'])\n            &gt;&gt;&gt; extractor.create_data_directories()\n            &gt;&gt;&gt; extractor.run()\n            &gt;&gt;&gt; gene_properties_df = extractor.extract_gene_properties('Data/AllDataConnected.csv')\n            &gt;&gt;&gt; print(gene_properties_df.head())\n\n            This would read gene IDs from 'Data/AllDataConnected.csv', fetch\n            their properties from PubChem, and compile the details into a\n            DataFrame, filtering for specified genes of interest and saving\n            the results to 'Data/Nodes/Gene_Properties.csv'.\n\n        Note:\n            The method filters the resulting DataFrame to include only genes with\n            symbols in the predefined enzyme_list. Adjust this list as necessary\n            to match the focus of your study or application.\n        \"\"\"\n        df = pd.read_csv(main_data)\n        df_gene = pd.DataFrame(columns=['GeneID', 'Symbol', 'Taxonomy',\n                                        'Taxonomy ID', 'Synonyms'])\n\n        unique_gene_ids = df['Target GeneID'].unique().tolist()\n\n        gene_details = []\n\n        for gene_id in unique_gene_ids:\n            try:\n                gene_id, symbol, taxonomy, taxonomy_id, synonyms = self._fetch_gene_details(gene_id)\n                gene_details.append({\n                    'GeneID': gene_id,\n                    'Symbol': symbol,\n                    'Taxonomy': taxonomy,\n                    'Taxonomy ID': taxonomy_id,\n                    'Synonyms': str(synonyms)\n                })\n            except Exception as exc:\n                logging.error(\"Error occurred while processing gene_id %s: %s\",\n                              gene_id, exc)\n                gene_details.append({\n                    'GeneID': gene_id,\n                    'Symbol': None,\n                    'Taxonomy': None,\n                    'Taxonomy ID': None,\n                    'Synonyms': None\n                })\n\n        # Now create the DataFrame from the list of dictionaries\n        df_gene = pd.DataFrame(gene_details)\n        n = self._enzyme_count\n        gene_ids = df['Target GeneID'].value_counts().head(n).index.tolist()\n        df_gene = df_gene[df_gene['GeneID'].isin([int(item) for item in gene_ids])]\n        df_gene.to_csv('Data/Nodes/Gene_Properties.csv', sep=',', index=False)\n        return df_gene\n\n    def _fetch_assay_details(self, aid):\n        \"\"\"\n        Fetches assay details from the PubChem API for a given assay ID.\n\n        Args:\n            aid (int): The assay ID to fetch details for.\n\n        Returns:\n            dict: A dictionary containing assay details like AID, SourceName,\n            SourceID, Name, and Description. Returns None if an error occurs\n            during fetching or parsing.\n        \"\"\"\n        BASE_URL = \"https://pubchem.ncbi.nlm.nih.gov/rest/pug\"\n        url = f\"{BASE_URL}/assay/aid/{aid}/summary/XML\"  # Constructing the API URL\n        response = self._make_request(url)  # Making the API request\n        xml_data = response.text  # Getting the response text\n\n        try:\n            # Parsing the XML response\n            data_dict = xmltodict.parse(xml_data)\n            properties = ['AID', 'SourceName', 'SourceID', 'Name', 'Description',\n                          'Protocol', 'Comment', 'Method', 'Target', 'CIDCountAll',\n                          'CIDCountActive', 'CIDCountInactive', 'CIDCountInconclusive',\n                          'CIDCountUnspecified', 'CIDCountProbe']\n\n            assay_data = {}\n            # Extracting required properties from the parsed XML\n            for prop in properties:\n                assay_data[prop] = data_dict.get('AssaySummaries', {}).get('AssaySummary', {}).get(prop, None)\n            return assay_data\n        except Exception as e:\n            logging.error(f\"Error parsing XML for AID {aid}: {e}\")\n            return None\n\n    def extract_assay_properties(self, main_data):\n        \"\"\"\n        Extracts detailed properties of assays from PubChem for each unique assay\n        ID found in the input data file.\n\n        This method processes an input CSV file containing assay IDs (AID) and\n        performs concurrent HTTP requests to fetch detailed assay properties\n        from the PubChem database. The retrieved details include assay type,\n        activity name, source name, source ID, name, and description. These\n        properties are compiled into a new DataFrame, which is then\n        saved to a CSV file for further analysis or use.\n\n        The method employs a ThreadPoolExecutor to manage concurrent requests\n        efficiently, improving the performance when dealing with a large number\n        of assay IDs. Errors encountered during data fetching are logged, and the\n        process continues with the next assay ID, ensuring the method's robustness.\n\n        Parameters:\n            main_data (str): Path to a CSV file containing main data was which\n            generated after running `extractor.run()`.\n\n        Returns:\n            pd.DataFrame: A DataFrame containing the fetched assay properties,\n            including columns for AID, Assay Type, Activity Name, SourceName,\n            SourceID, Name, and Description. This DataFrame is saved to\n            'Data/Nodes/Assay_Properties.csv' in the current working directory.\n\n        Raises:\n            ValueError: If the input CSV file is empty or does not contain the 'AID' column.\n\n        Example:\n            &gt;&gt;&gt; extractor = NodePropertiesExtractor(['CYP2D6', 'CYP3A4'])\n            &gt;&gt;&gt; extractor.create_data_directories()\n            &gt;&gt;&gt; extractor.run()\n            &gt;&gt;&gt; assay_properties_df = extractor.extract_assay_properties('Data/AllDataConnected.csv')\n            &gt;&gt;&gt; print(assay_properties_df.head())\n\n            This example reads assay IDs from 'Data/AllDataConnected.csv',\n            queries PubChem for their detailed properties, and compiles the\n            results into a DataFrame, which is also saved to 'Data/Nodes/Assay_Properties.csv'.\n\n        Note:\n            This method requires network access to the PubChem API and assumes\n            the availability of a valid 'AID' column in the input CSV file.\n            Ensure the input file path is correct and accessible to avoid errors during processing.\n        \"\"\"\n\n        df = pd.read_csv(main_data)\n\n        # Check if the DataFrame is valid\n        if df.empty or 'AID' not in df.columns:\n            logging.error(\"DataFrame is empty or does not contain 'AID' column.\")\n            return pd.DataFrame()\n\n        unique_aids = df['AID'].unique().tolist()  # Extracting unique assay IDs\n\n        columns = ['AID', 'Assay Type', 'Activity Name', 'SourceName',\n                   'SourceID', 'Name', 'Description', 'Protocol',\n                   'Comment', 'Method', 'Target', 'CIDCountAll',\n                   'CIDCountActive', 'CIDCountInactive', 'CIDCountInconclusive',\n                   'CIDCountUnspecified', 'CIDCountProbe']\n        assay_df = pd.DataFrame(columns=columns)  # Initializing a DataFrame to store assay properties\n\n        # Using ThreadPoolExecutor for concurrent fetching of assay details\n        with concurrent.futures.ThreadPoolExecutor(max_workers=self._CONCURRENT_REQUEST_LIMIT) as executor:\n            future_to_aid = {executor.submit(self._fetch_assay_details, aid): aid for aid in unique_aids}\n\n            # Iterating over completed futures\n            for future in concurrent.futures.as_completed(future_to_aid):\n                aid = future_to_aid[future]\n                try:\n                    assay_data = future.result()  # Fetching the result from the future\n                    if assay_data:\n                        # Preparing a new row with the fetched data\n                        new_row = {\n                            'AID': aid,\n                            'Assay Type': df.loc[df['AID'] == aid, 'Assay Type'].iloc[0],\n                            'Activity Name': df.loc[df['AID'] == aid, 'Activity Name'].iloc[0],\n                            **assay_data\n                        }\n                        # Adding the new row to the DataFrame\n                        assay_df = pd.concat([assay_df, pd.DataFrame([new_row])], ignore_index=True)\n                except Exception as exc:\n                    # Logging any errors encountered during the fetch\n                    logging.error(f\"Error occurred while processing AID {aid}: {exc}\")\n\n        # Saving the updated DataFrame to a CSV file\n        assay_df.to_csv('Data/Nodes/Assay_Properties.csv', sep=',', index=False)\n        return assay_df\n\n    def extract_protein_properties(self, main_data):\n        \"\"\"\n        Extracts and compiles protein properties from the NCBI protein database\n        based on accession numbers.\n\n        Given a CSV file specified by `main_data`, this method reads protein\n        accession numbers and performs web scraping on the NCBI protein database\n        pages to extract protein titles. The method constructs a URL for\n        each accession number, sends a request to retrieve the page content,\n        and parses the HTML to find the protein title. The extracted titles,\n        along with their corresponding accession numbers and URLs, are\n        compiled into a DataFrame. This DataFrame is saved to a CSV file,\n        providing a structured summary of protein properties for further analysis or use.\n\n        Parameters:\n            main_data (str): Path to a CSV file containing main data was which\n            generated after running `extractor.run()`.\n\n        Returns:\n            pd.DataFrame: A DataFrame with columns 'RefSeq Accession', 'URL',\n            and 'Description', where 'Description' contains the title of the\n            protein extracted from its NCBI page. This DataFrame is saved to\n            'Data/Nodes/Protein_Properties.csv' in the current working directory.\n\n        Raises:\n            Exception: If there's an issue reading the initial CSV file or\n            querying the NCBI database, details of the exception are logged.\n            The method continues processing the next accession number,\n            ensuring robustness against individual failures.\n\n        Example:\n            Assuming 'protein_data.csv' contains a column 'Target Accession'\n            with accession numbers:\n\n            &gt;&gt;&gt; extractor = NodePropertiesExtractor(['CYP2D6', 'CYP3A4'])\n            &gt;&gt;&gt; extractor.create_data_directories()\n            &gt;&gt;&gt; extractor.run() # you need to run this only once\n            &gt;&gt;&gt; protein_properties_df = extractor.extract_protein_properties('Data/AllDataConnected.csv')\n            &gt;&gt;&gt; print(protein_properties_df.head())\n\n            This would read accession numbers from 'Data/AllDataConnected.csv',\n            scrape their titles from the NCBI protein database, and compile the\n            results into a DataFrame, which is also saved to\n            'Data/Nodes/Protein_Properties.csv'.\n\n        Note:\n            This method requires internet access to query the NCBI protein\n            database. Ensure the input file path is correct and accessible to\n            avoid errors during processing. Web scraping is dependent on the\n            structure of the web page; changes to the NCBI protein database\n            pages may require updates to the scraping logic.\n        \"\"\"\n\n        # Initialize a list to store the extracted data\n        data = []\n\n        n = self._enzyme_count\n        df = pd.read_csv(main_data)\n        gene_ids = df['Target GeneID'].value_counts().head(n).index.tolist()\n        df = df[df['Target GeneID'].isin([int(item) for item in gene_ids])]\n        Accessions = df['Target Accession'].unique().tolist()\n        # Iterate over each protein accession number in the DataFrame\n        for accession in Accessions:\n            # Construct the URL to query the NCBI protein database\n            url = f\"https://www.ncbi.nlm.nih.gov/protein/{accession}\"\n\n            try:\n                # Send an HTTP request to the URL\n                response = requests.get(url)\n\n                # Parse the HTML content of the response\n                soup = BeautifulSoup(response.text, 'html.parser')\n\n                # Extract the title from the parsed HTML\n                title = soup.title.string if soup.title else 'Title Not Found'\n\n                # Append the extracted data to the list\n                data.append({'RefSeq Accession': accession,\n                             'URL': url, 'Description': title})\n            except Exception as e:\n                # In case of an error, log the error message\n                logging.error(f\"Error fetching data for accession {accession}: {e}\")\n                data.append({'RefSeq Accession': accession, 'URL': url,\n                             'Description': f'Error: {e}'})\n\n        # Convert the list of data into a DataFrame\n        protein_df = pd.DataFrame(data)\n\n        # Save the DataFrame to a CSV file\n        protein_df.to_csv('Data/Nodes/Protein_Properties.csv',\n                          sep=',', index=False)\n\n        # Return the DataFrame\n        return protein_df\n\n    def fetch_data(self, cid):\n        \"\"\"\n        Retrieves detailed chemical compound properties for a specified\n        Compound ID (CID) from the PubChem database.\n\n        This method constructs a query URL to fetch a wide range of properties\n        for the given CID from PubChem, including molecular formula,\n        molecular weight, canonical and isomeric SMILES, InChI codes,\n        physicochemical properties, and more. If the CID is valid and data is\n        available, it returns a pandas DataFrame containing these properties. This\n        method also generates a URL to retrieve the structure image of the\n        compound as a 2D PNG image, adding it as a column in the DataFrame.\n        In cases where the CID is NaN or an error occurs during data retrieval,\n        an empty DataFrame is returned.\n\n        Parameters:\n            cid (int or float): The Compound ID for which to fetch data.\n            Can be an integer or NaN.\n\n        Returns:\n            pd.DataFrame: A DataFrame containing the fetched properties for the\n            given CID. The DataFrame includes columns for each property fetched\n            from PubChem, along with a 'StructureImage2DURL' column containing\n            the URL to the compound's structure image. Returns an empty DataFrame\n            if the CID is NaN or if any error occurs during the fetch operation.\n\n        Raises:\n            Exception: Logs an error message if the request to PubChem fails or\n            if the response cannot be processed into a DataFrame.\n\n        Example:\n            &gt;&gt;&gt; extractor = NodePropertiesExtractor(['CYP2D6', 'CYP3A4'])\n            &gt;&gt;&gt; extractor.create_data_directories()\n            &gt;&gt;&gt; compound_data_df = extractor.fetch_data(2244)\n            &gt;&gt;&gt; print(compound_data_df.head())\n\n            This example fetches the properties for the compound with CID 2244\n            from PubChem and prints the first few rows\n            of the resulting DataFrame.\n\n        Note:\n            This method requires an active internet connection to access the\n            PubChem database. Ensure that the CID provided is valid and not NaN\n            to avoid fetching errors. The structure and availability of data\n            fields are subject to the current state of the PubChem database\n            and may vary.\n        \"\"\"\n        if pd.isna(cid):\n            return pd.DataFrame()  # Return an empty DataFrame for NaN CIDs\n\n        cid = int(cid)  # Convert CID to integer\n        url = (f\"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/cid/{cid}/property/\"\n               \"MolecularFormula,MolecularWeight,CanonicalSMILES,IsomericSMILES,InChI,\"\n               \"InChIKey,IUPACName,Title,XLogP,ExactMass,MonoisotopicMass,TPSA,Complexity,\"\n               \"Charge,HBondDonorCount,HBondAcceptorCount,RotatableBondCount,HeavyAtomCount,\"\n               \"IsotopeAtomCount,AtomStereoCount,DefinedAtomStereoCount,UndefinedAtomStereoCount,\"\n               \"BondStereoCount,DefinedBondStereoCount,UndefinedBondStereoCount,CovalentUnitCount,\"\n               \"PatentCount,PatentFamilyCount,LiteratureCount,Volume3D,XStericQuadrupole3D,\"\n               \"YStericQuadrupole3D,ZStericQuadrupole3D,FeatureCount3D,FeatureAcceptorCount3D,\"\n               \"FeatureDonorCount3D,FeatureAnionCount3D,FeatureCationCount3D,FeatureRingCount3D,\"\n               \"FeatureHydrophobeCount3D,ConformerModelRMSD3D,EffectiveRotorCount3D,ConformerCount3D,\"\n               \"Fingerprint2D/CSV\")\n        try:\n            response = requests.get(url)\n            response.raise_for_status()\n            compound_data = pd.read_csv(StringIO(response.text),\n                                        sep=',', low_memory=False)\n            compound_data['StructureImage2DURL'] = f\"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/cid/{cid}/PNG\"\n            return compound_data\n        except Exception as e:\n            logging.error(f\"Error processing CID {cid}: {e}\")\n            return pd.DataFrame()  # Return an empty DataFrame in case of error\n\n    def extract_compound_properties(self, main_data, start_chunk=0):\n        \"\"\"\n        Extracts and aggregates compound properties from PubChem for a list of\n        compounds associated with specific genes.\n\n        This method processes a CSV file specified by `main_data`, which contains\n        gene identifiers and their associated compound IDs (CIDs). It selects\n        compounds related to the top `n` most frequently occurring genes in the\n        dataset, where `n` is determined by the instance's `_enzyme_count`\n        attribute. The method then fetches detailed compound properties from\n        PubChem in chunks, using concurrent requests to improve efficiency and\n        manage the load on the PubChem API. The fetched compound properties are\n        aggregated into a single DataFrame and saved to multiple CSV files,\n        one for each chunk of compound IDs processed.\n\n        Parameters:\n            main_data (str): Path to a CSV file containing main data was which\n            generated after running `extractor.run()`.\n\n        Side Effects:\n            - Saves the aggregated compound properties to CSV files in the current\n            working directory. The files are named\n            'Data/Nodes/Compound_Properties/Chunk_{i}.csv', where `{i}` is\n            the chunk index.\n\n        Returns:\n            None: This method does not return a value. Instead, it saves the\n            fetched compound data directly to CSV files.\n\n        Raises:\n            Exception: Logs an error and continues processing the next CID if\n            an error occurs while fetching data for a specific CID.\n\n        Example:\n            &gt;&gt;&gt; extractor = NodePropertiesExtractor(['CYP2D6', 'CYP3A4'])\n            &gt;&gt;&gt; extractor.create_data_directories()\n            &gt;&gt;&gt; extractor.extract_compound_properties('Data/AllDataConnected.csv')\n            This will read 'Data/AllDataConnected.csv', filter for compounds\n            associated with the top n genes, fetch their properties from PubChem,\n            and save the results into multiple CSV files for each chunk\n            of compounds processed.\n\n        Note:\n            - Ensure that the 'main_data' CSV file exists and is accessible at\n            the specified path.\n            - The method automatically handles NaN values in the 'CID' column\n            and excludes them from processing.\n            - The `enzyme_count` attribute determines the number of top genes\n            for which compound properties will be fetched.\n            - Internet access is required to fetch compound data from the PubChem API.\n            - The method employs a `ThreadPoolExecutor` with a configurable\n            number of workers (default is len(enzyme_list)) to parallelize\n            requests, which can be adjusted based on system capabilities and\n            API rate limits.\n        \"\"\"\n\n        n = self._enzyme_count\n        df = pd.read_csv(main_data)\n        gene_ids = df['Target GeneID'].value_counts().head(n).index.tolist()\n        df = df[df['Target GeneID'].isin([int(item) for item in gene_ids])]\n        df = df.dropna(subset=['CID'])\n        IDs = df['CID'].unique().tolist()\n\n        # Define chunk size and calculate number of chunks\n        chunk_size = 10000\n        num_chunks = math.ceil(len(IDs) / chunk_size)\n\n        if num_chunks &gt;= start_chunk:\n            for i in range(start_chunk, num_chunks):\n                # Calculate start and end indices for each chunk\n                start_index = i * chunk_size\n                end_index = start_index + chunk_size\n\n                # Extract chunk of CIDs\n                chunk_cids = IDs[start_index:end_index]\n                # chunk_cids = [x for x in chunk_cids if not np.isnan(x)]\n\n                # Use ThreadPoolExecutor to parallelize requests for the chunk\n                with ThreadPoolExecutor(max_workers=5) as executor:\n                    future_to_cid = {executor.submit(self.fetch_data, cid): cid for cid in chunk_cids}\n                    results = []\n\n                    for future in as_completed(future_to_cid):\n                        cid = future_to_cid[future]\n                        try:\n                            data = future.result()\n                            results.append(data)\n                        except Exception as e:\n                            logging.error(f\"Error processing CID {cid}: {e}\")\n\n                # Concatenate results for the current chunk\n                chunk_df = pd.concat(results, ignore_index=True)\n\n                # Save the concatenated DataFrame to a CSV file for the chunk\n                chunk_df.to_csv(f'Data/Nodes/Compound_Properties/Chunk_{i}.csv',\n                                sep=',', index=False)\n        else:\n            logging.info(\"No more chunks to process.\")\n</code></pre>"},{"location":"documentation/#chemgraphbuilder.node_properties_extractor.NodePropertiesExtractor.__init__","title":"<code>__init__(enzyme_list, base_url='https://pubchem.ncbi.nlm.nih.gov/rest/pug/assay/target/genesymbol', sep=',')</code>","text":"<p>Initializes a NodePropertiesExtractor instance, setting up the base URL for API requests, the separator for CSV parsing, and the list of enzymes to query from the PubChem database.</p> <p>Parameters:</p> Name Type Description Default <code>enzyme_list</code> <code>list of str</code> <p>A list of enzyme names for which to fetch</p> required <code>base_url</code> <code>str</code> <p>The base URL for PubChem API requests.</p> <code>'https://pubchem.ncbi.nlm.nih.gov/rest/pug/assay/target/genesymbol'</code> <code>sep</code> <code>str</code> <p>The delimiter to use for parsing CSV files</p> <code>','</code> <p>Attributes:</p> Name Type Description <code>_base_url</code> <code>str</code> <p>Stores the base URL for API requests.</p> <code>_sep</code> <code>str</code> <p>Stores the delimiter for parsing CSV data.</p> <code>enzyme_list</code> <code>list of str</code> <p>Stores the list of enzyme names provided</p> <code>_enzyme_count</code> <code>int</code> <p>The number of enzymes in the enzyme_list.</p> Source code in <code>chemgraphbuilder/node_properties_extractor.py</code> <pre><code>def __init__(self, enzyme_list,\n             base_url=\"https://pubchem.ncbi.nlm.nih.gov/rest/pug/assay/target/genesymbol\",\n             sep=\",\"):\n    \"\"\"\n    Initializes a NodePropertiesExtractor instance, setting up the base URL\n    for API requests, the separator for CSV parsing, and the list of enzymes\n    to query from the PubChem database.\n\n    Parameters:\n        enzyme_list (list of str): A list of enzyme names for which to fetch\n        assay data.\n        base_url (str, optional): The base URL for PubChem API requests.\n        Default is set to the assay target genesymbol endpoint.\n        sep (str, optional): The delimiter to use for parsing CSV files\n        returned by PubChem. Defaults to ','.\n\n    Attributes:\n        _base_url (str): Stores the base URL for API requests.\n        _sep (str): Stores the delimiter for parsing CSV data.\n        enzyme_list (list of str): Stores the list of enzyme names provided\n        during initialization.\n        _enzyme_count (int): The number of enzymes in the enzyme_list.\n    \"\"\"\n    self._base_url = base_url\n    self._sep = sep\n    self.enzyme_list = enzyme_list\n    self._enzyme_count = len(enzyme_list)\n</code></pre>"},{"location":"documentation/#chemgraphbuilder.node_properties_extractor.NodePropertiesExtractor.extract_assay_properties","title":"<code>extract_assay_properties(main_data)</code>","text":"<p>Extracts detailed properties of assays from PubChem for each unique assay ID found in the input data file.</p> <p>This method processes an input CSV file containing assay IDs (AID) and performs concurrent HTTP requests to fetch detailed assay properties from the PubChem database. The retrieved details include assay type, activity name, source name, source ID, name, and description. These properties are compiled into a new DataFrame, which is then saved to a CSV file for further analysis or use.</p> <p>The method employs a ThreadPoolExecutor to manage concurrent requests efficiently, improving the performance when dealing with a large number of assay IDs. Errors encountered during data fetching are logged, and the process continues with the next assay ID, ensuring the method's robustness.</p> <p>Parameters:</p> Name Type Description Default <code>main_data</code> <code>str</code> <p>Path to a CSV file containing main data was which</p> required <p>Returns:</p> Type Description <p>pd.DataFrame: A DataFrame containing the fetched assay properties,</p> <p>including columns for AID, Assay Type, Activity Name, SourceName,</p> <p>SourceID, Name, and Description. This DataFrame is saved to</p> <p>'Data/Nodes/Assay_Properties.csv' in the current working directory.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input CSV file is empty or does not contain the 'AID' column.</p> Example <p>extractor = NodePropertiesExtractor(['CYP2D6', 'CYP3A4']) extractor.create_data_directories() extractor.run() assay_properties_df = extractor.extract_assay_properties('Data/AllDataConnected.csv') print(assay_properties_df.head())</p> <p>This example reads assay IDs from 'Data/AllDataConnected.csv', queries PubChem for their detailed properties, and compiles the results into a DataFrame, which is also saved to 'Data/Nodes/Assay_Properties.csv'.</p> Note <p>This method requires network access to the PubChem API and assumes the availability of a valid 'AID' column in the input CSV file. Ensure the input file path is correct and accessible to avoid errors during processing.</p> Source code in <code>chemgraphbuilder/node_properties_extractor.py</code> <pre><code>def extract_assay_properties(self, main_data):\n    \"\"\"\n    Extracts detailed properties of assays from PubChem for each unique assay\n    ID found in the input data file.\n\n    This method processes an input CSV file containing assay IDs (AID) and\n    performs concurrent HTTP requests to fetch detailed assay properties\n    from the PubChem database. The retrieved details include assay type,\n    activity name, source name, source ID, name, and description. These\n    properties are compiled into a new DataFrame, which is then\n    saved to a CSV file for further analysis or use.\n\n    The method employs a ThreadPoolExecutor to manage concurrent requests\n    efficiently, improving the performance when dealing with a large number\n    of assay IDs. Errors encountered during data fetching are logged, and the\n    process continues with the next assay ID, ensuring the method's robustness.\n\n    Parameters:\n        main_data (str): Path to a CSV file containing main data was which\n        generated after running `extractor.run()`.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the fetched assay properties,\n        including columns for AID, Assay Type, Activity Name, SourceName,\n        SourceID, Name, and Description. This DataFrame is saved to\n        'Data/Nodes/Assay_Properties.csv' in the current working directory.\n\n    Raises:\n        ValueError: If the input CSV file is empty or does not contain the 'AID' column.\n\n    Example:\n        &gt;&gt;&gt; extractor = NodePropertiesExtractor(['CYP2D6', 'CYP3A4'])\n        &gt;&gt;&gt; extractor.create_data_directories()\n        &gt;&gt;&gt; extractor.run()\n        &gt;&gt;&gt; assay_properties_df = extractor.extract_assay_properties('Data/AllDataConnected.csv')\n        &gt;&gt;&gt; print(assay_properties_df.head())\n\n        This example reads assay IDs from 'Data/AllDataConnected.csv',\n        queries PubChem for their detailed properties, and compiles the\n        results into a DataFrame, which is also saved to 'Data/Nodes/Assay_Properties.csv'.\n\n    Note:\n        This method requires network access to the PubChem API and assumes\n        the availability of a valid 'AID' column in the input CSV file.\n        Ensure the input file path is correct and accessible to avoid errors during processing.\n    \"\"\"\n\n    df = pd.read_csv(main_data)\n\n    # Check if the DataFrame is valid\n    if df.empty or 'AID' not in df.columns:\n        logging.error(\"DataFrame is empty or does not contain 'AID' column.\")\n        return pd.DataFrame()\n\n    unique_aids = df['AID'].unique().tolist()  # Extracting unique assay IDs\n\n    columns = ['AID', 'Assay Type', 'Activity Name', 'SourceName',\n               'SourceID', 'Name', 'Description', 'Protocol',\n               'Comment', 'Method', 'Target', 'CIDCountAll',\n               'CIDCountActive', 'CIDCountInactive', 'CIDCountInconclusive',\n               'CIDCountUnspecified', 'CIDCountProbe']\n    assay_df = pd.DataFrame(columns=columns)  # Initializing a DataFrame to store assay properties\n\n    # Using ThreadPoolExecutor for concurrent fetching of assay details\n    with concurrent.futures.ThreadPoolExecutor(max_workers=self._CONCURRENT_REQUEST_LIMIT) as executor:\n        future_to_aid = {executor.submit(self._fetch_assay_details, aid): aid for aid in unique_aids}\n\n        # Iterating over completed futures\n        for future in concurrent.futures.as_completed(future_to_aid):\n            aid = future_to_aid[future]\n            try:\n                assay_data = future.result()  # Fetching the result from the future\n                if assay_data:\n                    # Preparing a new row with the fetched data\n                    new_row = {\n                        'AID': aid,\n                        'Assay Type': df.loc[df['AID'] == aid, 'Assay Type'].iloc[0],\n                        'Activity Name': df.loc[df['AID'] == aid, 'Activity Name'].iloc[0],\n                        **assay_data\n                    }\n                    # Adding the new row to the DataFrame\n                    assay_df = pd.concat([assay_df, pd.DataFrame([new_row])], ignore_index=True)\n            except Exception as exc:\n                # Logging any errors encountered during the fetch\n                logging.error(f\"Error occurred while processing AID {aid}: {exc}\")\n\n    # Saving the updated DataFrame to a CSV file\n    assay_df.to_csv('Data/Nodes/Assay_Properties.csv', sep=',', index=False)\n    return assay_df\n</code></pre>"},{"location":"documentation/#chemgraphbuilder.node_properties_extractor.NodePropertiesExtractor.extract_compound_properties","title":"<code>extract_compound_properties(main_data, start_chunk=0)</code>","text":"<p>Extracts and aggregates compound properties from PubChem for a list of compounds associated with specific genes.</p> <p>This method processes a CSV file specified by <code>main_data</code>, which contains gene identifiers and their associated compound IDs (CIDs). It selects compounds related to the top <code>n</code> most frequently occurring genes in the dataset, where <code>n</code> is determined by the instance's <code>_enzyme_count</code> attribute. The method then fetches detailed compound properties from PubChem in chunks, using concurrent requests to improve efficiency and manage the load on the PubChem API. The fetched compound properties are aggregated into a single DataFrame and saved to multiple CSV files, one for each chunk of compound IDs processed.</p> <p>Parameters:</p> Name Type Description Default <code>main_data</code> <code>str</code> <p>Path to a CSV file containing main data was which</p> required Side Effects <ul> <li>Saves the aggregated compound properties to CSV files in the current working directory. The files are named 'Data/Nodes/Compound_Properties/Chunk_{i}.csv', where <code>{i}</code> is the chunk index.</li> </ul> <p>Returns:</p> Name Type Description <code>None</code> <p>This method does not return a value. Instead, it saves the</p> <p>fetched compound data directly to CSV files.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>Logs an error and continues processing the next CID if</p> Example <p>extractor = NodePropertiesExtractor(['CYP2D6', 'CYP3A4']) extractor.create_data_directories() extractor.extract_compound_properties('Data/AllDataConnected.csv') This will read 'Data/AllDataConnected.csv', filter for compounds associated with the top n genes, fetch their properties from PubChem, and save the results into multiple CSV files for each chunk of compounds processed.</p> Note <ul> <li>Ensure that the 'main_data' CSV file exists and is accessible at the specified path.</li> <li>The method automatically handles NaN values in the 'CID' column and excludes them from processing.</li> <li>The <code>enzyme_count</code> attribute determines the number of top genes for which compound properties will be fetched.</li> <li>Internet access is required to fetch compound data from the PubChem API.</li> <li>The method employs a <code>ThreadPoolExecutor</code> with a configurable number of workers (default is len(enzyme_list)) to parallelize requests, which can be adjusted based on system capabilities and API rate limits.</li> </ul> Source code in <code>chemgraphbuilder/node_properties_extractor.py</code> <pre><code>def extract_compound_properties(self, main_data, start_chunk=0):\n    \"\"\"\n    Extracts and aggregates compound properties from PubChem for a list of\n    compounds associated with specific genes.\n\n    This method processes a CSV file specified by `main_data`, which contains\n    gene identifiers and their associated compound IDs (CIDs). It selects\n    compounds related to the top `n` most frequently occurring genes in the\n    dataset, where `n` is determined by the instance's `_enzyme_count`\n    attribute. The method then fetches detailed compound properties from\n    PubChem in chunks, using concurrent requests to improve efficiency and\n    manage the load on the PubChem API. The fetched compound properties are\n    aggregated into a single DataFrame and saved to multiple CSV files,\n    one for each chunk of compound IDs processed.\n\n    Parameters:\n        main_data (str): Path to a CSV file containing main data was which\n        generated after running `extractor.run()`.\n\n    Side Effects:\n        - Saves the aggregated compound properties to CSV files in the current\n        working directory. The files are named\n        'Data/Nodes/Compound_Properties/Chunk_{i}.csv', where `{i}` is\n        the chunk index.\n\n    Returns:\n        None: This method does not return a value. Instead, it saves the\n        fetched compound data directly to CSV files.\n\n    Raises:\n        Exception: Logs an error and continues processing the next CID if\n        an error occurs while fetching data for a specific CID.\n\n    Example:\n        &gt;&gt;&gt; extractor = NodePropertiesExtractor(['CYP2D6', 'CYP3A4'])\n        &gt;&gt;&gt; extractor.create_data_directories()\n        &gt;&gt;&gt; extractor.extract_compound_properties('Data/AllDataConnected.csv')\n        This will read 'Data/AllDataConnected.csv', filter for compounds\n        associated with the top n genes, fetch their properties from PubChem,\n        and save the results into multiple CSV files for each chunk\n        of compounds processed.\n\n    Note:\n        - Ensure that the 'main_data' CSV file exists and is accessible at\n        the specified path.\n        - The method automatically handles NaN values in the 'CID' column\n        and excludes them from processing.\n        - The `enzyme_count` attribute determines the number of top genes\n        for which compound properties will be fetched.\n        - Internet access is required to fetch compound data from the PubChem API.\n        - The method employs a `ThreadPoolExecutor` with a configurable\n        number of workers (default is len(enzyme_list)) to parallelize\n        requests, which can be adjusted based on system capabilities and\n        API rate limits.\n    \"\"\"\n\n    n = self._enzyme_count\n    df = pd.read_csv(main_data)\n    gene_ids = df['Target GeneID'].value_counts().head(n).index.tolist()\n    df = df[df['Target GeneID'].isin([int(item) for item in gene_ids])]\n    df = df.dropna(subset=['CID'])\n    IDs = df['CID'].unique().tolist()\n\n    # Define chunk size and calculate number of chunks\n    chunk_size = 10000\n    num_chunks = math.ceil(len(IDs) / chunk_size)\n\n    if num_chunks &gt;= start_chunk:\n        for i in range(start_chunk, num_chunks):\n            # Calculate start and end indices for each chunk\n            start_index = i * chunk_size\n            end_index = start_index + chunk_size\n\n            # Extract chunk of CIDs\n            chunk_cids = IDs[start_index:end_index]\n            # chunk_cids = [x for x in chunk_cids if not np.isnan(x)]\n\n            # Use ThreadPoolExecutor to parallelize requests for the chunk\n            with ThreadPoolExecutor(max_workers=5) as executor:\n                future_to_cid = {executor.submit(self.fetch_data, cid): cid for cid in chunk_cids}\n                results = []\n\n                for future in as_completed(future_to_cid):\n                    cid = future_to_cid[future]\n                    try:\n                        data = future.result()\n                        results.append(data)\n                    except Exception as e:\n                        logging.error(f\"Error processing CID {cid}: {e}\")\n\n            # Concatenate results for the current chunk\n            chunk_df = pd.concat(results, ignore_index=True)\n\n            # Save the concatenated DataFrame to a CSV file for the chunk\n            chunk_df.to_csv(f'Data/Nodes/Compound_Properties/Chunk_{i}.csv',\n                            sep=',', index=False)\n    else:\n        logging.info(\"No more chunks to process.\")\n</code></pre>"},{"location":"documentation/#chemgraphbuilder.node_properties_extractor.NodePropertiesExtractor.extract_gene_properties","title":"<code>extract_gene_properties(main_data)</code>","text":"<p>Extracts and processes gene properties from a given data source, specifically targeting genes relevant to the study (e.g., CYP enzymes) and records their details in a structured DataFrame.</p> <p>This method reads gene data from a CSV file specified by <code>main_data</code>, queries the PubChem database for additional properties of each unique gene ID found in the file, and compiles these properties into a new DataFrame. It focuses on fetching details like gene symbols, taxonomy, taxonomy IDs, and synonyms for each gene. The final DataFrame is filtered to include only genes of particular interest (e.g., certain CYP enzymes) and saved to a separate CSV file for further analysis or use.</p> <p>Parameters:</p> Name Type Description Default <code>main_data</code> <code>str</code> <p>Path to a CSV file containing main data was which</p> required <p>Returns:</p> Type Description <p>pd.DataFrame: A DataFrame containing the compiled gene properties,</p> <p>including GeneID, Symbol, Taxonomy, Taxonomy ID, and Synonyms,</p> <p>filtered to include only specified genes of interest. This DataFrame</p> <p>is also saved to 'Data/Nodes/Gene_Properties.csv'.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there's an issue reading the initial CSV file or</p> Example <p>extractor = NodePropertiesExtractor(['CYP2D6', 'CYP3A4']) extractor.create_data_directories() extractor.run() gene_properties_df = extractor.extract_gene_properties('Data/AllDataConnected.csv') print(gene_properties_df.head())</p> <p>This would read gene IDs from 'Data/AllDataConnected.csv', fetch their properties from PubChem, and compile the details into a DataFrame, filtering for specified genes of interest and saving the results to 'Data/Nodes/Gene_Properties.csv'.</p> Note <p>The method filters the resulting DataFrame to include only genes with symbols in the predefined enzyme_list. Adjust this list as necessary to match the focus of your study or application.</p> Source code in <code>chemgraphbuilder/node_properties_extractor.py</code> <pre><code>def extract_gene_properties(self, main_data):\n    \"\"\"\n    Extracts and processes gene properties from a given data source,\n    specifically targeting genes relevant to the study (e.g., CYP enzymes)\n    and records their details in a structured DataFrame.\n\n    This method reads gene data from a CSV file specified by `main_data`,\n    queries the PubChem database for additional properties of each unique\n    gene ID found in the file, and compiles these properties into a new\n    DataFrame. It focuses on fetching details like gene symbols, taxonomy,\n    taxonomy IDs, and synonyms for each gene. The final DataFrame is filtered\n    to include only genes of particular interest (e.g., certain CYP enzymes)\n    and saved to a separate CSV file for further analysis or use.\n\n    Parameters:\n        main_data (str): Path to a CSV file containing main data was which\n        generated after running `extractor.run()`.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the compiled gene properties,\n        including GeneID, Symbol, Taxonomy, Taxonomy ID, and Synonyms,\n        filtered to include only specified genes of interest. This DataFrame\n        is also saved to 'Data/Nodes/Gene_Properties.csv'.\n\n    Raises:\n        Exception: If there's an issue reading the initial CSV file or\n        fetching gene details from PubChem, details of the exception are\n        logged, and the method proceeds to process the next gene ID.\n\n    Example:\n        &gt;&gt;&gt; extractor = NodePropertiesExtractor(['CYP2D6', 'CYP3A4'])\n        &gt;&gt;&gt; extractor.create_data_directories()\n        &gt;&gt;&gt; extractor.run()\n        &gt;&gt;&gt; gene_properties_df = extractor.extract_gene_properties('Data/AllDataConnected.csv')\n        &gt;&gt;&gt; print(gene_properties_df.head())\n\n        This would read gene IDs from 'Data/AllDataConnected.csv', fetch\n        their properties from PubChem, and compile the details into a\n        DataFrame, filtering for specified genes of interest and saving\n        the results to 'Data/Nodes/Gene_Properties.csv'.\n\n    Note:\n        The method filters the resulting DataFrame to include only genes with\n        symbols in the predefined enzyme_list. Adjust this list as necessary\n        to match the focus of your study or application.\n    \"\"\"\n    df = pd.read_csv(main_data)\n    df_gene = pd.DataFrame(columns=['GeneID', 'Symbol', 'Taxonomy',\n                                    'Taxonomy ID', 'Synonyms'])\n\n    unique_gene_ids = df['Target GeneID'].unique().tolist()\n\n    gene_details = []\n\n    for gene_id in unique_gene_ids:\n        try:\n            gene_id, symbol, taxonomy, taxonomy_id, synonyms = self._fetch_gene_details(gene_id)\n            gene_details.append({\n                'GeneID': gene_id,\n                'Symbol': symbol,\n                'Taxonomy': taxonomy,\n                'Taxonomy ID': taxonomy_id,\n                'Synonyms': str(synonyms)\n            })\n        except Exception as exc:\n            logging.error(\"Error occurred while processing gene_id %s: %s\",\n                          gene_id, exc)\n            gene_details.append({\n                'GeneID': gene_id,\n                'Symbol': None,\n                'Taxonomy': None,\n                'Taxonomy ID': None,\n                'Synonyms': None\n            })\n\n    # Now create the DataFrame from the list of dictionaries\n    df_gene = pd.DataFrame(gene_details)\n    n = self._enzyme_count\n    gene_ids = df['Target GeneID'].value_counts().head(n).index.tolist()\n    df_gene = df_gene[df_gene['GeneID'].isin([int(item) for item in gene_ids])]\n    df_gene.to_csv('Data/Nodes/Gene_Properties.csv', sep=',', index=False)\n    return df_gene\n</code></pre>"},{"location":"documentation/#chemgraphbuilder.node_properties_extractor.NodePropertiesExtractor.extract_protein_properties","title":"<code>extract_protein_properties(main_data)</code>","text":"<p>Extracts and compiles protein properties from the NCBI protein database based on accession numbers.</p> <p>Given a CSV file specified by <code>main_data</code>, this method reads protein accession numbers and performs web scraping on the NCBI protein database pages to extract protein titles. The method constructs a URL for each accession number, sends a request to retrieve the page content, and parses the HTML to find the protein title. The extracted titles, along with their corresponding accession numbers and URLs, are compiled into a DataFrame. This DataFrame is saved to a CSV file, providing a structured summary of protein properties for further analysis or use.</p> <p>Parameters:</p> Name Type Description Default <code>main_data</code> <code>str</code> <p>Path to a CSV file containing main data was which</p> required <p>Returns:</p> Type Description <p>pd.DataFrame: A DataFrame with columns 'RefSeq Accession', 'URL',</p> <p>and 'Description', where 'Description' contains the title of the</p> <p>protein extracted from its NCBI page. This DataFrame is saved to</p> <p>'Data/Nodes/Protein_Properties.csv' in the current working directory.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there's an issue reading the initial CSV file or</p> Example <p>Assuming 'protein_data.csv' contains a column 'Target Accession' with accession numbers:</p> <p>extractor = NodePropertiesExtractor(['CYP2D6', 'CYP3A4']) extractor.create_data_directories() extractor.run() # you need to run this only once protein_properties_df = extractor.extract_protein_properties('Data/AllDataConnected.csv') print(protein_properties_df.head())</p> <p>This would read accession numbers from 'Data/AllDataConnected.csv', scrape their titles from the NCBI protein database, and compile the results into a DataFrame, which is also saved to 'Data/Nodes/Protein_Properties.csv'.</p> Note <p>This method requires internet access to query the NCBI protein database. Ensure the input file path is correct and accessible to avoid errors during processing. Web scraping is dependent on the structure of the web page; changes to the NCBI protein database pages may require updates to the scraping logic.</p> Source code in <code>chemgraphbuilder/node_properties_extractor.py</code> <pre><code>def extract_protein_properties(self, main_data):\n    \"\"\"\n    Extracts and compiles protein properties from the NCBI protein database\n    based on accession numbers.\n\n    Given a CSV file specified by `main_data`, this method reads protein\n    accession numbers and performs web scraping on the NCBI protein database\n    pages to extract protein titles. The method constructs a URL for\n    each accession number, sends a request to retrieve the page content,\n    and parses the HTML to find the protein title. The extracted titles,\n    along with their corresponding accession numbers and URLs, are\n    compiled into a DataFrame. This DataFrame is saved to a CSV file,\n    providing a structured summary of protein properties for further analysis or use.\n\n    Parameters:\n        main_data (str): Path to a CSV file containing main data was which\n        generated after running `extractor.run()`.\n\n    Returns:\n        pd.DataFrame: A DataFrame with columns 'RefSeq Accession', 'URL',\n        and 'Description', where 'Description' contains the title of the\n        protein extracted from its NCBI page. This DataFrame is saved to\n        'Data/Nodes/Protein_Properties.csv' in the current working directory.\n\n    Raises:\n        Exception: If there's an issue reading the initial CSV file or\n        querying the NCBI database, details of the exception are logged.\n        The method continues processing the next accession number,\n        ensuring robustness against individual failures.\n\n    Example:\n        Assuming 'protein_data.csv' contains a column 'Target Accession'\n        with accession numbers:\n\n        &gt;&gt;&gt; extractor = NodePropertiesExtractor(['CYP2D6', 'CYP3A4'])\n        &gt;&gt;&gt; extractor.create_data_directories()\n        &gt;&gt;&gt; extractor.run() # you need to run this only once\n        &gt;&gt;&gt; protein_properties_df = extractor.extract_protein_properties('Data/AllDataConnected.csv')\n        &gt;&gt;&gt; print(protein_properties_df.head())\n\n        This would read accession numbers from 'Data/AllDataConnected.csv',\n        scrape their titles from the NCBI protein database, and compile the\n        results into a DataFrame, which is also saved to\n        'Data/Nodes/Protein_Properties.csv'.\n\n    Note:\n        This method requires internet access to query the NCBI protein\n        database. Ensure the input file path is correct and accessible to\n        avoid errors during processing. Web scraping is dependent on the\n        structure of the web page; changes to the NCBI protein database\n        pages may require updates to the scraping logic.\n    \"\"\"\n\n    # Initialize a list to store the extracted data\n    data = []\n\n    n = self._enzyme_count\n    df = pd.read_csv(main_data)\n    gene_ids = df['Target GeneID'].value_counts().head(n).index.tolist()\n    df = df[df['Target GeneID'].isin([int(item) for item in gene_ids])]\n    Accessions = df['Target Accession'].unique().tolist()\n    # Iterate over each protein accession number in the DataFrame\n    for accession in Accessions:\n        # Construct the URL to query the NCBI protein database\n        url = f\"https://www.ncbi.nlm.nih.gov/protein/{accession}\"\n\n        try:\n            # Send an HTTP request to the URL\n            response = requests.get(url)\n\n            # Parse the HTML content of the response\n            soup = BeautifulSoup(response.text, 'html.parser')\n\n            # Extract the title from the parsed HTML\n            title = soup.title.string if soup.title else 'Title Not Found'\n\n            # Append the extracted data to the list\n            data.append({'RefSeq Accession': accession,\n                         'URL': url, 'Description': title})\n        except Exception as e:\n            # In case of an error, log the error message\n            logging.error(f\"Error fetching data for accession {accession}: {e}\")\n            data.append({'RefSeq Accession': accession, 'URL': url,\n                         'Description': f'Error: {e}'})\n\n    # Convert the list of data into a DataFrame\n    protein_df = pd.DataFrame(data)\n\n    # Save the DataFrame to a CSV file\n    protein_df.to_csv('Data/Nodes/Protein_Properties.csv',\n                      sep=',', index=False)\n\n    # Return the DataFrame\n    return protein_df\n</code></pre>"},{"location":"documentation/#chemgraphbuilder.node_properties_extractor.NodePropertiesExtractor.fetch_data","title":"<code>fetch_data(cid)</code>","text":"<p>Retrieves detailed chemical compound properties for a specified Compound ID (CID) from the PubChem database.</p> <p>This method constructs a query URL to fetch a wide range of properties for the given CID from PubChem, including molecular formula, molecular weight, canonical and isomeric SMILES, InChI codes, physicochemical properties, and more. If the CID is valid and data is available, it returns a pandas DataFrame containing these properties. This method also generates a URL to retrieve the structure image of the compound as a 2D PNG image, adding it as a column in the DataFrame. In cases where the CID is NaN or an error occurs during data retrieval, an empty DataFrame is returned.</p> <p>Parameters:</p> Name Type Description Default <code>cid</code> <code>int or float</code> <p>The Compound ID for which to fetch data.</p> required <p>Returns:</p> Type Description <p>pd.DataFrame: A DataFrame containing the fetched properties for the</p> <p>given CID. The DataFrame includes columns for each property fetched</p> <p>from PubChem, along with a 'StructureImage2DURL' column containing</p> <p>the URL to the compound's structure image. Returns an empty DataFrame</p> <p>if the CID is NaN or if any error occurs during the fetch operation.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>Logs an error message if the request to PubChem fails or</p> Example <p>extractor = NodePropertiesExtractor(['CYP2D6', 'CYP3A4']) extractor.create_data_directories() compound_data_df = extractor.fetch_data(2244) print(compound_data_df.head())</p> <p>This example fetches the properties for the compound with CID 2244 from PubChem and prints the first few rows of the resulting DataFrame.</p> Note <p>This method requires an active internet connection to access the PubChem database. Ensure that the CID provided is valid and not NaN to avoid fetching errors. The structure and availability of data fields are subject to the current state of the PubChem database and may vary.</p> Source code in <code>chemgraphbuilder/node_properties_extractor.py</code> <pre><code>def fetch_data(self, cid):\n    \"\"\"\n    Retrieves detailed chemical compound properties for a specified\n    Compound ID (CID) from the PubChem database.\n\n    This method constructs a query URL to fetch a wide range of properties\n    for the given CID from PubChem, including molecular formula,\n    molecular weight, canonical and isomeric SMILES, InChI codes,\n    physicochemical properties, and more. If the CID is valid and data is\n    available, it returns a pandas DataFrame containing these properties. This\n    method also generates a URL to retrieve the structure image of the\n    compound as a 2D PNG image, adding it as a column in the DataFrame.\n    In cases where the CID is NaN or an error occurs during data retrieval,\n    an empty DataFrame is returned.\n\n    Parameters:\n        cid (int or float): The Compound ID for which to fetch data.\n        Can be an integer or NaN.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the fetched properties for the\n        given CID. The DataFrame includes columns for each property fetched\n        from PubChem, along with a 'StructureImage2DURL' column containing\n        the URL to the compound's structure image. Returns an empty DataFrame\n        if the CID is NaN or if any error occurs during the fetch operation.\n\n    Raises:\n        Exception: Logs an error message if the request to PubChem fails or\n        if the response cannot be processed into a DataFrame.\n\n    Example:\n        &gt;&gt;&gt; extractor = NodePropertiesExtractor(['CYP2D6', 'CYP3A4'])\n        &gt;&gt;&gt; extractor.create_data_directories()\n        &gt;&gt;&gt; compound_data_df = extractor.fetch_data(2244)\n        &gt;&gt;&gt; print(compound_data_df.head())\n\n        This example fetches the properties for the compound with CID 2244\n        from PubChem and prints the first few rows\n        of the resulting DataFrame.\n\n    Note:\n        This method requires an active internet connection to access the\n        PubChem database. Ensure that the CID provided is valid and not NaN\n        to avoid fetching errors. The structure and availability of data\n        fields are subject to the current state of the PubChem database\n        and may vary.\n    \"\"\"\n    if pd.isna(cid):\n        return pd.DataFrame()  # Return an empty DataFrame for NaN CIDs\n\n    cid = int(cid)  # Convert CID to integer\n    url = (f\"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/cid/{cid}/property/\"\n           \"MolecularFormula,MolecularWeight,CanonicalSMILES,IsomericSMILES,InChI,\"\n           \"InChIKey,IUPACName,Title,XLogP,ExactMass,MonoisotopicMass,TPSA,Complexity,\"\n           \"Charge,HBondDonorCount,HBondAcceptorCount,RotatableBondCount,HeavyAtomCount,\"\n           \"IsotopeAtomCount,AtomStereoCount,DefinedAtomStereoCount,UndefinedAtomStereoCount,\"\n           \"BondStereoCount,DefinedBondStereoCount,UndefinedBondStereoCount,CovalentUnitCount,\"\n           \"PatentCount,PatentFamilyCount,LiteratureCount,Volume3D,XStericQuadrupole3D,\"\n           \"YStericQuadrupole3D,ZStericQuadrupole3D,FeatureCount3D,FeatureAcceptorCount3D,\"\n           \"FeatureDonorCount3D,FeatureAnionCount3D,FeatureCationCount3D,FeatureRingCount3D,\"\n           \"FeatureHydrophobeCount3D,ConformerModelRMSD3D,EffectiveRotorCount3D,ConformerCount3D,\"\n           \"Fingerprint2D/CSV\")\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n        compound_data = pd.read_csv(StringIO(response.text),\n                                    sep=',', low_memory=False)\n        compound_data['StructureImage2DURL'] = f\"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/cid/{cid}/PNG\"\n        return compound_data\n    except Exception as e:\n        logging.error(f\"Error processing CID {cid}: {e}\")\n        return pd.DataFrame()  # Return an empty DataFrame in case of error\n</code></pre>"},{"location":"documentation/#chemgraphbuilder.node_properties_extractor.NodePropertiesExtractor.get_enzyme_assays","title":"<code>get_enzyme_assays(enzyme)</code>","text":"<p>Fetches assay data for a specified enzyme from the PubChem database and returns it as a pandas DataFrame.</p> <p>This method constructs a URL to query the PubChem database for concise assay data related to the given enzyme. It processes the CSV response into a DataFrame, which includes various assay data points provided by PubChem.</p> <p>Parameters:</p> Name Type Description Default <code>enzyme</code> <code>str</code> <p>The name of the enzyme for which assay data is</p> required <p>Returns:</p> Type Description <p>pd.DataFrame: A DataFrame containing the assay data fetched from</p> <p>PubChem for the specified enzyme. The DataFrame includes columns</p> <p>based on the CSV response from PubChem, such as assay ID, results,</p> <p>and conditions. Returns None if no data is available or if an error</p> <p>occurs during data fetching or processing.</p> <p>Raises:</p> Type Description <code>RequestException</code> <p>If an error occurs during the HTTP</p> <code>EmptyDataError</code> <p>If the response from PubChem contains no data.</p> Example <p>extractor = NodePropertiesExtractor(['enzyme']) enzyme_assays_df = extractor.get_enzyme_assays('enzyme') print(enzyme_assays_df.head())</p> Source code in <code>chemgraphbuilder/node_properties_extractor.py</code> <pre><code>def get_enzyme_assays(self, enzyme):\n    \"\"\"\n    Fetches assay data for a specified enzyme from the PubChem database and\n    returns it as a pandas DataFrame.\n\n    This method constructs a URL to query the PubChem database for concise\n    assay data related to the given enzyme. It processes the CSV response\n    into a DataFrame, which includes various assay data points provided by PubChem.\n\n    Parameters:\n        enzyme (str): The name of the enzyme for which assay data is\n        requested. This name is used in the API query.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the assay data fetched from\n        PubChem for the specified enzyme. The DataFrame includes columns\n        based on the CSV response from PubChem, such as assay ID, results,\n        and conditions. Returns None if no data is available or if an error\n        occurs during data fetching or processing.\n\n    Raises:\n        requests.RequestException: If an error occurs during the HTTP\n        request to the PubChem API.\n        pd.errors.EmptyDataError: If the response from PubChem contains no data.\n\n    Example:\n        &gt;&gt;&gt; extractor = NodePropertiesExtractor(['enzyme'])\n        &gt;&gt;&gt; enzyme_assays_df = extractor.get_enzyme_assays('enzyme')\n        &gt;&gt;&gt; print(enzyme_assays_df.head())\n    \"\"\"\n    assays_url = f\"{self._base_url}/{enzyme.lower()}/concise/CSV\"\n    logging.info(f\"Fetching assays for enzyme: {enzyme}\")\n\n    response = self._make_request(assays_url)\n\n    assays_csv_string = response.text\n    assays_csv_string_io = StringIO(assays_csv_string)\n    try:\n        assays_df = pd.read_csv(assays_csv_string_io,\n                                sep=self._sep,\n                                low_memory=False)\n        logging.info(\"Assays DataFrame for enzyme %s has shape: %s\",\n                     enzyme, assays_df.shape)\n        return assays_df\n    except pd.errors.EmptyDataError:\n        logging.warning(\"No data available for enzyme %s.\", enzyme)\n        return None\n</code></pre>"},{"location":"documentation/#chemgraphbuilder.node_properties_extractor.NodePropertiesExtractor.run","title":"<code>run()</code>","text":"<p>Orchestrates the process of fetching, filtering, and aggregating assay data from PubChem for a predefined list of enzymes.</p> <p>This method iteratively queries PubChem for assay data corresponding to each enzyme specified in the <code>enzyme_list</code> attribute during class initialization. It performs the following steps for each enzyme: 1. Constructs a query URL and fetches assay data from PubChem. 2. Filters the fetched data based on predefined criteria (e.g., containing specific substrings in the assay name). 3. Aggregates the filtered data into a single pandas DataFrame. 4. Identifies enzymes for which data could not be fetched or were excluded based on filtering criteria, logging their names.</p> <p>The final aggregated DataFrame, containing assay data for all successfully processed enzymes, is then saved to a CSV file. This method facilitates the extraction and preprocessing of chemical assay data for further analysis or integration into knowledge graphs.</p> Note <ul> <li>This method relies on the successful response from PubChem for each enzyme query.</li> <li>Enzymes with no available data or failing to meet the filtering criteria are excluded from the final DataFrame.</li> <li>The output CSV file is saved in the current working directory with the name 'Data/AllDataConnected.csv'.</li> </ul> <p>Returns:</p> Type Description <p>pd.DataFrame: A DataFrame containing the aggregated and filtered</p> <p>assay data for the specified enzymes. Columns in the DataFrame</p> <p>correspond to the assay data fields returned by PubChem, subject to</p> <p>the filtering criteria applied within this method.</p> <p>Raises:</p> Type Description <code>RequestException</code> <p>If there is an issue with fetching data</p> Example <p>Assuming <code>enzyme_list</code> was set to ['CYP2D6', 'CYP3A4'] during class initialization:</p> <p>extractor = NodePropertiesExtractor(['CYP2D6', 'CYP3A4']) extractor.create_data_directories() result_df = extractor.run() print(result_df.head())</p> <p>This will fetch and process assay data for 'CYP2D6' and 'CYP3A4', returning a DataFrame with the processed data.</p> Source code in <code>chemgraphbuilder/node_properties_extractor.py</code> <pre><code>def run(self):\n    \"\"\"\n    Orchestrates the process of fetching, filtering, and aggregating assay\n    data from PubChem for a predefined list of enzymes.\n\n    This method iteratively queries PubChem for assay data corresponding\n    to each enzyme specified in the `enzyme_list` attribute during class\n    initialization. It performs the following steps for each enzyme:\n    1. Constructs a query URL and fetches assay data from PubChem.\n    2. Filters the fetched data based on predefined criteria\n    (e.g., containing specific substrings in the assay name).\n    3. Aggregates the filtered data into a single pandas DataFrame.\n    4. Identifies enzymes for which data could not be fetched or were\n    excluded based on filtering criteria, logging their names.\n\n    The final aggregated DataFrame, containing assay data for all successfully\n    processed enzymes, is then saved to a CSV file. This method facilitates\n    the extraction and preprocessing of chemical assay data for further\n    analysis or integration into knowledge graphs.\n\n    Note:\n        - This method relies on the successful response from PubChem\n        for each enzyme query.\n        - Enzymes with no available data or failing to meet the filtering\n        criteria are excluded from the final DataFrame.\n        - The output CSV file is saved in the current working directory\n        with the name 'Data/AllDataConnected.csv'.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the aggregated and filtered\n        assay data for the specified enzymes. Columns in the DataFrame\n        correspond to the assay data fields returned by PubChem, subject to\n        the filtering criteria applied within this method.\n\n    Raises:\n        requests.RequestException: If there is an issue with fetching data\n        from PubChem, such as a network problem or an invalid response.\n\n    Example:\n        Assuming `enzyme_list` was set to ['CYP2D6', 'CYP3A4'] during\n        class initialization:\n\n        &gt;&gt;&gt; extractor = NodePropertiesExtractor(['CYP2D6', 'CYP3A4'])\n        &gt;&gt;&gt; extractor.create_data_directories()\n        &gt;&gt;&gt; result_df = extractor.run()\n        &gt;&gt;&gt; print(result_df.head())\n\n        This will fetch and process assay data for 'CYP2D6' and 'CYP3A4',\n        returning a DataFrame with the processed data.\n    \"\"\"\n\n    # Initialize an empty list to store enzymes with successful responses\n    enzymes_with_response = []\n\n    # Keep a copy of the original list to identify removed enzymes later\n    original_enzyme_list = self.enzyme_list.copy()\n\n    for enzyme in self.enzyme_list:\n        # Formulate the URL\n        url = f\"{self._base_url}/{enzyme}/concise/CSV\"\n\n        try:\n            response = requests.get(url)\n            # Check for a successful response (status code 200)\n            if response.status_code == 200:\n                enzymes_with_response.append(enzyme)  # Keep the enzyme in the new list\n        except requests.RequestException:\n            # If there's an exception, skip adding the enzyme to the new list\n            pass\n\n    # Update the enzyme list with only the enzymes that had a successful response\n    self.enzyme_list = enzymes_with_response\n\n    # Identify and print the removed enzymes\n    removed_enzymes = [enzyme for enzyme in original_enzyme_list if enzyme not in enzymes_with_response]\n    if removed_enzymes:\n        logging.info(\"These enzymes were removed because their names aren't correct: %s\",\n                     \", \".join(removed_enzymes))\n\n    df_list = self._process_enzymes(self.enzyme_list)\n    df = self._concatenate_data(df_list)\n    substrings_to_filter = ['CYP', 'Cytochrome']\n    pattern = '|'.join(substrings_to_filter)\n    df = df[df['Assay Name'].str.contains(pattern, case=False, na=False)]\n    df.to_csv('Data/AllDataConnected.csv', index=False)\n    return df\n</code></pre>"},{"location":"documentation/#4-node-data-processor","title":"4. Node Data Processor","text":"<p>node_data_processor.py</p> <p>This module provides the NodeDataProcessor class, which is responsible for preprocessing various types of node data (assays, proteins, genes, and compounds) for use in chemical knowledge graph construction. The preprocessing includes renaming columns, consolidating multiple files, and saving the processed data in a consistent format. This step ensures uniformity and ease of access for subsequent data analysis and integration processes.</p> <p>Classes:</p> Name Description <code>NodeDataProcessor</code> <p>Handles preprocessing of assay, protein, gene, and compound data.</p> Example Usage <p>processor = NodeDataProcessor(data_dir='path/to/data') processor.preprocess_assays() processor.preprocess_proteins() processor.preprocess_genes() processor.preprocess_compounds()</p>"},{"location":"documentation/#chemgraphbuilder.node_data_processor.NodeDataProcessor","title":"<code>NodeDataProcessor</code>","text":"<p>NodeDataProcessor is responsible for preprocessing various types of node data (assays, proteins, genes, and compounds) by renaming columns, consolidating multiple files, and saving the processed data. This preprocessing step is crucial for ensuring uniformity and ease of access in subsequent analysis and integration processes.</p> <p>Attributes:</p> Name Type Description <code>data_dir</code> <code>str</code> <p>The directory where the node data files are stored.</p> <p>Methods:</p> Name Description <code>preprocess_assays</code> <p>Processes and renames columns in assay data.</p> <code>preprocess_proteins</code> <p>Processes and renames columns in protein data.</p> <code>preprocess_genes</code> <p>Processes and renames columns in gene data.</p> <code>preprocess_compounds</code> <p>Consolidates and renames columns in compound data.</p> Source code in <code>chemgraphbuilder/node_data_processor.py</code> <pre><code>class NodeDataProcessor:\n    \"\"\"\n    NodeDataProcessor is responsible for preprocessing various types of node data\n    (assays, proteins, genes, and compounds) by renaming columns, consolidating\n    multiple files, and saving the processed data. This preprocessing step is\n    crucial for ensuring uniformity and ease of access in subsequent analysis\n    and integration processes.\n\n    Attributes:\n        data_dir (str): The directory where the node data files are stored.\n\n    Methods:\n        preprocess_assays(): Processes and renames columns in assay data.\n        preprocess_proteins(): Processes and renames columns in protein data.\n        preprocess_genes(): Processes and renames columns in gene data.\n        preprocess_compounds(): Consolidates and renames columns in compound data.\n    \"\"\"\n\n    def __init__(self, data_dir: str):\n        \"\"\"\n        Initializes the NodeDataProcessor with a directory path to manage the data files.\n\n        Args:\n            data_dir (str): The directory where the node data files are stored.\n        \"\"\"\n        self.data_dir = data_dir\n\n\n    def preprocess_assays(self):\n        \"\"\"\n        Processes the assay data by renaming columns and saving the modified data back to disk.\n        This method also handles visualization of assay data distributions if necessary.\n        \"\"\"\n        df = pd.read_csv(f'{self.data_dir}/Nodes/Assay_Properties.csv')\n        df.rename(columns={\"AID\": \"AssayID\", \"Assay Type\": \"AssayType\",\n                           \"Activity Name\": \"AssayActivityName\", \"SourceID\": \"AssaySourceID\",\n                           \"SourceName\": \"AssaySourceName\", \"Name\": \"AssayName\",\n                           \"Description\": \"AssayDescription\"}, inplace=True)\n        df.to_csv(f'{self.data_dir}/Nodes/Assay_Properties_Processed.csv', index=False)\n\n\n    def preprocess_proteins(self):\n        \"\"\"\n        Processes the protein data by renaming columns and saving the processed data.\n        This method simplifies access to protein data for downstream analysis.\n        \"\"\"\n        df = pd.read_csv(f'{self.data_dir}/Nodes/Protein_Properties.csv')\n        df.rename(columns={\"ID\": \"ProteinID\", \"Name\": \"ProteinName\",\n                           \"Description\": \"ProteinDescription\"}, inplace=True)\n        df.to_csv(f'{self.data_dir}/Nodes/Protein_Properties_Processed.csv', index=False)\n\n\n    def preprocess_genes(self):\n        \"\"\"\n        Processes gene data by renaming columns and changing data types for specific fields.\n        The processed data is saved for further use in gene-related analyses.\n        \"\"\"\n        df = pd.read_csv(f'{self.data_dir}/Nodes/Gene_Properties.csv')\n        df.rename(columns={\"Symbol\": \"GeneSymbol\", \"Taxonomy ID\": \"TaxonomyID\",\n                           \"Synonyms\": \"GeneSynonyms\"}, inplace=True)\n        df['GeneID'] = df['GeneID'].astype('Int64')\n        df.to_csv(f'{self.data_dir}/Nodes/Gene_Properties_Processed.csv', index=False)\n\n\n    def preprocess_compounds(self):\n        \"\"\"\n        Concatenates multiple CSV files containing compound data into a single file,\n        renames columns for uniformity, and saves the consolidated data. This method\n        facilitates easier management and analysis of compound data.\n        \"\"\"\n        path = f'{self.data_dir}/Nodes/Compound_Properties'\n        all_csv_files = glob.glob(path + \"/*.csv\")\n        first_file = True\n        output_file = f'Data/Nodes/Compound_Properties.csv'\n\n        with open(output_file, 'w', newline='', encoding='utf-8') as f_out:\n            for file in all_csv_files:\n                with open(file, 'r', newline='', encoding='utf-8') as f_in:\n                    header = f_in.readline()\n                    if first_file:\n                        f_out.write(header)\n                        first_file = False\n                    for line in f_in:\n                        f_out.write(line)\n\n        df = pd.read_csv(output_file)\n        df.rename(columns={\"CID\": \"CompoundID\", \"Title\": \"CompoundName\"}, inplace=True)\n        df.to_csv(f\"{output_file.replace('.csv', '_Processed')}\", index=False)\n</code></pre>"},{"location":"documentation/#chemgraphbuilder.node_data_processor.NodeDataProcessor.__init__","title":"<code>__init__(data_dir)</code>","text":"<p>Initializes the NodeDataProcessor with a directory path to manage the data files.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>str</code> <p>The directory where the node data files are stored.</p> required Source code in <code>chemgraphbuilder/node_data_processor.py</code> <pre><code>def __init__(self, data_dir: str):\n    \"\"\"\n    Initializes the NodeDataProcessor with a directory path to manage the data files.\n\n    Args:\n        data_dir (str): The directory where the node data files are stored.\n    \"\"\"\n    self.data_dir = data_dir\n</code></pre>"},{"location":"documentation/#chemgraphbuilder.node_data_processor.NodeDataProcessor.preprocess_assays","title":"<code>preprocess_assays()</code>","text":"<p>Processes the assay data by renaming columns and saving the modified data back to disk. This method also handles visualization of assay data distributions if necessary.</p> Source code in <code>chemgraphbuilder/node_data_processor.py</code> <pre><code>def preprocess_assays(self):\n    \"\"\"\n    Processes the assay data by renaming columns and saving the modified data back to disk.\n    This method also handles visualization of assay data distributions if necessary.\n    \"\"\"\n    df = pd.read_csv(f'{self.data_dir}/Nodes/Assay_Properties.csv')\n    df.rename(columns={\"AID\": \"AssayID\", \"Assay Type\": \"AssayType\",\n                       \"Activity Name\": \"AssayActivityName\", \"SourceID\": \"AssaySourceID\",\n                       \"SourceName\": \"AssaySourceName\", \"Name\": \"AssayName\",\n                       \"Description\": \"AssayDescription\"}, inplace=True)\n    df.to_csv(f'{self.data_dir}/Nodes/Assay_Properties_Processed.csv', index=False)\n</code></pre>"},{"location":"documentation/#chemgraphbuilder.node_data_processor.NodeDataProcessor.preprocess_compounds","title":"<code>preprocess_compounds()</code>","text":"<p>Concatenates multiple CSV files containing compound data into a single file, renames columns for uniformity, and saves the consolidated data. This method facilitates easier management and analysis of compound data.</p> Source code in <code>chemgraphbuilder/node_data_processor.py</code> <pre><code>def preprocess_compounds(self):\n    \"\"\"\n    Concatenates multiple CSV files containing compound data into a single file,\n    renames columns for uniformity, and saves the consolidated data. This method\n    facilitates easier management and analysis of compound data.\n    \"\"\"\n    path = f'{self.data_dir}/Nodes/Compound_Properties'\n    all_csv_files = glob.glob(path + \"/*.csv\")\n    first_file = True\n    output_file = f'Data/Nodes/Compound_Properties.csv'\n\n    with open(output_file, 'w', newline='', encoding='utf-8') as f_out:\n        for file in all_csv_files:\n            with open(file, 'r', newline='', encoding='utf-8') as f_in:\n                header = f_in.readline()\n                if first_file:\n                    f_out.write(header)\n                    first_file = False\n                for line in f_in:\n                    f_out.write(line)\n\n    df = pd.read_csv(output_file)\n    df.rename(columns={\"CID\": \"CompoundID\", \"Title\": \"CompoundName\"}, inplace=True)\n    df.to_csv(f\"{output_file.replace('.csv', '_Processed')}\", index=False)\n</code></pre>"},{"location":"documentation/#chemgraphbuilder.node_data_processor.NodeDataProcessor.preprocess_genes","title":"<code>preprocess_genes()</code>","text":"<p>Processes gene data by renaming columns and changing data types for specific fields. The processed data is saved for further use in gene-related analyses.</p> Source code in <code>chemgraphbuilder/node_data_processor.py</code> <pre><code>def preprocess_genes(self):\n    \"\"\"\n    Processes gene data by renaming columns and changing data types for specific fields.\n    The processed data is saved for further use in gene-related analyses.\n    \"\"\"\n    df = pd.read_csv(f'{self.data_dir}/Nodes/Gene_Properties.csv')\n    df.rename(columns={\"Symbol\": \"GeneSymbol\", \"Taxonomy ID\": \"TaxonomyID\",\n                       \"Synonyms\": \"GeneSynonyms\"}, inplace=True)\n    df['GeneID'] = df['GeneID'].astype('Int64')\n    df.to_csv(f'{self.data_dir}/Nodes/Gene_Properties_Processed.csv', index=False)\n</code></pre>"},{"location":"documentation/#chemgraphbuilder.node_data_processor.NodeDataProcessor.preprocess_proteins","title":"<code>preprocess_proteins()</code>","text":"<p>Processes the protein data by renaming columns and saving the processed data. This method simplifies access to protein data for downstream analysis.</p> Source code in <code>chemgraphbuilder/node_data_processor.py</code> <pre><code>def preprocess_proteins(self):\n    \"\"\"\n    Processes the protein data by renaming columns and saving the processed data.\n    This method simplifies access to protein data for downstream analysis.\n    \"\"\"\n    df = pd.read_csv(f'{self.data_dir}/Nodes/Protein_Properties.csv')\n    df.rename(columns={\"ID\": \"ProteinID\", \"Name\": \"ProteinName\",\n                       \"Description\": \"ProteinDescription\"}, inplace=True)\n    df.to_csv(f'{self.data_dir}/Nodes/Protein_Properties_Processed.csv', index=False)\n</code></pre>"},{"location":"documentation/#5-add-graph-nodes","title":"5. Add Graph Nodes","text":"<p>Module for adding node data from CSV files to a Neo4j database.</p> <p>This module provides a class and methods to read node data from CSV files and add them to a Neo4j database, including creating uniqueness constraints and generating Cypher queries.</p>"},{"location":"documentation/#chemgraphbuilder.add_graph_nodes.AddGraphNodes","title":"<code>AddGraphNodes</code>","text":"<p>               Bases: <code>Neo4jBase</code></p> <p>A class used to add node data from a CSV file or a directory of CSV files to a Neo4j database.</p>"},{"location":"documentation/#chemgraphbuilder.add_graph_nodes.AddGraphNodes--methods","title":"Methods:","text":"<p>create_uniqueness_constraint(driver, label, unique_property):     Create a uniqueness constraint for the unique property of nodes in Neo4j. generate_cypher_queries(node_dict, label, unique_property):     Generate Cypher queries to update nodes in Neo4j based on the data from the CSV file. execute_queries(queries):     Execute a list of provided Cypher queries against the Neo4j database. read_csv_file(file_path, unique_property):     Read data from a CSV file and extract node properties. combine_csv_files(input_directory):     Combine multiple CSV files with the same columns into a single DataFrame. process_and_add_nodes(file_path, label, unique_property):     Process the CSV file and add node data to the Neo4j database. process_and_add_nodes_from_directory(directory_path, label, unique_property):     Combine CSV files from a directory and add node data to the Neo4j database.</p> Source code in <code>chemgraphbuilder/add_graph_nodes.py</code> <pre><code>class AddGraphNodes(Neo4jBase):\n    \"\"\"\n    A class used to add node data from a CSV file or a directory of CSV files to a Neo4j database.\n\n    Methods:\n    --------\n    create_uniqueness_constraint(driver, label, unique_property):\n        Create a uniqueness constraint for the unique property of nodes in Neo4j.\n    generate_cypher_queries(node_dict, label, unique_property):\n        Generate Cypher queries to update nodes in Neo4j based on the data from the CSV file.\n    execute_queries(queries):\n        Execute a list of provided Cypher queries against the Neo4j database.\n    read_csv_file(file_path, unique_property):\n        Read data from a CSV file and extract node properties.\n    combine_csv_files(input_directory):\n        Combine multiple CSV files with the same columns into a single DataFrame.\n    process_and_add_nodes(file_path, label, unique_property):\n        Process the CSV file and add node data to the Neo4j database.\n    process_and_add_nodes_from_directory(directory_path, label, unique_property):\n        Combine CSV files from a directory and add node data to the Neo4j database.\n    \"\"\"\n\n    def __init__(self, driver):\n        \"\"\"\n        Initializes the AddGraphNodes class with a Neo4j driver.\n\n        Parameters:\n        -----------\n        driver : neo4j.GraphDatabase.driver\n            A driver instance to connect to the Neo4j database.\n        \"\"\"\n        super().__init__()\n        self.driver = driver\n        self.logger.info(\"AddGraphNodes class initialized.\")\n\n    @staticmethod\n    def create_uniqueness_constraint(driver, label, unique_property):\n        \"\"\"\n        Create a uniqueness constraint for the unique property of nodes in Neo4j.\n\n        Parameters:\n        -----------\n        driver : neo4j.GraphDatabase.driver\n            A driver instance to connect to the Neo4j database.\n        label : str\n            The label of the node.\n        unique_property : str\n            The unique property of the node.\n        \"\"\"\n        constraint_query = (\n            f\"CREATE CONSTRAINT IF NOT EXISTS FOR (n:{label}) \"\n            f\"REQUIRE n.{unique_property} IS UNIQUE\"\n        )\n        with driver.session() as session:\n            try:\n                session.run(constraint_query)\n                logging.info(\n                    \"Uniqueness constraint created successfully on %s property of %s nodes.\",\n                    unique_property, label)\n            except Exception as e:\n                logging.error(\"Failed to create uniqueness constraint: %s\", e)\n\n    @staticmethod\n    def _generate_property_string(value):\n        if isinstance(value, (int, float)):\n            return value\n        try:\n            return float(value)\n        except (TypeError, ValueError):\n            escaped_value = \"'\" + str(value).replace(\"'\", \"\\\\'\").replace(\"\\n\", \"\\\\n\") + \"'\"\n            return escaped_value\n\n    def generate_cypher_queries(self, node_dict, label, unique_property):\n        \"\"\"\n        Generate Cypher queries for updating Neo4j based on the provided node data dictionary.\n\n        Parameters:\n        -----------\n        node_dict : dict\n            A dictionary with unique identifiers as keys and node data as values.\n        label : str\n            The label of the node.\n        unique_property : str\n            The unique property of the node.\n\n        Yields:\n        -------\n        str\n            A Cypher query string.\n        \"\"\"\n        # Create an index for the unique_property\n        create_index_query = f\"CREATE INDEX IF NOT EXISTS FOR (n:{label}) ON (n.{unique_property})\"\n        self.logger.debug(create_index_query)\n        yield create_index_query\n\n        for unique_id, properties in node_dict.items():\n            unique_id = f'\"{unique_id}\"' if isinstance(unique_id, str) else unique_id\n            query = f\"MERGE (n:{label} {{{unique_property}: {unique_id}}})\"\n            set_clauses = [\n                f\"n.{prop.replace(' ', '')} = {self._generate_property_string(value)}\"\n                for prop, value in properties.items()\n            ]\n            if set_clauses:\n                query += \" SET \" + \", \".join(set_clauses)\n            else:\n                query += \";\"\n            self.logger.debug(query)\n            yield query\n        self.logger.info(\"Cypher queries generated successfully.\")\n\n    def execute_queries(self, queries):\n        \"\"\"\n        Execute the provided list of Cypher queries against the Neo4j database.\n\n        Parameters:\n        -----------\n        queries : list\n            A list of Cypher query strings to execute.\n        \"\"\"\n        self.logger.info(\"Executing Cypher queries...\")\n        with self.driver.session() as session:\n            self.logger.info(\"Executing Cypher queries Started....\")\n            for query in queries:\n                try:\n                    session.run(query)\n                except Exception as e:\n                    self.logger.error(\"Failed to execute query: %s\", e)\n        self.logger.info(\"All queries executed.\")\n\n    def read_csv_file(self, file_path, unique_property):\n        \"\"\"\n        Read data from a CSV file and extract node properties.\n\n        Parameters:\n        -----------\n        file_path : str\n            The path to the CSV file.\n        unique_property : str\n            The column name that serves as the unique identifier for the nodes.\n\n        Returns:\n        --------\n        dict\n            A dictionary with unique identifiers as keys and extracted data as values.\n        \"\"\"\n        self.logger.info(\"Reading data from CSV file: %s\", file_path)\n        df = pd.read_csv(file_path).dropna(subset=[unique_property], how='any')\n        node_dict = {\n            row[unique_property]: row.drop(labels=[unique_property]).to_dict()\n            for _, row in df.iterrows()\n        }\n        self.logger.info(\"Successfully read data for %d nodes from CSV.\", len(node_dict))\n        return node_dict\n\n    def combine_csv_files(self, input_directory):\n        \"\"\"\n        Combine multiple CSV files with the same columns into a single DataFrame.\n\n        Parameters:\n        -----------\n        input_directory : str\n            The directory containing the CSV files to be combined.\n\n        Returns:\n        --------\n        DataFrame\n            A combined DataFrame containing data from all the CSV files.\n        \"\"\"\n        self.logger.info(\"Combining CSV files from directory: %s\", input_directory)\n        dfs = [\n            pd.read_csv(os.path.join(input_directory, file))\n            for file in os.listdir(input_directory)\n            if file.endswith(\".csv\")\n        ]\n        combined_df = pd.concat(dfs, ignore_index=True)\n        self.logger.info(\"Successfully combined %d CSV files.\", len(dfs))\n        return combined_df\n\n    def process_and_add_nodes(self, file_path, label, unique_property):\n        \"\"\"\n        Process the CSV file and add node data to the Neo4j database.\n\n        Parameters:\n        -----------\n        file_path : str\n            The path to the CSV file.\n        label : str\n            The label of the node.\n        unique_property : str\n            The unique property of the node.\n        \"\"\"\n        self.logger.info(\"Processing and adding nodes from file: %s\", file_path)\n        node_dict = self.read_csv_file(file_path, unique_property)\n        queries = list(self.generate_cypher_queries(node_dict, label, unique_property))\n        self.execute_queries(queries)\n        self.logger.info(\"Successfully processed and added nodes from file: %s\", file_path)\n\n    def process_and_add_nodes_from_directory(self, directory_path, label, unique_property):\n        \"\"\"\n        Combine CSV files from a directory and add node data to the Neo4j database.\n\n        Parameters:\n        -----------\n        directory_path : str\n            The path to the directory containing the CSV files.\n        label : str\n            The label of the node.\n        unique_property : str\n            The unique property of the node.\n        \"\"\"\n        self.logger.info(\"Processing and adding nodes from directory: %s\", directory_path)\n        combined_df = self.combine_csv_files(directory_path)\n        temp_file = os.path.join(directory_path, \"combined_temp.csv\")\n        combined_df.to_csv(temp_file, index=False)\n        self.process_and_add_nodes(temp_file, label, unique_property)\n        os.remove(temp_file)\n        self.logger.info(\"Successfully processed and added nodes from directory: %s\",\n                         directory_path)\n\n    def public_generate_property_string(self, value):\n        \"\"\"\n        Public method to access the protected _generate_property_string method for testing.\n\n        Parameters:\n        -----------\n        value : Any\n            The value to be formatted.\n\n        Returns:\n        --------\n        str\n            The formatted property string.\n        \"\"\"\n        return self._generate_property_string(value)\n</code></pre>"},{"location":"documentation/#chemgraphbuilder.add_graph_nodes.AddGraphNodes.__init__","title":"<code>__init__(driver)</code>","text":"<p>Initializes the AddGraphNodes class with a Neo4j driver.</p>"},{"location":"documentation/#chemgraphbuilder.add_graph_nodes.AddGraphNodes.__init__--parameters","title":"Parameters:","text":"<p>driver : neo4j.GraphDatabase.driver     A driver instance to connect to the Neo4j database.</p> Source code in <code>chemgraphbuilder/add_graph_nodes.py</code> <pre><code>def __init__(self, driver):\n    \"\"\"\n    Initializes the AddGraphNodes class with a Neo4j driver.\n\n    Parameters:\n    -----------\n    driver : neo4j.GraphDatabase.driver\n        A driver instance to connect to the Neo4j database.\n    \"\"\"\n    super().__init__()\n    self.driver = driver\n    self.logger.info(\"AddGraphNodes class initialized.\")\n</code></pre>"},{"location":"documentation/#chemgraphbuilder.add_graph_nodes.AddGraphNodes.combine_csv_files","title":"<code>combine_csv_files(input_directory)</code>","text":"<p>Combine multiple CSV files with the same columns into a single DataFrame.</p>"},{"location":"documentation/#chemgraphbuilder.add_graph_nodes.AddGraphNodes.combine_csv_files--parameters","title":"Parameters:","text":"<p>input_directory : str     The directory containing the CSV files to be combined.</p>"},{"location":"documentation/#chemgraphbuilder.add_graph_nodes.AddGraphNodes.combine_csv_files--returns","title":"Returns:","text":"<p>DataFrame     A combined DataFrame containing data from all the CSV files.</p> Source code in <code>chemgraphbuilder/add_graph_nodes.py</code> <pre><code>def combine_csv_files(self, input_directory):\n    \"\"\"\n    Combine multiple CSV files with the same columns into a single DataFrame.\n\n    Parameters:\n    -----------\n    input_directory : str\n        The directory containing the CSV files to be combined.\n\n    Returns:\n    --------\n    DataFrame\n        A combined DataFrame containing data from all the CSV files.\n    \"\"\"\n    self.logger.info(\"Combining CSV files from directory: %s\", input_directory)\n    dfs = [\n        pd.read_csv(os.path.join(input_directory, file))\n        for file in os.listdir(input_directory)\n        if file.endswith(\".csv\")\n    ]\n    combined_df = pd.concat(dfs, ignore_index=True)\n    self.logger.info(\"Successfully combined %d CSV files.\", len(dfs))\n    return combined_df\n</code></pre>"},{"location":"documentation/#chemgraphbuilder.add_graph_nodes.AddGraphNodes.create_uniqueness_constraint","title":"<code>create_uniqueness_constraint(driver, label, unique_property)</code>  <code>staticmethod</code>","text":"<p>Create a uniqueness constraint for the unique property of nodes in Neo4j.</p>"},{"location":"documentation/#chemgraphbuilder.add_graph_nodes.AddGraphNodes.create_uniqueness_constraint--parameters","title":"Parameters:","text":"<p>driver : neo4j.GraphDatabase.driver     A driver instance to connect to the Neo4j database. label : str     The label of the node. unique_property : str     The unique property of the node.</p> Source code in <code>chemgraphbuilder/add_graph_nodes.py</code> <pre><code>@staticmethod\ndef create_uniqueness_constraint(driver, label, unique_property):\n    \"\"\"\n    Create a uniqueness constraint for the unique property of nodes in Neo4j.\n\n    Parameters:\n    -----------\n    driver : neo4j.GraphDatabase.driver\n        A driver instance to connect to the Neo4j database.\n    label : str\n        The label of the node.\n    unique_property : str\n        The unique property of the node.\n    \"\"\"\n    constraint_query = (\n        f\"CREATE CONSTRAINT IF NOT EXISTS FOR (n:{label}) \"\n        f\"REQUIRE n.{unique_property} IS UNIQUE\"\n    )\n    with driver.session() as session:\n        try:\n            session.run(constraint_query)\n            logging.info(\n                \"Uniqueness constraint created successfully on %s property of %s nodes.\",\n                unique_property, label)\n        except Exception as e:\n            logging.error(\"Failed to create uniqueness constraint: %s\", e)\n</code></pre>"},{"location":"documentation/#chemgraphbuilder.add_graph_nodes.AddGraphNodes.execute_queries","title":"<code>execute_queries(queries)</code>","text":"<p>Execute the provided list of Cypher queries against the Neo4j database.</p>"},{"location":"documentation/#chemgraphbuilder.add_graph_nodes.AddGraphNodes.execute_queries--parameters","title":"Parameters:","text":"<p>queries : list     A list of Cypher query strings to execute.</p> Source code in <code>chemgraphbuilder/add_graph_nodes.py</code> <pre><code>def execute_queries(self, queries):\n    \"\"\"\n    Execute the provided list of Cypher queries against the Neo4j database.\n\n    Parameters:\n    -----------\n    queries : list\n        A list of Cypher query strings to execute.\n    \"\"\"\n    self.logger.info(\"Executing Cypher queries...\")\n    with self.driver.session() as session:\n        self.logger.info(\"Executing Cypher queries Started....\")\n        for query in queries:\n            try:\n                session.run(query)\n            except Exception as e:\n                self.logger.error(\"Failed to execute query: %s\", e)\n    self.logger.info(\"All queries executed.\")\n</code></pre>"},{"location":"documentation/#chemgraphbuilder.add_graph_nodes.AddGraphNodes.generate_cypher_queries","title":"<code>generate_cypher_queries(node_dict, label, unique_property)</code>","text":"<p>Generate Cypher queries for updating Neo4j based on the provided node data dictionary.</p>"},{"location":"documentation/#chemgraphbuilder.add_graph_nodes.AddGraphNodes.generate_cypher_queries--parameters","title":"Parameters:","text":"<p>node_dict : dict     A dictionary with unique identifiers as keys and node data as values. label : str     The label of the node. unique_property : str     The unique property of the node.</p>"},{"location":"documentation/#chemgraphbuilder.add_graph_nodes.AddGraphNodes.generate_cypher_queries--yields","title":"Yields:","text":"<p>str     A Cypher query string.</p> Source code in <code>chemgraphbuilder/add_graph_nodes.py</code> <pre><code>def generate_cypher_queries(self, node_dict, label, unique_property):\n    \"\"\"\n    Generate Cypher queries for updating Neo4j based on the provided node data dictionary.\n\n    Parameters:\n    -----------\n    node_dict : dict\n        A dictionary with unique identifiers as keys and node data as values.\n    label : str\n        The label of the node.\n    unique_property : str\n        The unique property of the node.\n\n    Yields:\n    -------\n    str\n        A Cypher query string.\n    \"\"\"\n    # Create an index for the unique_property\n    create_index_query = f\"CREATE INDEX IF NOT EXISTS FOR (n:{label}) ON (n.{unique_property})\"\n    self.logger.debug(create_index_query)\n    yield create_index_query\n\n    for unique_id, properties in node_dict.items():\n        unique_id = f'\"{unique_id}\"' if isinstance(unique_id, str) else unique_id\n        query = f\"MERGE (n:{label} {{{unique_property}: {unique_id}}})\"\n        set_clauses = [\n            f\"n.{prop.replace(' ', '')} = {self._generate_property_string(value)}\"\n            for prop, value in properties.items()\n        ]\n        if set_clauses:\n            query += \" SET \" + \", \".join(set_clauses)\n        else:\n            query += \";\"\n        self.logger.debug(query)\n        yield query\n    self.logger.info(\"Cypher queries generated successfully.\")\n</code></pre>"},{"location":"documentation/#chemgraphbuilder.add_graph_nodes.AddGraphNodes.process_and_add_nodes","title":"<code>process_and_add_nodes(file_path, label, unique_property)</code>","text":"<p>Process the CSV file and add node data to the Neo4j database.</p>"},{"location":"documentation/#chemgraphbuilder.add_graph_nodes.AddGraphNodes.process_and_add_nodes--parameters","title":"Parameters:","text":"<p>file_path : str     The path to the CSV file. label : str     The label of the node. unique_property : str     The unique property of the node.</p> Source code in <code>chemgraphbuilder/add_graph_nodes.py</code> <pre><code>def process_and_add_nodes(self, file_path, label, unique_property):\n    \"\"\"\n    Process the CSV file and add node data to the Neo4j database.\n\n    Parameters:\n    -----------\n    file_path : str\n        The path to the CSV file.\n    label : str\n        The label of the node.\n    unique_property : str\n        The unique property of the node.\n    \"\"\"\n    self.logger.info(\"Processing and adding nodes from file: %s\", file_path)\n    node_dict = self.read_csv_file(file_path, unique_property)\n    queries = list(self.generate_cypher_queries(node_dict, label, unique_property))\n    self.execute_queries(queries)\n    self.logger.info(\"Successfully processed and added nodes from file: %s\", file_path)\n</code></pre>"},{"location":"documentation/#chemgraphbuilder.add_graph_nodes.AddGraphNodes.process_and_add_nodes_from_directory","title":"<code>process_and_add_nodes_from_directory(directory_path, label, unique_property)</code>","text":"<p>Combine CSV files from a directory and add node data to the Neo4j database.</p>"},{"location":"documentation/#chemgraphbuilder.add_graph_nodes.AddGraphNodes.process_and_add_nodes_from_directory--parameters","title":"Parameters:","text":"<p>directory_path : str     The path to the directory containing the CSV files. label : str     The label of the node. unique_property : str     The unique property of the node.</p> Source code in <code>chemgraphbuilder/add_graph_nodes.py</code> <pre><code>def process_and_add_nodes_from_directory(self, directory_path, label, unique_property):\n    \"\"\"\n    Combine CSV files from a directory and add node data to the Neo4j database.\n\n    Parameters:\n    -----------\n    directory_path : str\n        The path to the directory containing the CSV files.\n    label : str\n        The label of the node.\n    unique_property : str\n        The unique property of the node.\n    \"\"\"\n    self.logger.info(\"Processing and adding nodes from directory: %s\", directory_path)\n    combined_df = self.combine_csv_files(directory_path)\n    temp_file = os.path.join(directory_path, \"combined_temp.csv\")\n    combined_df.to_csv(temp_file, index=False)\n    self.process_and_add_nodes(temp_file, label, unique_property)\n    os.remove(temp_file)\n    self.logger.info(\"Successfully processed and added nodes from directory: %s\",\n                     directory_path)\n</code></pre>"},{"location":"documentation/#chemgraphbuilder.add_graph_nodes.AddGraphNodes.public_generate_property_string","title":"<code>public_generate_property_string(value)</code>","text":"<p>Public method to access the protected _generate_property_string method for testing.</p>"},{"location":"documentation/#chemgraphbuilder.add_graph_nodes.AddGraphNodes.public_generate_property_string--parameters","title":"Parameters:","text":"<p>value : Any     The value to be formatted.</p>"},{"location":"documentation/#chemgraphbuilder.add_graph_nodes.AddGraphNodes.public_generate_property_string--returns","title":"Returns:","text":"<p>str     The formatted property string.</p> Source code in <code>chemgraphbuilder/add_graph_nodes.py</code> <pre><code>def public_generate_property_string(self, value):\n    \"\"\"\n    Public method to access the protected _generate_property_string method for testing.\n\n    Parameters:\n    -----------\n    value : Any\n        The value to be formatted.\n\n    Returns:\n    --------\n    str\n        The formatted property string.\n    \"\"\"\n    return self._generate_property_string(value)\n</code></pre>"},{"location":"documentation/#chemgraphbuilder.add_graph_nodes.AddGraphNodes.read_csv_file","title":"<code>read_csv_file(file_path, unique_property)</code>","text":"<p>Read data from a CSV file and extract node properties.</p>"},{"location":"documentation/#chemgraphbuilder.add_graph_nodes.AddGraphNodes.read_csv_file--parameters","title":"Parameters:","text":"<p>file_path : str     The path to the CSV file. unique_property : str     The column name that serves as the unique identifier for the nodes.</p>"},{"location":"documentation/#chemgraphbuilder.add_graph_nodes.AddGraphNodes.read_csv_file--returns","title":"Returns:","text":"<p>dict     A dictionary with unique identifiers as keys and extracted data as values.</p> Source code in <code>chemgraphbuilder/add_graph_nodes.py</code> <pre><code>def read_csv_file(self, file_path, unique_property):\n    \"\"\"\n    Read data from a CSV file and extract node properties.\n\n    Parameters:\n    -----------\n    file_path : str\n        The path to the CSV file.\n    unique_property : str\n        The column name that serves as the unique identifier for the nodes.\n\n    Returns:\n    --------\n    dict\n        A dictionary with unique identifiers as keys and extracted data as values.\n    \"\"\"\n    self.logger.info(\"Reading data from CSV file: %s\", file_path)\n    df = pd.read_csv(file_path).dropna(subset=[unique_property], how='any')\n    node_dict = {\n        row[unique_property]: row.drop(labels=[unique_property]).to_dict()\n        for _, row in df.iterrows()\n    }\n    self.logger.info(\"Successfully read data for %d nodes from CSV.\", len(node_dict))\n    return node_dict\n</code></pre>"},{"location":"documentation/#6-relationship-properties-extractor","title":"6. Relationship Properties Extractor","text":"<p>This module defines the <code>RelationshipPropertiesExtractor</code> class, which is responsible for extracting and analyzing relationship properties among compounds, genes, and assays from the PubChem database.</p> <p>The class facilitates the retrieval of complex relational data between chemical entities, enabling detailed analysis of biochemical interactions and properties. The extracted data is ideal for constructing knowledge graphs, supporting drug discovery, and understanding genetic influences on compound behavior.</p> <p>Classes:</p> Name Description <code>- RelationshipPropertiesExtractor</code> <p>A class to extract and analyze relationship properties from PubChem.</p> Usage Example <p>extractor = RelationshipPropertiesExtractor() extractor.assay_compound_relationship(\"Data/AllDataCollected.csv\") This example fetches assay-compound relationship data for specified assays and saves the data to CSV files.</p> Note <p>Ensure network access to the PubChem API for data retrieval.</p>"},{"location":"documentation/#chemgraphbuilder.relationship_properties_extractor.RelationshipPropertiesExtractor","title":"<code>RelationshipPropertiesExtractor</code>","text":"<p>Extracts and analyzes relationship properties among compounds, genes, and assays from the PubChem database.</p> <p>This class facilitates the retrieval of complex relational data between chemical entities, enabling detailed analysis of biochemical interactions and properties. The extracted data is ideal for constructing knowledge graphs, supporting drug discovery, and understanding genetic influences on compound behavior.</p> <p>Methods within the class are tailored to query specific relationship types from PubChem, including compound-assay relationships, compound co-occurrences, and compound transformations influenced by genes. Data fetched from PubChem is processed and saved in structured formats (CSV files), ready for further analysis or database integration.</p> <p>Attributes:</p> Name Type Description <code>session</code> <code>Session</code> <p>Session object to persist certain parameters</p> Usage <p>extractor = RelationshipPropertiesExtractor() extractor.assay_compound_relationship(\"Data/AllDataCollected.csv\") This example fetches assay-compound relationship data for specified assays and saves the data to CSV files.</p> Source code in <code>chemgraphbuilder/relationship_properties_extractor.py</code> <pre><code>class RelationshipPropertiesExtractor:\n    \"\"\"\n    Extracts and analyzes relationship properties among compounds, genes, and\n    assays from the PubChem database.\n\n    This class facilitates the retrieval of complex relational data between\n    chemical entities, enabling detailed analysis of biochemical interactions\n    and properties. The extracted data is ideal for constructing knowledge\n    graphs, supporting drug discovery, and understanding genetic influences\n    on compound behavior.\n\n    Methods within the class are tailored to query specific relationship types\n    from PubChem, including compound-assay relationships, compound co-occurrences,\n    and compound transformations influenced by genes. Data fetched from PubChem\n    is processed and saved in structured formats (CSV files), ready for further\n    analysis or database integration.\n\n    Attributes:\n        session (requests.Session): Session object to persist certain parameters\n        across requests.\n\n    Usage:\n        &gt;&gt;&gt; extractor = RelationshipPropertiesExtractor()\n        &gt;&gt;&gt; extractor.assay_compound_relationship(\"Data/AllDataCollected.csv\")\n        This example fetches assay-compound relationship data for specified\n        assays and saves the data to CSV files.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initializes a RelationshipPropertiesExtractor with a Requests session\n         for efficient network calls.\"\"\"\n        self.session = requests.Session()\n\n\n    def _send_request(self, url, max_retries=5, initial_wait=1):\n        for attempt in range(max_retries):\n            try:\n                response = self.session.get(url, timeout=30)\n                response.raise_for_status()\n                return response\n            except requests.HTTPError as e:\n                if response.status_code == 503:\n                    wait = initial_wait * (2 ** attempt)\n                    print(f\"Server busy or under maintenance. Retrying in {wait} seconds...\")\n                    time.sleep(wait)\n                else:\n                    print(f\"HTTP Error: {e}\")\n                    break  # Break the loop for non-503 HTTP errors\n            except requests.RequestException as e:\n                print(f\"Request Exception: {e}\")\n                wait = initial_wait * (2 ** attempt)\n                print(f\"Network error. Retrying in {wait} seconds...\")\n                time.sleep(wait)\n        return None  # Return None to indicate failure after all retries\n\n\n    def fetch_data_for_aid(self, aid, columns_to_remove):\n        \"\"\"\n        Fetches and processes assay data for a specified Assay ID (AID) from the\n        PubChem database, preparing it for analysis or further processing.\n\n        This method queries the PubChem database for assay data associated with\n        a given AID. It constructs the query URL, sends the request using a\n        previously established session, and processes the response. The response\n        is expected to be in CSV format, which this method reads into a pandas\n        DataFrame. Specific columns can be removed from this DataFrame based on\n        the requirements for analysis. This allows for the customization of\n        the fetched data, making it easier to work with specific datasets.\n\n        If the request is successful and the data is fetched without issues,\n        it undergoes initial processing to remove unwanted columns as specified\n        by the 'columns_to_remove' parameter. In case of an error during the\n        data fetching or processing (e.g., issues with parsing the CSV data),\n        appropriate error messages are logged, and an empty DataFrame is\n        returned as a fallback.\n\n        Parameters:\n            aid (int): The assay ID for which data is to be fetched. This ID is\n            used to construct the query URL to the PubChem database.\n            columns_to_remove (list of str): A list of column names that should\n            be removed from the fetched DataFrame. This allows for the exclusion\n            of data that might not be relevant to the subsequent analysis or\n            processing steps.\n\n        Returns:\n            pandas.DataFrame: A DataFrame containing the processed data\n            associated with the given AID. The DataFrame will exclude columns\n            listed in 'columns_to_remove'. If the data fetching fails or if\n            an error occurs during processing, an empty DataFrame is returned.\n\n        Raises:\n            requests.RequestException: If an error occurs during the HTTP request\n            to the PubChem API. This includes scenarios such as timeout issues,\n            non-200 status codes, or network-related errors. The exception is\n            handled internally with logging, but it's important to be aware of\n            its possibility.\n            pd.errors.ParserError: If an error occurs while parsing the CSV\n            response from PubChem into a DataFrame. This could happen due to\n            malformed data or unexpected changes in the response format.\n            Like with RequestException, this error is logged and results in\n            the return of an empty DataFrame.\n\n        Example:\n            &gt;&gt;&gt; extractor = RelationshipPropertiesExtractor()\n            &gt;&gt;&gt; processed_data_df = extractor.fetch_data_for_aid(12345, ['UnwantedColumn1', 'UnwantedColumn2'])\n            &gt;&gt;&gt; print(processed_data_df.head())\n            This example demonstrates how to fetch and process assay data for\n            the assay with ID 12345, removing 'UnwantedColumn1' and\n            'UnwantedColumn2' from the resulting DataFrame. The first few rows\n            of the processed DataFrame are printed as an output.\n\n        Note:\n            - This method is part of a class that requires a valid session with\n            the PubChem API. Ensure that the class is properly initialized and that\n            the session is active.\n            - The removal of columns is an optional step and can be customized\n            based on the analysis needs. If no columns need to be removed, pass an\n            empty list as 'columns_to_remove'.\n        \"\"\"\n        url = (\n            \"https://pubchem.ncbi.nlm.nih.gov/assay/pcget.cgi?\"\n            \"query=download&amp;record_type=datatable&amp;actvty=\"\n            f\"all&amp;response_type=display&amp;aid={aid}\"\n        )\n\n        response = self._send_request(url)\n        if response and response.status_code == 200:\n            try:\n                compound_df = pd.read_csv(StringIO(response.text), sep=',')\n                # Drop specified columns and process column names in-place for memory efficiency\n                columns_to_remove_set = set(columns_to_remove)\n                existing_columns_set = set(compound_df.columns)\n                columns_to_actually_remove = list(columns_to_remove_set &amp; existing_columns_set)\n                compound_df.drop(columns=columns_to_actually_remove,\n                                 errors='ignore', inplace=True)\n                compound_df.rename(columns=lambda x: x.replace('PUBCHEM_', '') if x.startswith('PUBCHEM_') else x, inplace=True)\n\n                # compound_df.drop(columns=[col for col in columns_to_remove if col in compound_df.columns], errors='ignore', inplace=True)\n                # compound_df.columns = [col.replace('PUBCHEM_', '') if col.startswith('PUBCHEM_') else col for col in compound_df.columns]\n                compound_df['AID'] = aid\n                return compound_df\n            except pd.errors.ParserError as e:\n                logging.error(f\"CSV parsing failed for AID {aid}: {e}\")\n        else:\n            logging.error(f\"Failed to fetch data for AID {aid}. Status code: {response.status_code if response else 'No Response'}\")\n        return pd.DataFrame()  # Return an empty DataFrame in case of failure\n\n\n\n    def _process_dataframe(self, df, aid, columns_to_remove):\n        \"\"\"\n        Processes the DataFrame by removing specified columns and renaming others.\n\n        Parameters:\n            df (pandas.DataFrame): The DataFrame to be processed.\n            aid (int): The assay ID associated with the DataFrame.\n            columns_to_remove (list of str): Columns to be removed from the DataFrame.\n        \"\"\"\n        # Drop unnecessary columns efficiently\n        columns_to_remove_set = set(columns_to_remove)\n        df = df.drop(columns=list(columns_to_remove_set.intersection(df.columns)), errors='ignore')\n\n        # Efficiently rename columns that start with 'PUBCHEM_'\n        df.columns = [col.replace('PUBCHEM_', '') if col.startswith('PUBCHEM_') else col for col in df.columns]\n        df['AID'] = aid\n\n\n    def assay_compound_relationship(self, assays_data, start_chunk=0):\n        \"\"\"\n        Processes and stores relationships between assays and compounds based\n        on assay data from PubChem.\n\n        Parameters:\n            assays_data (str): Path to a CSV file containing assay IDs (AIDs).\n            start_chunk (int): The starting index for processing chunks.\n        \"\"\"\n        for chunk_idx, chunk in enumerate(pd.read_csv(assays_data, chunksize=100)):\n            if chunk_idx &gt;= start_chunk:\n                columns_to_remove = ['PUBCHEM_RESULT_TAG', 'PUBCHEM_SID', 'PUBCHEM_EXT_DATASOURCE_SMILES']\n                output_dir = 'Data/Relationships/Assay_Compound_Relationship'\n\n                for aid in chunk['AID']:\n                    if not os.path.exists(f'{output_dir}/AID_{aid}.csv'):\n                        df = self.fetch_data_for_aid(aid, columns_to_remove)\n                        if not df.empty:\n                            if not os.path.exists(output_dir):\n                                os.makedirs(output_dir)\n                            df.to_csv(f'{output_dir}/AID_{aid}.csv', index=False)\n                logging.info(f\"Processed chunk {chunk_idx} for assay-compound relationships.\")\n            else:\n                logging.info(f\"No More Chunck to Process.\")\n\n\n\n    def _write_to_csv(self, df, filename):\n        \"\"\"\n        Writes a DataFrame to a CSV file.\n        \"\"\"\n        df.to_csv(filename, index=False)\n\n\n    def assay_enzyme_relationship(self, main_data):\n        \"\"\"\n        Extracts and saves relationships between assays and enzymes from the\n        specified dataset.\n\n        This method processes assay data to identify relationships between\n        assays and their target enzymes. It selects relevant columns from the\n        input data, removes duplicates to ensure unique relationships, and saves\n        the cleaned data to a CSV file for further analysis or integration into\n        knowledge graphs.\n\n        Parameters:\n            main_data (str): Path to the CSV file containing the main data. The\n            file should include columns for 'AID' (Assay ID), 'Target GeneID',\n            and 'Activity Name'.\n\n        Returns:\n            pandas.DataFrame: A DataFrame containing the unique relationships\n            between assays and enzymes, including the assay ID, target gene ID,\n            and activity name.\n\n        Side Effects:\n            - Writes a CSV file to 'Data/Relationships/Assay_Enzyme_Relationship.csv',\n            containing the processed relationships data.\n        \"\"\"\n        df = pd.read_csv(main_data)\n        columns_to_select = ['AID', 'Target GeneID', 'Activity Name']\n        df = df[columns_to_select]\n        df = df.drop_duplicates(keep='first', ignore_index=True)\n        df.to_csv(f'Data/Relationships/Assay_Enzyme_Relationship.csv', index=False)\n        return df\n\n\n    def gene_enzyme_relationship(self, main_data):\n        \"\"\"\n        Extracts and saves relationships between genes and enzymes based on\n        the provided dataset.\n\n        This method selects relevant columns to highlight the relationships\n        between genes and their corresponding enzymes.\n        It removes duplicate entries to ensure that each relationship is\n        represented uniquely and saves the resultant data to\n        a CSV file. This facilitates easy integration of genetic data into\n        knowledge bases or further analysis.\n\n        Parameters:\n            main_data (str): Path to the CSV file containing gene and enzyme data.\n            Expected columns include 'Target GeneID' and 'Target Accession'.\n\n        Returns:\n            pandas.DataFrame: A DataFrame of unique gene-enzyme relationships,\n            including gene ID and enzyme accession numbers.\n\n        Side Effects:\n            - Writes the processed data to 'Data/Gene_Enzyme_Relationship.csv'\n            in a structured CSV format.\n        \"\"\"\n        df = pd.read_csv(main_data)\n        columns_to_select = ['Target GeneID', 'Target Accession']\n        df = df[columns_to_select]\n        df = df.drop_duplicates(keep='first', ignore_index=True)\n        df.to_csv(f'Data/Relationships/Gene_Enzyme_Relationship.csv', index=False)\n        return df\n\n\n    def compound_gene_relationship(self, main_data):\n        \"\"\"\n        Identifies and records relationships between compounds and enzymes from\n        the input data.\n\n        This method focuses on extracting compound-enzyme interaction data,\n        including activity outcomes and values. It selects\n        pertinent columns, removes duplicate records, and sorts the data by\n        Compound ID and Target Accession for clarity. The cleaned dataset is\n        then saved to a CSV file, providing a structured view  of how compounds\n        interact with various enzymes, which can be critical for drug discovery\n        and pharmacological research.\n\n        Parameters:\n            main_data (str): Path to the CSV file with compound and enzyme data.\n            This file should contain columns for 'CID' (Compound ID),\n            'Target Accession', 'Activity Outcome', 'Activity Name', and\n            'Activity Value [uM]'.\n\n        Returns:\n            pandas.DataFrame: A DataFrame with processed compound-enzyme\n            relationships, sorted and cleaned for direct analysis or database\n            insertion.\n\n        Side Effects:\n            - Saves the processed relationships data to\n            'Data/Relationships/Compound_Gene_Relationship.csv',\n            facilitating easy access and integration.\n        \"\"\"\n        df = pd.read_csv(main_data)\n        columns_to_select = ['CID', 'Target GeneID', 'Target Accession',\n                             'Activity Outcome', 'Activity Name',\n                             'Activity Value [uM]']\n        df = df[columns_to_select]\n        df = df.drop_duplicates(keep='first', ignore_index=True)\n        df = df.sort_values(['CID', 'Target Accession'])\n        df.dropna(axis=0 , thresh=1, inplace=True) ###\n        df.to_csv(f'Data/Relationships/Compound_Gene_Relationship.csv', index=False)\n        return df\n\n\n    def fetch_similar_cids(self, cid):\n        \"\"\"\n        Fetches similar compound IDs (CIDs) from the PubChem database for a\n        given compound ID (CID) using 2D similarity.\n\n        This method queries the PubChem database to find compounds that are\n        similar to the given CID based on 2D structural similarity.\n        The similarity threshold is set to 95%, and a maximum of 100 similar\n        CIDs are fetched. The response is parsed from XML format to extract\n        the similar CIDs.\n\n        Parameters:\n            cid (int): The compound ID for which similar CIDs are to be fetched.\n\n        Returns:\n            tuple: A tuple containing the original CID and a list of similar\n            CIDs. If an error occurs, the list of similar CIDs will be empty.\n\n        Raises:\n            Exception: Logs an error message with the original CID and the\n            exception if the request to PubChem fails or if parsing the XML\n            response encounters an error.\n\n        Note:\n            - The method utilizes the `requests` library for HTTP requests and\n            `xml.etree.ElementTree` for XML parsing.\n            - In case of a request failure or parsing error, the method logs\n            the error and returns the original CID with an empty list,\n            allowing the calling function to handle the exception as needed.\n        \"\"\"\n        url = (\"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/\"\n               f\"fastsimilarity_2d/cid/{int(cid)}/cids/XML?Threshold=95&amp;MaxRecords=100\")\n        try:\n            response = requests.get(url)\n            response.raise_for_status()\n            xml_data = response.text\n\n            # Parse XML data\n            tree = ET.parse(io.StringIO(xml_data))\n            root = tree.getroot()\n\n            # Extracting CID values\n            similar_cids = [element.text for element in root.findall('{http://pubchem.ncbi.nlm.nih.gov/pug_rest}CID')]\n            return cid, similar_cids\n        except Exception as e:\n            logging.error(f\"Error processing CID {cid}: {e}\")\n            return cid, []\n\n\n    def process_chunk(self, chunk):\n        \"\"\"\n        Processes a chunk of CIDs in parallel to fetch similar CIDs for each CID\n        in the chunk.\n\n        This method uses a ThreadPoolExecutor to send out concurrent requests for\n        fetching similar CIDs for a list of CIDs.\n        The number of worker threads is set to 5. Each CID's request is handled\n        by `fetch_similar_cids` method.\n\n        Parameters:\n            chunk (list of int): A list of compound IDs (CIDs) to process in\n            parallel.\n\n        Returns:\n            list of tuples: A list of tuples, each containing a CID and its\n            corresponding list of similar CIDs.\n\n        Side Effects:\n            - Utilizes concurrent threads to speed up the fetching process.\n            - May log errors if any occur during the fetching of similar CIDs\n            for individual CIDs.\n        \"\"\"\n        with ThreadPoolExecutor(max_workers=5) as executor:\n            futures = [executor.submit(self.fetch_similar_cids, cid) for cid in chunk]\n            results = [future.result() for future in as_completed(futures)]\n        return results\n\n\n    def compound_similarity_relationship(self, main_data, start_chunk=0):\n        \"\"\"\n        Identifies and records the similarity relationships between compounds\n        based on a list of CIDs. The similarity is detrmined by the Tanimoto\n        similarity coefficient with threshold 95% to ensure highe structural\n        similarity.\n\n        This method reads a CSV file containing compound data, filters compounds\n        based on specific 'Target GeneID' values,\n        and fetches similar CIDs for each compound. The compounds are processed\n        in chunks to manage memory usage and improve efficiency. The results are\n        saved into separate CSV files for each chunk.\n\n        Parameters:\n            main_data (str): Path to the CSV file containing the main compound data.\n            start_chunk (int): The starting index for processing chunks.\n        Note:\n            - The method filters the main data for compounds associated with\n            specific 'Target GeneID' values before fetching similar CIDs,\n            optimizing the process for relevant compounds only.\n            - The division of CIDs into chunks and concurrent processing helps\n            in managing large datasets and utilizes parallelism for faster\n            execution.\n        \"\"\"\n        df = pd.read_csv(main_data)\n        df = df[df['Target GeneID'].isin([1576, 1544, 1557, 1559, 1565])]\n        df = df.dropna(subset=['CID'])\n        IDs = df['CID'].unique().tolist()\n\n        chunk_size=10000\n        chunks = [IDs[i:i + chunk_size] for i in range(0, len(IDs), chunk_size)]\n\n        for i, chunk in enumerate(chunks, start=0):\n            if i &gt;= start_chunk:\n                chunk_results = self.process_chunk(chunk)\n                chunk_df = pd.DataFrame(chunk_results, columns=['CID', 'Similar CIDs'])\n                if not os.path.exists('Data/Relationships/Compound_Similarities'):\n                    os.makedirs('Data/Relationships/Compound_Similarities')\n                chunk_df.to_csv(f'Data/Relationships/Compound_Similarities/Chunk_{i}.csv', index=False)\n                logging.info(f\"Processed chunk {i} for compound similarity relationships.\")\n\n\n\n    def _fetch_data(self, cid):\n        \"\"\"\n        Fetches chemical-chemical and chemical-gene relationship data for a given\n        compound ID (CID). Checks if each data file exists before fetching.\n\n        Args:\n            cid (int): The compound ID for which data is to be fetched.\n\n        Returns:\n            tuple: A tuple containing the CID, and two lists of data\n            (chemical-chemical and chemical-gene relationships).\n        \"\"\"\n        cpd_cpd_file = f'Data/Relationships/Cpd_Cpd_CoOcuurence/CID_{cid}.csv'\n        cpd_gene_file = f'Data/Relationships/Cpd_gene_CoOcuurence/CID_{cid}.csv'\n\n        cpd_cpd_data = self._fetch_chemical_neighbor_data(cid) if not os.path.exists(cpd_cpd_file) else []\n        cpd_gene_data = self._fetch_chemical_gene_data(cid) if not os.path.exists(cpd_gene_file) else []\n\n        return cid, cpd_cpd_data, cpd_gene_data\n\n\n    def _fetch_chemical_neighbor_data(self, cid):\n        \"\"\"\n        Fetches chemical-chemical relationship data for a given CID.\n\n        Args:\n            cid (int): The compound ID for which data is to be fetched.\n\n        Returns:\n            list: List of chemical-chemical relationship data.\n        \"\"\"\n        cpd_cpd_url = (\"https://pubchem.ncbi.nlm.nih.gov/link_db/link_db_server.cgi?format=JSON&amp;type=\"\n                       f\"ChemicalNeighbor&amp;operation=GetAllLinks&amp;id_1={cid}&amp;response_type=display\")\n        try:\n            response = self._send_request(cpd_cpd_url)\n            data = response.json()\n            return data.get('LinkDataSet', {}).get('LinkData', [])\n        except Exception as e:\n            logging.error(f\"Failed to fetch chemical-chemical data for CID {cid}: {e}\")\n            return []\n\n\n    def _fetch_chemical_gene_data(self, cid):\n        \"\"\"\n        Fetches chemical-gene relationship data for a given CID.\n\n        Args:\n            cid (int): The compound ID for which data is to be fetched.\n\n        Returns:\n            list: List of chemical-gene relationship data.\n        \"\"\"\n        cpd_gene_url = (\"https://pubchem.ncbi.nlm.nih.gov/link_db/link_db_server.cgi?format=JSON&amp;type=\"\n                        f\"ChemicalGeneSymbolNeighbor&amp;operation=GetAllLinks&amp;id_1={cid}&amp;response_type=display\")\n        try:\n            response = self._send_request(cpd_gene_url)\n            data = response.json()\n            return data.get('LinkDataSet', {}).get('LinkData', [])\n        except Exception as e:\n            logging.error(f\"Failed to fetch chemical-gene data for CID {cid}: {e}\")\n            return []\n\n\n    def _write_data_to_csv(self, data, filename, filter_condition=None):\n        \"\"\"\n        Writes given data to a CSV file, with optional filtering before saving.\n\n        This method takes a list of dictionaries (data), converts it into a\n        pandas DataFrame, and optionally filters the DataFrame based on\n        specified conditions before writing the result to a CSV file. The\n        filtering is performed on specified columns with their expected\n        values provided in 'filter_condition'. This allows for selective\n        data saving, especially useful when dealing with large datasets\n        or when only a subset of data is needed for further processing\n        or analysis.\n\n        Parameters:\n            data (list of dict): Data to be written to a CSV file. Each\n            dictionary in the list represents a row in the DataFrame, with keys\n            as column names and values as row values.\n            filename (str): Path to the CSV file where the data will be saved.\n            If the file exists, it will be overwritten.\n            filter_condition (dict, optional): A dictionary specifying the\n            columns to filter by and the values to include. Keys in the\n            dictionary are column names, and values are lists of acceptable\n            values for that column. Rows not meeting the filter condition are\n            excluded from the final DataFrame to be saved.\n\n        Side Effects:\n            - Writes a CSV file to the given filename path. The file is overwritten\n            if it already exists.\n            - Logs a warning if a specified column for filtering is not found in\n            the DataFrame.\n        \"\"\"\n\n        df = pd.DataFrame(data)\n        if filter_condition:\n            for column, values in filter_condition.items():\n                if column in df.columns:\n                    df = df[df[column].isin(values)]\n                else:\n                    logging.warning(f\"Column {column} not found in DataFrame.\")\n        if not df.empty:\n            df.to_csv(filename, index=False)\n\n\n    def compound_cooccurrence(self, main_data, rate_limit=5, start_chunk=0):\n        \"\"\"\n        Analyzes compound co-occurrence relationships from the specified main\n        data file and saves the results into structured CSV files.\n\n        This method takes a path to a CSV file containing compound data and\n        performs batch processing to extract relationships between compounds\n        and genes from the PubChem database. It filters compounds based on their\n        association with specific genes of interest, then fetches co-occurrence\n        data for each compound using parallel requests. The data fetched\n        includes both compound-compound and compound-gene co-occurrence\n        relationships. Results are saved in separate CSV files within specific\n        directories for later analysis.\n\n        Parameters:\n            main_data (str): Path to the CSV file containing the main data. This\n            file should include 'CID' (Compound ID) and 'Target GeneID' columns.\n            rate_limit (int): Controls the rate of API requests to avoid\n            exceeding PubChem's request limits. Specifies the maximum number of\n            requests that can be made per second.\n            start_chunk (int): The starting index for processing chunks.\n\n        Returns:\n            str: A message indicating the successful completion of data\n            processing and saving.\n\n        Raises:\n            FileNotFoundError: If the specified 'main_data' file does not exist\n            or cannot be read.\n            ValueError: If 'main_data' does not contain the required columns\n            ('CID' and 'Target GeneID').\n\n        Example:\n            &gt;&gt;&gt; extractor = RelationshipPropertiesExtractor()\n            &gt;&gt;&gt; completion_message = extractor.compound_cooccurrence('Data/AllDataConnected.csv', rate_limit=5)\n            &gt;&gt;&gt; print(completion_message)\n            This would process the compound data, fetch co-occurrence data from\n            PubChem, and save the results into CSV files.\n            The completion message would indicate successful processing.\n\n        Note:\n            The 'main_data' file must be properly formatted, with at least 'CID'\n            and 'Target GeneID' columns present. The method assumes the existence\n            of 'Data/Relationships/Cpd_Cpd_CoOcuurence' and\n            'Data/Relationships/Cpd_gene_CoOcuurence' directories for saving\n            the output CSV files. It is recommended to check and adhere to\n            PubChem's current rate limits when setting the 'rate_limit'\n            parameter to avoid potential blocks or restrictions on your\n            IP address due to excessive requests.\n        \"\"\"\n        df = pd.read_csv(main_data, chunksize=3000)  # Reading in chunks for large files\n        for chunk_idx, chunk in enumerate(df):\n            if chunk_idx &gt;= start_chunk:\n                chunk = chunk[chunk['Target GeneID'].isin([1576, 1544, 1557, 1559, 1565])]\n                chunk.dropna(subset=['CID'], inplace=True)\n                IDs = chunk['CID'].unique().tolist()\n\n                start_time = timeit.default_timer()\n                with ThreadPoolExecutor(max_workers=rate_limit) as executor:\n                    futures = {executor.submit(self._fetch_data, int(cid)): cid for cid in IDs}\n                    for future in as_completed(futures):\n                        cid, cpd_cpd_data, cpd_gene_data = future.result()\n                        self._write_data_to_csv(cpd_cpd_data, f'Data/Relationships/Cpd_Cpd_CoOcuurence/CID_{cid}.csv')\n                        self._write_data_to_csv(cpd_gene_data, f'Data/Relationships/Cpd_gene_CoOcuurence/CID_{cid}.csv',\n                                                filter_condition={\"ID_2\": [\"{'GeneSymbol': 'cyp3a4'}\", \"{'GeneSymbol': 'cyp1a2'}\",\n                                                                           \"{'GeneSymbol': 'cyp2c9'}\", \"{'GeneSymbol': 'cyp2c19'}\",\n                                                                           \"{'GeneSymbol': 'cyp2d6'}\"]})\n                        time.sleep(1 / rate_limit)  # Ensuring we don't exceed rate limit\n                elapsed = timeit.default_timer() - start_time\n                logging.info(f\"Processed chunk {chunk_idx} in {elapsed:.2f} seconds\")\n\n        return \"Data fetching and saving completed.\"\n\n\n    def compound_transformation(self, gene_properties):\n        \"\"\"\n        Analyzes compound transformation data based on gene properties, focusing\n        on metabolic transformations involving specified genes. This method\n        queries the PubChem database for transformation data related\n        to compounds associated with the genes identified in the provided CSV file.\n\n        Parameters:\n            gene_properties (str): Path to the CSV file containing gene properties\n            generated by the NodePropertiesExtractor class, which should include\n            'GeneID' as one of its columns. This file is used to identify genes\n            of interest for which compound transformation data will be fetched.\n\n        Processing Steps:\n            1. Reads the provided CSV file to extract unique gene identifiers.\n            2. For each gene identifier, constructs a query to fetch relevant\n            compound transformation data from PubChem, focusing on metabolic\n            transformations where the gene plays a role.\n            3. Processes and aggregates the fetched data into a structured\n            pandas DataFrame.\n            4. Filters the aggregated data to retain specific columns relevant\n            to compound transformations, including substrate and metabolite\n            Compound IDs (CIDs), the type of metabolic conversion, gene\n            identifiers, PubMed IDs, and DOIs for related publications.\n            5. Saves the aggregated and filtered DataFrame to a CSV file for\n            further analysis or integration into knowledge graphs or other\n            data models.\n\n        Returns:\n            pandas.DataFrame: A DataFrame containing processed compound\n            transformation data, including substrate and metabolite CIDs,\n            metabolic conversion types, gene identifiers, PubMed IDs, and DOIs.\n            The DataFrame structure facilitates further analysis or use in\n            constructing detailed views of metabolic pathways involving the\n            specified genes.\n\n        Side Effects:\n            - Saves the aggregated compound transformation data to\n            'Data/Relationships/Compound_Transformation.csv'\n            in the current working directory. This file captures the relationship\n            between substrates, metabolites, and genes based on the input gene\n            properties.\n\n        Raises:\n            FileNotFoundError: If the specified 'gene_properties' file does not\n            exist or cannot be read.\n            ValueError: If 'gene_properties' does not contain the required\n            'GeneID' column.\n\n        Example:\n            &gt;&gt;&gt; extractor = RelationshipPropertiesExtractor()\n            &gt;&gt;&gt; transformation_df = extractor.compound_transformation('Data/Nodes/gene_properties.csv')\n            &gt;&gt;&gt; print(transformation_df.head())\n            This example processes gene properties from\n            'path/to/gene_properties.csv', queries PubChem for\n            compound transformation data related to the genes,\n            and compiles the results into a DataFrame.\n\n        Note:\n            The method assumes that the input 'gene_properties' file is\n            accessible and correctly formatted.\n            The availability and structure of the PubChem database may affect\n            the completeness and accuracy of the fetched transformation data.\n            Users should verify the existence of the 'Data/Relationships'\n            directory and have appropriate permissions to write files to it.\n        \"\"\"\n        df = pd.read_csv(gene_properties)\n        IDs = df['GeneID'].unique().tolist()\n\n        transformation_dfs = []\n\n        for gid in IDs:\n            gid = int(gid)\n            url = (\"https://pubchem.ncbi.nlm.nih.gov/sdq/sdqagent.cgi?infmt=json&amp;outfmt=csv\"\n                   \"&amp;query={{\\\"download\\\":\\\"*\\\",\\\"collection\\\":\\\"chemblmetabolism\\\",\\\"where\\\":\"\n                   f\"{{\\\"ands\\\":[{{\\\"geneid\\\":\\\"{gid}\\\"}}]}},\\\"order\\\":[\\\"relevancescore,desc\\\"]\"\n                   f\",\\\"start\\\":1,\\\"limit\\\":10000000,\\\"downloadfilename\\\":\\\"pubchem_geneid_{gid}_chemblmetabolism\\\"}}\")\n\n            response = self._send_request(url)\n            if response:\n                try:\n                    transformation_df = pd.read_csv(StringIO(response.text),\n                                                    sep=',', header=0,\n                                                    low_memory=False)\n                    transformation_df = transformation_df[['substratecid',\n                                                           'metabolitecid',\n                                                           'metconversion',\n                                                           'geneids',\n                                                           'pmids',\n                                                           'dois']]\n                    transformation_dfs.append(transformation_df)\n                except pd.errors.ParserError as e:\n                    print(f\"Error parsing CSV for gene ID {gid}: {e}\")\n                    continue  # Skip this gene ID and continue with others\n\n        transformation_df = pd.concat(transformation_dfs, ignore_index=True) if transformation_dfs else pd.DataFrame()\n        self._write_to_csv(transformation_df,\n                           'Data/Relationships/Compound_Transformation.csv')\n\n        return transformation_df\n</code></pre>"},{"location":"documentation/#chemgraphbuilder.relationship_properties_extractor.RelationshipPropertiesExtractor.__init__","title":"<code>__init__()</code>","text":"<p>Initializes a RelationshipPropertiesExtractor with a Requests session for efficient network calls.</p> Source code in <code>chemgraphbuilder/relationship_properties_extractor.py</code> <pre><code>def __init__(self):\n    \"\"\"Initializes a RelationshipPropertiesExtractor with a Requests session\n     for efficient network calls.\"\"\"\n    self.session = requests.Session()\n</code></pre>"},{"location":"documentation/#chemgraphbuilder.relationship_properties_extractor.RelationshipPropertiesExtractor.assay_compound_relationship","title":"<code>assay_compound_relationship(assays_data, start_chunk=0)</code>","text":"<p>Processes and stores relationships between assays and compounds based on assay data from PubChem.</p> <p>Parameters:</p> Name Type Description Default <code>assays_data</code> <code>str</code> <p>Path to a CSV file containing assay IDs (AIDs).</p> required <code>start_chunk</code> <code>int</code> <p>The starting index for processing chunks.</p> <code>0</code> Source code in <code>chemgraphbuilder/relationship_properties_extractor.py</code> <pre><code>def assay_compound_relationship(self, assays_data, start_chunk=0):\n    \"\"\"\n    Processes and stores relationships between assays and compounds based\n    on assay data from PubChem.\n\n    Parameters:\n        assays_data (str): Path to a CSV file containing assay IDs (AIDs).\n        start_chunk (int): The starting index for processing chunks.\n    \"\"\"\n    for chunk_idx, chunk in enumerate(pd.read_csv(assays_data, chunksize=100)):\n        if chunk_idx &gt;= start_chunk:\n            columns_to_remove = ['PUBCHEM_RESULT_TAG', 'PUBCHEM_SID', 'PUBCHEM_EXT_DATASOURCE_SMILES']\n            output_dir = 'Data/Relationships/Assay_Compound_Relationship'\n\n            for aid in chunk['AID']:\n                if not os.path.exists(f'{output_dir}/AID_{aid}.csv'):\n                    df = self.fetch_data_for_aid(aid, columns_to_remove)\n                    if not df.empty:\n                        if not os.path.exists(output_dir):\n                            os.makedirs(output_dir)\n                        df.to_csv(f'{output_dir}/AID_{aid}.csv', index=False)\n            logging.info(f\"Processed chunk {chunk_idx} for assay-compound relationships.\")\n        else:\n            logging.info(f\"No More Chunck to Process.\")\n</code></pre>"},{"location":"documentation/#chemgraphbuilder.relationship_properties_extractor.RelationshipPropertiesExtractor.assay_enzyme_relationship","title":"<code>assay_enzyme_relationship(main_data)</code>","text":"<p>Extracts and saves relationships between assays and enzymes from the specified dataset.</p> <p>This method processes assay data to identify relationships between assays and their target enzymes. It selects relevant columns from the input data, removes duplicates to ensure unique relationships, and saves the cleaned data to a CSV file for further analysis or integration into knowledge graphs.</p> <p>Parameters:</p> Name Type Description Default <code>main_data</code> <code>str</code> <p>Path to the CSV file containing the main data. The</p> required <p>Returns:</p> Type Description <p>pandas.DataFrame: A DataFrame containing the unique relationships</p> <p>between assays and enzymes, including the assay ID, target gene ID,</p> <p>and activity name.</p> Side Effects <ul> <li>Writes a CSV file to 'Data/Relationships/Assay_Enzyme_Relationship.csv', containing the processed relationships data.</li> </ul> Source code in <code>chemgraphbuilder/relationship_properties_extractor.py</code> <pre><code>def assay_enzyme_relationship(self, main_data):\n    \"\"\"\n    Extracts and saves relationships between assays and enzymes from the\n    specified dataset.\n\n    This method processes assay data to identify relationships between\n    assays and their target enzymes. It selects relevant columns from the\n    input data, removes duplicates to ensure unique relationships, and saves\n    the cleaned data to a CSV file for further analysis or integration into\n    knowledge graphs.\n\n    Parameters:\n        main_data (str): Path to the CSV file containing the main data. The\n        file should include columns for 'AID' (Assay ID), 'Target GeneID',\n        and 'Activity Name'.\n\n    Returns:\n        pandas.DataFrame: A DataFrame containing the unique relationships\n        between assays and enzymes, including the assay ID, target gene ID,\n        and activity name.\n\n    Side Effects:\n        - Writes a CSV file to 'Data/Relationships/Assay_Enzyme_Relationship.csv',\n        containing the processed relationships data.\n    \"\"\"\n    df = pd.read_csv(main_data)\n    columns_to_select = ['AID', 'Target GeneID', 'Activity Name']\n    df = df[columns_to_select]\n    df = df.drop_duplicates(keep='first', ignore_index=True)\n    df.to_csv(f'Data/Relationships/Assay_Enzyme_Relationship.csv', index=False)\n    return df\n</code></pre>"},{"location":"documentation/#chemgraphbuilder.relationship_properties_extractor.RelationshipPropertiesExtractor.compound_cooccurrence","title":"<code>compound_cooccurrence(main_data, rate_limit=5, start_chunk=0)</code>","text":"<p>Analyzes compound co-occurrence relationships from the specified main data file and saves the results into structured CSV files.</p> <p>This method takes a path to a CSV file containing compound data and performs batch processing to extract relationships between compounds and genes from the PubChem database. It filters compounds based on their association with specific genes of interest, then fetches co-occurrence data for each compound using parallel requests. The data fetched includes both compound-compound and compound-gene co-occurrence relationships. Results are saved in separate CSV files within specific directories for later analysis.</p> <p>Parameters:</p> Name Type Description Default <code>main_data</code> <code>str</code> <p>Path to the CSV file containing the main data. This</p> required <code>rate_limit</code> <code>int</code> <p>Controls the rate of API requests to avoid</p> <code>5</code> <code>start_chunk</code> <code>int</code> <p>The starting index for processing chunks.</p> <code>0</code> <p>Returns:</p> Name Type Description <code>str</code> <p>A message indicating the successful completion of data</p> <p>processing and saving.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified 'main_data' file does not exist</p> <code>ValueError</code> <p>If 'main_data' does not contain the required columns</p> Example <p>extractor = RelationshipPropertiesExtractor() completion_message = extractor.compound_cooccurrence('Data/AllDataConnected.csv', rate_limit=5) print(completion_message) This would process the compound data, fetch co-occurrence data from PubChem, and save the results into CSV files. The completion message would indicate successful processing.</p> Note <p>The 'main_data' file must be properly formatted, with at least 'CID' and 'Target GeneID' columns present. The method assumes the existence of 'Data/Relationships/Cpd_Cpd_CoOcuurence' and 'Data/Relationships/Cpd_gene_CoOcuurence' directories for saving the output CSV files. It is recommended to check and adhere to PubChem's current rate limits when setting the 'rate_limit' parameter to avoid potential blocks or restrictions on your IP address due to excessive requests.</p> Source code in <code>chemgraphbuilder/relationship_properties_extractor.py</code> <pre><code>def compound_cooccurrence(self, main_data, rate_limit=5, start_chunk=0):\n    \"\"\"\n    Analyzes compound co-occurrence relationships from the specified main\n    data file and saves the results into structured CSV files.\n\n    This method takes a path to a CSV file containing compound data and\n    performs batch processing to extract relationships between compounds\n    and genes from the PubChem database. It filters compounds based on their\n    association with specific genes of interest, then fetches co-occurrence\n    data for each compound using parallel requests. The data fetched\n    includes both compound-compound and compound-gene co-occurrence\n    relationships. Results are saved in separate CSV files within specific\n    directories for later analysis.\n\n    Parameters:\n        main_data (str): Path to the CSV file containing the main data. This\n        file should include 'CID' (Compound ID) and 'Target GeneID' columns.\n        rate_limit (int): Controls the rate of API requests to avoid\n        exceeding PubChem's request limits. Specifies the maximum number of\n        requests that can be made per second.\n        start_chunk (int): The starting index for processing chunks.\n\n    Returns:\n        str: A message indicating the successful completion of data\n        processing and saving.\n\n    Raises:\n        FileNotFoundError: If the specified 'main_data' file does not exist\n        or cannot be read.\n        ValueError: If 'main_data' does not contain the required columns\n        ('CID' and 'Target GeneID').\n\n    Example:\n        &gt;&gt;&gt; extractor = RelationshipPropertiesExtractor()\n        &gt;&gt;&gt; completion_message = extractor.compound_cooccurrence('Data/AllDataConnected.csv', rate_limit=5)\n        &gt;&gt;&gt; print(completion_message)\n        This would process the compound data, fetch co-occurrence data from\n        PubChem, and save the results into CSV files.\n        The completion message would indicate successful processing.\n\n    Note:\n        The 'main_data' file must be properly formatted, with at least 'CID'\n        and 'Target GeneID' columns present. The method assumes the existence\n        of 'Data/Relationships/Cpd_Cpd_CoOcuurence' and\n        'Data/Relationships/Cpd_gene_CoOcuurence' directories for saving\n        the output CSV files. It is recommended to check and adhere to\n        PubChem's current rate limits when setting the 'rate_limit'\n        parameter to avoid potential blocks or restrictions on your\n        IP address due to excessive requests.\n    \"\"\"\n    df = pd.read_csv(main_data, chunksize=3000)  # Reading in chunks for large files\n    for chunk_idx, chunk in enumerate(df):\n        if chunk_idx &gt;= start_chunk:\n            chunk = chunk[chunk['Target GeneID'].isin([1576, 1544, 1557, 1559, 1565])]\n            chunk.dropna(subset=['CID'], inplace=True)\n            IDs = chunk['CID'].unique().tolist()\n\n            start_time = timeit.default_timer()\n            with ThreadPoolExecutor(max_workers=rate_limit) as executor:\n                futures = {executor.submit(self._fetch_data, int(cid)): cid for cid in IDs}\n                for future in as_completed(futures):\n                    cid, cpd_cpd_data, cpd_gene_data = future.result()\n                    self._write_data_to_csv(cpd_cpd_data, f'Data/Relationships/Cpd_Cpd_CoOcuurence/CID_{cid}.csv')\n                    self._write_data_to_csv(cpd_gene_data, f'Data/Relationships/Cpd_gene_CoOcuurence/CID_{cid}.csv',\n                                            filter_condition={\"ID_2\": [\"{'GeneSymbol': 'cyp3a4'}\", \"{'GeneSymbol': 'cyp1a2'}\",\n                                                                       \"{'GeneSymbol': 'cyp2c9'}\", \"{'GeneSymbol': 'cyp2c19'}\",\n                                                                       \"{'GeneSymbol': 'cyp2d6'}\"]})\n                    time.sleep(1 / rate_limit)  # Ensuring we don't exceed rate limit\n            elapsed = timeit.default_timer() - start_time\n            logging.info(f\"Processed chunk {chunk_idx} in {elapsed:.2f} seconds\")\n\n    return \"Data fetching and saving completed.\"\n</code></pre>"},{"location":"documentation/#chemgraphbuilder.relationship_properties_extractor.RelationshipPropertiesExtractor.compound_gene_relationship","title":"<code>compound_gene_relationship(main_data)</code>","text":"<p>Identifies and records relationships between compounds and enzymes from the input data.</p> <p>This method focuses on extracting compound-enzyme interaction data, including activity outcomes and values. It selects pertinent columns, removes duplicate records, and sorts the data by Compound ID and Target Accession for clarity. The cleaned dataset is then saved to a CSV file, providing a structured view  of how compounds interact with various enzymes, which can be critical for drug discovery and pharmacological research.</p> <p>Parameters:</p> Name Type Description Default <code>main_data</code> <code>str</code> <p>Path to the CSV file with compound and enzyme data.</p> required <p>Returns:</p> Type Description <p>pandas.DataFrame: A DataFrame with processed compound-enzyme</p> <p>relationships, sorted and cleaned for direct analysis or database</p> <p>insertion.</p> Side Effects <ul> <li>Saves the processed relationships data to 'Data/Relationships/Compound_Gene_Relationship.csv', facilitating easy access and integration.</li> </ul> Source code in <code>chemgraphbuilder/relationship_properties_extractor.py</code> <pre><code>def compound_gene_relationship(self, main_data):\n    \"\"\"\n    Identifies and records relationships between compounds and enzymes from\n    the input data.\n\n    This method focuses on extracting compound-enzyme interaction data,\n    including activity outcomes and values. It selects\n    pertinent columns, removes duplicate records, and sorts the data by\n    Compound ID and Target Accession for clarity. The cleaned dataset is\n    then saved to a CSV file, providing a structured view  of how compounds\n    interact with various enzymes, which can be critical for drug discovery\n    and pharmacological research.\n\n    Parameters:\n        main_data (str): Path to the CSV file with compound and enzyme data.\n        This file should contain columns for 'CID' (Compound ID),\n        'Target Accession', 'Activity Outcome', 'Activity Name', and\n        'Activity Value [uM]'.\n\n    Returns:\n        pandas.DataFrame: A DataFrame with processed compound-enzyme\n        relationships, sorted and cleaned for direct analysis or database\n        insertion.\n\n    Side Effects:\n        - Saves the processed relationships data to\n        'Data/Relationships/Compound_Gene_Relationship.csv',\n        facilitating easy access and integration.\n    \"\"\"\n    df = pd.read_csv(main_data)\n    columns_to_select = ['CID', 'Target GeneID', 'Target Accession',\n                         'Activity Outcome', 'Activity Name',\n                         'Activity Value [uM]']\n    df = df[columns_to_select]\n    df = df.drop_duplicates(keep='first', ignore_index=True)\n    df = df.sort_values(['CID', 'Target Accession'])\n    df.dropna(axis=0 , thresh=1, inplace=True) ###\n    df.to_csv(f'Data/Relationships/Compound_Gene_Relationship.csv', index=False)\n    return df\n</code></pre>"},{"location":"documentation/#chemgraphbuilder.relationship_properties_extractor.RelationshipPropertiesExtractor.compound_similarity_relationship","title":"<code>compound_similarity_relationship(main_data, start_chunk=0)</code>","text":"<p>Identifies and records the similarity relationships between compounds based on a list of CIDs. The similarity is detrmined by the Tanimoto similarity coefficient with threshold 95% to ensure highe structural similarity.</p> <p>This method reads a CSV file containing compound data, filters compounds based on specific 'Target GeneID' values, and fetches similar CIDs for each compound. The compounds are processed in chunks to manage memory usage and improve efficiency. The results are saved into separate CSV files for each chunk.</p> <p>Parameters:</p> Name Type Description Default <code>main_data</code> <code>str</code> <p>Path to the CSV file containing the main compound data.</p> required <code>start_chunk</code> <code>int</code> <p>The starting index for processing chunks.</p> <code>0</code> <p>Note:     - The method filters the main data for compounds associated with     specific 'Target GeneID' values before fetching similar CIDs,     optimizing the process for relevant compounds only.     - The division of CIDs into chunks and concurrent processing helps     in managing large datasets and utilizes parallelism for faster     execution.</p> Source code in <code>chemgraphbuilder/relationship_properties_extractor.py</code> <pre><code>def compound_similarity_relationship(self, main_data, start_chunk=0):\n    \"\"\"\n    Identifies and records the similarity relationships between compounds\n    based on a list of CIDs. The similarity is detrmined by the Tanimoto\n    similarity coefficient with threshold 95% to ensure highe structural\n    similarity.\n\n    This method reads a CSV file containing compound data, filters compounds\n    based on specific 'Target GeneID' values,\n    and fetches similar CIDs for each compound. The compounds are processed\n    in chunks to manage memory usage and improve efficiency. The results are\n    saved into separate CSV files for each chunk.\n\n    Parameters:\n        main_data (str): Path to the CSV file containing the main compound data.\n        start_chunk (int): The starting index for processing chunks.\n    Note:\n        - The method filters the main data for compounds associated with\n        specific 'Target GeneID' values before fetching similar CIDs,\n        optimizing the process for relevant compounds only.\n        - The division of CIDs into chunks and concurrent processing helps\n        in managing large datasets and utilizes parallelism for faster\n        execution.\n    \"\"\"\n    df = pd.read_csv(main_data)\n    df = df[df['Target GeneID'].isin([1576, 1544, 1557, 1559, 1565])]\n    df = df.dropna(subset=['CID'])\n    IDs = df['CID'].unique().tolist()\n\n    chunk_size=10000\n    chunks = [IDs[i:i + chunk_size] for i in range(0, len(IDs), chunk_size)]\n\n    for i, chunk in enumerate(chunks, start=0):\n        if i &gt;= start_chunk:\n            chunk_results = self.process_chunk(chunk)\n            chunk_df = pd.DataFrame(chunk_results, columns=['CID', 'Similar CIDs'])\n            if not os.path.exists('Data/Relationships/Compound_Similarities'):\n                os.makedirs('Data/Relationships/Compound_Similarities')\n            chunk_df.to_csv(f'Data/Relationships/Compound_Similarities/Chunk_{i}.csv', index=False)\n            logging.info(f\"Processed chunk {i} for compound similarity relationships.\")\n</code></pre>"},{"location":"documentation/#chemgraphbuilder.relationship_properties_extractor.RelationshipPropertiesExtractor.compound_transformation","title":"<code>compound_transformation(gene_properties)</code>","text":"<p>Analyzes compound transformation data based on gene properties, focusing on metabolic transformations involving specified genes. This method queries the PubChem database for transformation data related to compounds associated with the genes identified in the provided CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>gene_properties</code> <code>str</code> <p>Path to the CSV file containing gene properties</p> required Processing Steps <ol> <li>Reads the provided CSV file to extract unique gene identifiers.</li> <li>For each gene identifier, constructs a query to fetch relevant compound transformation data from PubChem, focusing on metabolic transformations where the gene plays a role.</li> <li>Processes and aggregates the fetched data into a structured pandas DataFrame.</li> <li>Filters the aggregated data to retain specific columns relevant to compound transformations, including substrate and metabolite Compound IDs (CIDs), the type of metabolic conversion, gene identifiers, PubMed IDs, and DOIs for related publications.</li> <li>Saves the aggregated and filtered DataFrame to a CSV file for further analysis or integration into knowledge graphs or other data models.</li> </ol> <p>Returns:</p> Type Description <p>pandas.DataFrame: A DataFrame containing processed compound</p> <p>transformation data, including substrate and metabolite CIDs,</p> <p>metabolic conversion types, gene identifiers, PubMed IDs, and DOIs.</p> <p>The DataFrame structure facilitates further analysis or use in</p> <p>constructing detailed views of metabolic pathways involving the</p> <p>specified genes.</p> Side Effects <ul> <li>Saves the aggregated compound transformation data to 'Data/Relationships/Compound_Transformation.csv' in the current working directory. This file captures the relationship between substrates, metabolites, and genes based on the input gene properties.</li> </ul> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified 'gene_properties' file does not</p> <code>ValueError</code> <p>If 'gene_properties' does not contain the required</p> Example <p>extractor = RelationshipPropertiesExtractor() transformation_df = extractor.compound_transformation('Data/Nodes/gene_properties.csv') print(transformation_df.head()) This example processes gene properties from 'path/to/gene_properties.csv', queries PubChem for compound transformation data related to the genes, and compiles the results into a DataFrame.</p> Note <p>The method assumes that the input 'gene_properties' file is accessible and correctly formatted. The availability and structure of the PubChem database may affect the completeness and accuracy of the fetched transformation data. Users should verify the existence of the 'Data/Relationships' directory and have appropriate permissions to write files to it.</p> Source code in <code>chemgraphbuilder/relationship_properties_extractor.py</code> <pre><code>def compound_transformation(self, gene_properties):\n    \"\"\"\n    Analyzes compound transformation data based on gene properties, focusing\n    on metabolic transformations involving specified genes. This method\n    queries the PubChem database for transformation data related\n    to compounds associated with the genes identified in the provided CSV file.\n\n    Parameters:\n        gene_properties (str): Path to the CSV file containing gene properties\n        generated by the NodePropertiesExtractor class, which should include\n        'GeneID' as one of its columns. This file is used to identify genes\n        of interest for which compound transformation data will be fetched.\n\n    Processing Steps:\n        1. Reads the provided CSV file to extract unique gene identifiers.\n        2. For each gene identifier, constructs a query to fetch relevant\n        compound transformation data from PubChem, focusing on metabolic\n        transformations where the gene plays a role.\n        3. Processes and aggregates the fetched data into a structured\n        pandas DataFrame.\n        4. Filters the aggregated data to retain specific columns relevant\n        to compound transformations, including substrate and metabolite\n        Compound IDs (CIDs), the type of metabolic conversion, gene\n        identifiers, PubMed IDs, and DOIs for related publications.\n        5. Saves the aggregated and filtered DataFrame to a CSV file for\n        further analysis or integration into knowledge graphs or other\n        data models.\n\n    Returns:\n        pandas.DataFrame: A DataFrame containing processed compound\n        transformation data, including substrate and metabolite CIDs,\n        metabolic conversion types, gene identifiers, PubMed IDs, and DOIs.\n        The DataFrame structure facilitates further analysis or use in\n        constructing detailed views of metabolic pathways involving the\n        specified genes.\n\n    Side Effects:\n        - Saves the aggregated compound transformation data to\n        'Data/Relationships/Compound_Transformation.csv'\n        in the current working directory. This file captures the relationship\n        between substrates, metabolites, and genes based on the input gene\n        properties.\n\n    Raises:\n        FileNotFoundError: If the specified 'gene_properties' file does not\n        exist or cannot be read.\n        ValueError: If 'gene_properties' does not contain the required\n        'GeneID' column.\n\n    Example:\n        &gt;&gt;&gt; extractor = RelationshipPropertiesExtractor()\n        &gt;&gt;&gt; transformation_df = extractor.compound_transformation('Data/Nodes/gene_properties.csv')\n        &gt;&gt;&gt; print(transformation_df.head())\n        This example processes gene properties from\n        'path/to/gene_properties.csv', queries PubChem for\n        compound transformation data related to the genes,\n        and compiles the results into a DataFrame.\n\n    Note:\n        The method assumes that the input 'gene_properties' file is\n        accessible and correctly formatted.\n        The availability and structure of the PubChem database may affect\n        the completeness and accuracy of the fetched transformation data.\n        Users should verify the existence of the 'Data/Relationships'\n        directory and have appropriate permissions to write files to it.\n    \"\"\"\n    df = pd.read_csv(gene_properties)\n    IDs = df['GeneID'].unique().tolist()\n\n    transformation_dfs = []\n\n    for gid in IDs:\n        gid = int(gid)\n        url = (\"https://pubchem.ncbi.nlm.nih.gov/sdq/sdqagent.cgi?infmt=json&amp;outfmt=csv\"\n               \"&amp;query={{\\\"download\\\":\\\"*\\\",\\\"collection\\\":\\\"chemblmetabolism\\\",\\\"where\\\":\"\n               f\"{{\\\"ands\\\":[{{\\\"geneid\\\":\\\"{gid}\\\"}}]}},\\\"order\\\":[\\\"relevancescore,desc\\\"]\"\n               f\",\\\"start\\\":1,\\\"limit\\\":10000000,\\\"downloadfilename\\\":\\\"pubchem_geneid_{gid}_chemblmetabolism\\\"}}\")\n\n        response = self._send_request(url)\n        if response:\n            try:\n                transformation_df = pd.read_csv(StringIO(response.text),\n                                                sep=',', header=0,\n                                                low_memory=False)\n                transformation_df = transformation_df[['substratecid',\n                                                       'metabolitecid',\n                                                       'metconversion',\n                                                       'geneids',\n                                                       'pmids',\n                                                       'dois']]\n                transformation_dfs.append(transformation_df)\n            except pd.errors.ParserError as e:\n                print(f\"Error parsing CSV for gene ID {gid}: {e}\")\n                continue  # Skip this gene ID and continue with others\n\n    transformation_df = pd.concat(transformation_dfs, ignore_index=True) if transformation_dfs else pd.DataFrame()\n    self._write_to_csv(transformation_df,\n                       'Data/Relationships/Compound_Transformation.csv')\n\n    return transformation_df\n</code></pre>"},{"location":"documentation/#chemgraphbuilder.relationship_properties_extractor.RelationshipPropertiesExtractor.fetch_data_for_aid","title":"<code>fetch_data_for_aid(aid, columns_to_remove)</code>","text":"<p>Fetches and processes assay data for a specified Assay ID (AID) from the PubChem database, preparing it for analysis or further processing.</p> <p>This method queries the PubChem database for assay data associated with a given AID. It constructs the query URL, sends the request using a previously established session, and processes the response. The response is expected to be in CSV format, which this method reads into a pandas DataFrame. Specific columns can be removed from this DataFrame based on the requirements for analysis. This allows for the customization of the fetched data, making it easier to work with specific datasets.</p> <p>If the request is successful and the data is fetched without issues, it undergoes initial processing to remove unwanted columns as specified by the 'columns_to_remove' parameter. In case of an error during the data fetching or processing (e.g., issues with parsing the CSV data), appropriate error messages are logged, and an empty DataFrame is returned as a fallback.</p> <p>Parameters:</p> Name Type Description Default <code>aid</code> <code>int</code> <p>The assay ID for which data is to be fetched. This ID is</p> required <code>columns_to_remove</code> <code>list of str</code> <p>A list of column names that should</p> required <p>Returns:</p> Type Description <p>pandas.DataFrame: A DataFrame containing the processed data</p> <p>associated with the given AID. The DataFrame will exclude columns</p> <p>listed in 'columns_to_remove'. If the data fetching fails or if</p> <p>an error occurs during processing, an empty DataFrame is returned.</p> <p>Raises:</p> Type Description <code>RequestException</code> <p>If an error occurs during the HTTP request</p> <code>ParserError</code> <p>If an error occurs while parsing the CSV</p> Example <p>extractor = RelationshipPropertiesExtractor() processed_data_df = extractor.fetch_data_for_aid(12345, ['UnwantedColumn1', 'UnwantedColumn2']) print(processed_data_df.head()) This example demonstrates how to fetch and process assay data for the assay with ID 12345, removing 'UnwantedColumn1' and 'UnwantedColumn2' from the resulting DataFrame. The first few rows of the processed DataFrame are printed as an output.</p> Note <ul> <li>This method is part of a class that requires a valid session with the PubChem API. Ensure that the class is properly initialized and that the session is active.</li> <li>The removal of columns is an optional step and can be customized based on the analysis needs. If no columns need to be removed, pass an empty list as 'columns_to_remove'.</li> </ul> Source code in <code>chemgraphbuilder/relationship_properties_extractor.py</code> <pre><code>def fetch_data_for_aid(self, aid, columns_to_remove):\n    \"\"\"\n    Fetches and processes assay data for a specified Assay ID (AID) from the\n    PubChem database, preparing it for analysis or further processing.\n\n    This method queries the PubChem database for assay data associated with\n    a given AID. It constructs the query URL, sends the request using a\n    previously established session, and processes the response. The response\n    is expected to be in CSV format, which this method reads into a pandas\n    DataFrame. Specific columns can be removed from this DataFrame based on\n    the requirements for analysis. This allows for the customization of\n    the fetched data, making it easier to work with specific datasets.\n\n    If the request is successful and the data is fetched without issues,\n    it undergoes initial processing to remove unwanted columns as specified\n    by the 'columns_to_remove' parameter. In case of an error during the\n    data fetching or processing (e.g., issues with parsing the CSV data),\n    appropriate error messages are logged, and an empty DataFrame is\n    returned as a fallback.\n\n    Parameters:\n        aid (int): The assay ID for which data is to be fetched. This ID is\n        used to construct the query URL to the PubChem database.\n        columns_to_remove (list of str): A list of column names that should\n        be removed from the fetched DataFrame. This allows for the exclusion\n        of data that might not be relevant to the subsequent analysis or\n        processing steps.\n\n    Returns:\n        pandas.DataFrame: A DataFrame containing the processed data\n        associated with the given AID. The DataFrame will exclude columns\n        listed in 'columns_to_remove'. If the data fetching fails or if\n        an error occurs during processing, an empty DataFrame is returned.\n\n    Raises:\n        requests.RequestException: If an error occurs during the HTTP request\n        to the PubChem API. This includes scenarios such as timeout issues,\n        non-200 status codes, or network-related errors. The exception is\n        handled internally with logging, but it's important to be aware of\n        its possibility.\n        pd.errors.ParserError: If an error occurs while parsing the CSV\n        response from PubChem into a DataFrame. This could happen due to\n        malformed data or unexpected changes in the response format.\n        Like with RequestException, this error is logged and results in\n        the return of an empty DataFrame.\n\n    Example:\n        &gt;&gt;&gt; extractor = RelationshipPropertiesExtractor()\n        &gt;&gt;&gt; processed_data_df = extractor.fetch_data_for_aid(12345, ['UnwantedColumn1', 'UnwantedColumn2'])\n        &gt;&gt;&gt; print(processed_data_df.head())\n        This example demonstrates how to fetch and process assay data for\n        the assay with ID 12345, removing 'UnwantedColumn1' and\n        'UnwantedColumn2' from the resulting DataFrame. The first few rows\n        of the processed DataFrame are printed as an output.\n\n    Note:\n        - This method is part of a class that requires a valid session with\n        the PubChem API. Ensure that the class is properly initialized and that\n        the session is active.\n        - The removal of columns is an optional step and can be customized\n        based on the analysis needs. If no columns need to be removed, pass an\n        empty list as 'columns_to_remove'.\n    \"\"\"\n    url = (\n        \"https://pubchem.ncbi.nlm.nih.gov/assay/pcget.cgi?\"\n        \"query=download&amp;record_type=datatable&amp;actvty=\"\n        f\"all&amp;response_type=display&amp;aid={aid}\"\n    )\n\n    response = self._send_request(url)\n    if response and response.status_code == 200:\n        try:\n            compound_df = pd.read_csv(StringIO(response.text), sep=',')\n            # Drop specified columns and process column names in-place for memory efficiency\n            columns_to_remove_set = set(columns_to_remove)\n            existing_columns_set = set(compound_df.columns)\n            columns_to_actually_remove = list(columns_to_remove_set &amp; existing_columns_set)\n            compound_df.drop(columns=columns_to_actually_remove,\n                             errors='ignore', inplace=True)\n            compound_df.rename(columns=lambda x: x.replace('PUBCHEM_', '') if x.startswith('PUBCHEM_') else x, inplace=True)\n\n            # compound_df.drop(columns=[col for col in columns_to_remove if col in compound_df.columns], errors='ignore', inplace=True)\n            # compound_df.columns = [col.replace('PUBCHEM_', '') if col.startswith('PUBCHEM_') else col for col in compound_df.columns]\n            compound_df['AID'] = aid\n            return compound_df\n        except pd.errors.ParserError as e:\n            logging.error(f\"CSV parsing failed for AID {aid}: {e}\")\n    else:\n        logging.error(f\"Failed to fetch data for AID {aid}. Status code: {response.status_code if response else 'No Response'}\")\n    return pd.DataFrame()  # Return an empty DataFrame in case of failure\n</code></pre>"},{"location":"documentation/#chemgraphbuilder.relationship_properties_extractor.RelationshipPropertiesExtractor.fetch_similar_cids","title":"<code>fetch_similar_cids(cid)</code>","text":"<p>Fetches similar compound IDs (CIDs) from the PubChem database for a given compound ID (CID) using 2D similarity.</p> <p>This method queries the PubChem database to find compounds that are similar to the given CID based on 2D structural similarity. The similarity threshold is set to 95%, and a maximum of 100 similar CIDs are fetched. The response is parsed from XML format to extract the similar CIDs.</p> <p>Parameters:</p> Name Type Description Default <code>cid</code> <code>int</code> <p>The compound ID for which similar CIDs are to be fetched.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple containing the original CID and a list of similar</p> <p>CIDs. If an error occurs, the list of similar CIDs will be empty.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>Logs an error message with the original CID and the</p> Note <ul> <li>The method utilizes the <code>requests</code> library for HTTP requests and <code>xml.etree.ElementTree</code> for XML parsing.</li> <li>In case of a request failure or parsing error, the method logs the error and returns the original CID with an empty list, allowing the calling function to handle the exception as needed.</li> </ul> Source code in <code>chemgraphbuilder/relationship_properties_extractor.py</code> <pre><code>def fetch_similar_cids(self, cid):\n    \"\"\"\n    Fetches similar compound IDs (CIDs) from the PubChem database for a\n    given compound ID (CID) using 2D similarity.\n\n    This method queries the PubChem database to find compounds that are\n    similar to the given CID based on 2D structural similarity.\n    The similarity threshold is set to 95%, and a maximum of 100 similar\n    CIDs are fetched. The response is parsed from XML format to extract\n    the similar CIDs.\n\n    Parameters:\n        cid (int): The compound ID for which similar CIDs are to be fetched.\n\n    Returns:\n        tuple: A tuple containing the original CID and a list of similar\n        CIDs. If an error occurs, the list of similar CIDs will be empty.\n\n    Raises:\n        Exception: Logs an error message with the original CID and the\n        exception if the request to PubChem fails or if parsing the XML\n        response encounters an error.\n\n    Note:\n        - The method utilizes the `requests` library for HTTP requests and\n        `xml.etree.ElementTree` for XML parsing.\n        - In case of a request failure or parsing error, the method logs\n        the error and returns the original CID with an empty list,\n        allowing the calling function to handle the exception as needed.\n    \"\"\"\n    url = (\"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/\"\n           f\"fastsimilarity_2d/cid/{int(cid)}/cids/XML?Threshold=95&amp;MaxRecords=100\")\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n        xml_data = response.text\n\n        # Parse XML data\n        tree = ET.parse(io.StringIO(xml_data))\n        root = tree.getroot()\n\n        # Extracting CID values\n        similar_cids = [element.text for element in root.findall('{http://pubchem.ncbi.nlm.nih.gov/pug_rest}CID')]\n        return cid, similar_cids\n    except Exception as e:\n        logging.error(f\"Error processing CID {cid}: {e}\")\n        return cid, []\n</code></pre>"},{"location":"documentation/#chemgraphbuilder.relationship_properties_extractor.RelationshipPropertiesExtractor.gene_enzyme_relationship","title":"<code>gene_enzyme_relationship(main_data)</code>","text":"<p>Extracts and saves relationships between genes and enzymes based on the provided dataset.</p> <p>This method selects relevant columns to highlight the relationships between genes and their corresponding enzymes. It removes duplicate entries to ensure that each relationship is represented uniquely and saves the resultant data to a CSV file. This facilitates easy integration of genetic data into knowledge bases or further analysis.</p> <p>Parameters:</p> Name Type Description Default <code>main_data</code> <code>str</code> <p>Path to the CSV file containing gene and enzyme data.</p> required <p>Returns:</p> Type Description <p>pandas.DataFrame: A DataFrame of unique gene-enzyme relationships,</p> <p>including gene ID and enzyme accession numbers.</p> Side Effects <ul> <li>Writes the processed data to 'Data/Gene_Enzyme_Relationship.csv' in a structured CSV format.</li> </ul> Source code in <code>chemgraphbuilder/relationship_properties_extractor.py</code> <pre><code>def gene_enzyme_relationship(self, main_data):\n    \"\"\"\n    Extracts and saves relationships between genes and enzymes based on\n    the provided dataset.\n\n    This method selects relevant columns to highlight the relationships\n    between genes and their corresponding enzymes.\n    It removes duplicate entries to ensure that each relationship is\n    represented uniquely and saves the resultant data to\n    a CSV file. This facilitates easy integration of genetic data into\n    knowledge bases or further analysis.\n\n    Parameters:\n        main_data (str): Path to the CSV file containing gene and enzyme data.\n        Expected columns include 'Target GeneID' and 'Target Accession'.\n\n    Returns:\n        pandas.DataFrame: A DataFrame of unique gene-enzyme relationships,\n        including gene ID and enzyme accession numbers.\n\n    Side Effects:\n        - Writes the processed data to 'Data/Gene_Enzyme_Relationship.csv'\n        in a structured CSV format.\n    \"\"\"\n    df = pd.read_csv(main_data)\n    columns_to_select = ['Target GeneID', 'Target Accession']\n    df = df[columns_to_select]\n    df = df.drop_duplicates(keep='first', ignore_index=True)\n    df.to_csv(f'Data/Relationships/Gene_Enzyme_Relationship.csv', index=False)\n    return df\n</code></pre>"},{"location":"documentation/#chemgraphbuilder.relationship_properties_extractor.RelationshipPropertiesExtractor.process_chunk","title":"<code>process_chunk(chunk)</code>","text":"<p>Processes a chunk of CIDs in parallel to fetch similar CIDs for each CID in the chunk.</p> <p>This method uses a ThreadPoolExecutor to send out concurrent requests for fetching similar CIDs for a list of CIDs. The number of worker threads is set to 5. Each CID's request is handled by <code>fetch_similar_cids</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>chunk</code> <code>list of int</code> <p>A list of compound IDs (CIDs) to process in</p> required <p>Returns:</p> Type Description <p>list of tuples: A list of tuples, each containing a CID and its</p> <p>corresponding list of similar CIDs.</p> Side Effects <ul> <li>Utilizes concurrent threads to speed up the fetching process.</li> <li>May log errors if any occur during the fetching of similar CIDs for individual CIDs.</li> </ul> Source code in <code>chemgraphbuilder/relationship_properties_extractor.py</code> <pre><code>def process_chunk(self, chunk):\n    \"\"\"\n    Processes a chunk of CIDs in parallel to fetch similar CIDs for each CID\n    in the chunk.\n\n    This method uses a ThreadPoolExecutor to send out concurrent requests for\n    fetching similar CIDs for a list of CIDs.\n    The number of worker threads is set to 5. Each CID's request is handled\n    by `fetch_similar_cids` method.\n\n    Parameters:\n        chunk (list of int): A list of compound IDs (CIDs) to process in\n        parallel.\n\n    Returns:\n        list of tuples: A list of tuples, each containing a CID and its\n        corresponding list of similar CIDs.\n\n    Side Effects:\n        - Utilizes concurrent threads to speed up the fetching process.\n        - May log errors if any occur during the fetching of similar CIDs\n        for individual CIDs.\n    \"\"\"\n    with ThreadPoolExecutor(max_workers=5) as executor:\n        futures = [executor.submit(self.fetch_similar_cids, cid) for cid in chunk]\n        results = [future.result() for future in as_completed(futures)]\n    return results\n</code></pre>"},{"location":"documentation/#7-relationship-data-processor","title":"7. Relationship Data Processor","text":"<p>Module for processing relationship data files using Dask and pandas.</p> <p>This module includes the <code>RelationshipDataProcessor</code> class, which is used to load, filter, clean, and process data files related to assay-compound relationships.</p>"},{"location":"documentation/#chemgraphbuilder.relationship_data_processor.RelationshipDataProcessor","title":"<code>RelationshipDataProcessor</code>","text":"<p>A class to process relationship data files, filtering and augmenting the data.</p> <p>Attributes:</p> Name Type Description <code>path</code> <code>str</code> <p>The directory path where the data files are stored.</p> <code>csv_files</code> <code>list</code> <p>List of CSV files matching the pattern 'AID_*.csv'.</p> <code>all_data_connected</code> <code>dict</code> <p>A dictionary containing additional data connected to assays.</p> Source code in <code>chemgraphbuilder/relationship_data_processor.py</code> <pre><code>class RelationshipDataProcessor:\n    \"\"\"\n    A class to process relationship data files, filtering and augmenting the data.\n\n    Attributes:\n        path (str): The directory path where the data files are stored.\n        csv_files (list): List of CSV files matching the pattern 'AID_*.csv'.\n        all_data_connected (dict): A dictionary containing additional data connected to assays.\n    \"\"\"\n\n    def __init__(self, path):\n        \"\"\"\n        Initializes the RelationshipDataProcessor with the specified path.\n\n        Args:\n            path (str): The directory path containing the CSV files.\n        \"\"\"\n        self.path = path\n        self.csv_files = glob.glob(os.path.join(path, \"AID_*.csv\"))\n        self.all_data_connected = self._load_all_data_connected('Data/AllDataConnected.csv')\n\n    def _load_all_data_connected(self, file_path):\n        \"\"\"\n        Loads additional data from a specified file and organizes it into a dictionary.\n\n        Args:\n            file_path (str): The path to the file containing additional data.\n\n        Returns:\n            dict: A dictionary with keys as tuples of (aid, cid, activity_outcome)\n                  and values as dictionaries of additional information.\n        \"\"\"\n        all_data_connected = {}\n        ddf = dd.read_csv(file_path, blocksize=20e6)\n        ddf.columns = [col.replace(' ', '_').lower() for col in ddf.columns]\n        ddf = ddf.dropna(subset=['aid', 'cid'], how='any')\n        partitions = ddf.to_delayed()\n        ddf = ddf.repartition(partition_size=10e5)\n\n        @dask.delayed\n        def process_partition(partition):\n            result = {}\n            partition = partition.dropna(subset=['aid', 'cid'], how='any')\n            for _, row in partition.iterrows():\n                key = (int(row['aid']), int(row['cid']), row['activity_outcome'])\n                result[key] = row.to_dict()\n            return result\n\n        results = dask.compute(*[process_partition(part) for part in partitions])\n        for result in results:\n            all_data_connected.update(result)\n\n        # Optionally save the dictionary to a file\n        with open(\"Data/Relationships/all_data_connected_dict.txt\", \"w\") as file:\n            for key, value in all_data_connected.items():\n                file.write(f\"{key}: {value}\\n\")\n\n        return all_data_connected\n\n    def _add_all_data_connected_info(self, row):\n        \"\"\"\n        Adds additional information from all_data_connected to a row.\n\n        Args:\n            row (pd.Series): A row from a DataFrame.\n\n        Returns:\n            pd.Series: The updated row with additional data if available.\n        \"\"\"\n        key = (int(row['aid']), int(row['cid']), row['activity_outcome'])\n        if key in self.all_data_connected:\n            additional_info = self.all_data_connected[key]\n            for col, val in additional_info.items():\n                row[col] = val\n        else:\n            logging.warning(f\"Key {key} not found in all_data_connected.\")\n        return row\n\n    def _get_filtered_columns(self):\n        \"\"\"\n        Extracts unique column names from the CSV files and additional data.\n\n        Returns:\n            list: A list of unique column names.\n        \"\"\"\n        all_columns = set()\n\n        # Extract additional columns from the all_data_connected dictionary\n        additional_columns = set()\n        for value in self.all_data_connected.values():\n            additional_columns.update(value.keys())\n\n        def read_columns(file):\n            try:\n                # Read only column names from the CSV file\n                df = pd.read_csv(file, nrows=0)\n                return set([col.replace(' ', '_').lower() for col in df.columns])\n            except Exception as e:\n                logging.error(f\"Error reading {file}: {e}\")\n                return set()\n\n        # Use ThreadPoolExecutor for concurrent reading of columns from multiple files\n        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n            results = list(executor.map(read_columns, self.csv_files))\n\n        for columns in results:\n            all_columns.update(columns)\n\n        all_columns.update(additional_columns)\n\n        # Save the combined columns to a file for reference\n        with open(\"Data/Relationships/all_columns.txt\", \"w\") as file:\n            for item in all_columns:\n                file.write(f\"{item}\\n\")\n\n        return list(all_columns)\n\n    def process_files(self):\n        \"\"\"\n        Processes the CSV files by filtering, cleaning, and augmenting data.\n\n        The processed data is saved to output files.\n        \"\"\"\n        self._filter_and_clean_data()\n        logging.info(\"Data filtered, cleaned, and combined successfully.\")\n\n    def _filter_and_clean_data(self):\n        \"\"\"\n        Filters and cleans data from CSV files, then saves to output files.\n        \"\"\"\n        output_file = os.path.join('Data/Relationships/Assay_Compound_Relationship.csv')\n        compound_gene_file = os.path.join('Data/Relationships/Compound_Gene_Relationship.csv')\n\n        if os.path.exists(output_file):\n            os.remove(output_file)\n        if os.path.exists(compound_gene_file):\n            os.remove(compound_gene_file)\n\n        unique_column_names = self._get_filtered_columns()+['activity']\n\n        # Initialize output files with headers\n        pd.DataFrame(columns=unique_column_names).to_csv(output_file, index=False)\n        pd.DataFrame(columns=['cid', 'target_geneid', 'activity', 'aid']).to_csv(compound_gene_file, index=False)\n\n        tasks = []\n        for i, file in enumerate(self.csv_files):\n            if i % 100 == 0:\n                logging.info(f\"Processing file {i+1}/{len(self.csv_files)}: {file}\")\n\n            tasks.append(dask.delayed(self._process_file)(file, unique_column_names, output_file, compound_gene_file))\n\n        dask.compute(*tasks)\n        logging.info(f\"Processed {len(self.csv_files)} files\")\n\n    def _process_file(self, file, unique_column_names, output_file, compound_gene_file):\n        \"\"\"\n        Processes a single CSV file, applying filtering, cleaning, and adding data.\n\n        Args:\n            file (str): The file path to the CSV file.\n            unique_column_names (list): The list of unique column names to use.\n            output_file (str): The path to the output file for combined data.\n            compound_gene_file (str): The path to the output file for compound-gene relationships.\n        \"\"\"\n        ddf = dd.read_csv(file, blocksize=10000, dtype={'ASSAYDATA_COMMENT': 'object'})\n        ddf.columns = [col.replace(' ', '_').lower() for col in ddf.columns]\n        ddf = ddf.dropna(subset=['cid'], how='any')\n\n        # Repartition to balance memory usage and performance\n        ddf = ddf.repartition(partition_size=1000)\n\n        phenotype_cols = [col for col in ddf.columns if col.startswith('phenotype')]\n\n        def process_partition(df):\n            try:\n                if isinstance(df, pd.Series):\n                    df = df.to_frame().T  # Convert to DataFrame if a Series is encountered\n                if df.columns.duplicated().any():\n                    df = df.loc[:, ~df.columns.duplicated()]\n                    logging.info(\"Duplicated columns removed from partition.\")\n                df = df.reindex(columns=unique_column_names, fill_value=pd.NA)\n                df = df.dropna(subset=['aid', 'cid'], how='any')\n\n                if not df.empty:\n                    df['measured_activity'] = df[phenotype_cols].apply(lambda row: row.mode()[0] if not row.mode().empty else None, axis=1)\n\n                    df = df.apply(self._add_all_data_connected_info, axis=1)\n\n                    if any(col in df.columns for col in phenotype_cols) and df['activity_outcome'].notna().all():\n                        df = df.groupby(['activity_outcome', 'assay_name']).apply(self.propagate_phenotype).reset_index(drop=True)\n\n                    if 'target_geneid' not in df.columns:\n                        df['target_geneid'] = pd.NA\n\n                    if 'sid' in df.columns:\n                        df['activity_url'] = df.apply(lambda row: f\"https://pubchem.ncbi.nlm.nih.gov/bioassay/{row['aid']}#sid={row['sid']}\", axis=1)\n                    else:\n                        df['activity_url'] = pd.NA\n\n                    # Drop rows where both aid and cid are 1\n                    df = df[(df['aid'] != 1) | (df['cid'] != 1)]\n\n                    df = self._determine_labels_and_activity(df)\n\n                    logging.info(f\"Processed partition with {len(df)} rows.\")\n                    if not df.empty:\n                        # Write the processed data to the output files\n                        df.to_csv(output_file, mode='a', header=not os.path.exists(output_file), index=False)\n                        df[['cid', 'target_geneid', 'activity', 'aid']].to_csv(compound_gene_file, mode='a', header=not os.path.exists(compound_gene_file), index=False)\n                else:\n                    logging.info(\"No data to process after filtering.\")\n            except Exception as e:\n                logging.error(f\"Error processing partition: {e}\")\n\n        ddf.map_partitions(process_partition).compute()\n\n    @staticmethod\n    def most_frequent(row):\n        \"\"\"\n        Finds the most frequent value in a row, excluding NaN values.\n\n        Args:\n            row (pd.Series): A row from a DataFrame.\n\n        Returns:\n            str: The most frequent value in the row.\n        \"\"\"\n        values = row.dropna()\n        string_values = values[values.apply(lambda x: isinstance(x, str))]\n        return string_values.mode()[0] if not string_values.empty else None\n\n    @staticmethod\n    def propagate_phenotype(group):\n        \"\"\"\n        Propagates the phenotype information within a group.\n\n        Args:\n            group (pd.DataFrame): A DataFrame group.\n\n        Returns:\n            pd.DataFrame: The updated group with propagated phenotype information.\n        \"\"\"\n        phenotype_value = group['phenotype'].dropna().unique()\n        if len(phenotype_value) &gt; 0:\n            group['phenotype'] = phenotype_value[0]\n        return group\n\n    def _determine_labels_and_activity(self, merged_df):\n        \"\"\"\n        Determines the activity labels for the data based on predefined keywords.\n\n        Args:\n            merged_df (pd.DataFrame): The DataFrame containing merged data.\n\n        Returns:\n            pd.DataFrame: The DataFrame with determined activity labels.\n        \"\"\"\n        inhibitor_keywords = [\n            'inhibition', 'reversible inhibition', 'time dependent inhibition',\n            'inhibitory activity', 'time-dependent inhibition', 'time dependent irreversible inhibition',\n            'inhibitory concentration', 'inhibitory effect', 'inhibitory potency',\n            'concentration required to inhibit', 'competitive inhibition', 'cyp inhibition',\n            'irreversible inhibition', 'mechanism based inhibition', 'mixed inhibition',\n            'mixed type inhibition', 'inhibitory constant', 'antagonistic activity', 'selectivity',\n            's1p4 agonists', 'small molecule antagonists', 'displacement', 'mediated midazolam 1-hydroxylation',\n            'time/nadph-dependent inhibition', 'reversal inhibition', 'mechanism-based inhibition',\n            'mechanism based time dependent inhibition', 'reversible competitive inhibition',\n            'predictive competitive inhibition','noncompetitive inhibition', 'in vitro inhibitory',\n            'in vitro inhibition', 'inhibition of', 'direct inhibition','enzyme inhibition', 'dndi',\n            'inhibition assay'\n        ]\n\n        ligand_keywords = [\n            'binding affinity', 'spectral binding', 'interaction with', 'bind',\n            'covalent binding affinity', 'apparent binding affinity'\n        ]\n\n        inhibitor_substrate_keywords = [\n            'inhibitors and substrates'\n        ]\n\n        inhibitor_activator_modulator_keywords = [\n            'apoprotein formation', 'panel assay', 'eurofins-panlabs enzyme assay'\n        ]\n\n        substrate_keywords = [\n            'drug metabolism', 'prodrug', 'metabolic', 'oxidation', 'substrate activity',\n            'michaelis-menten', 'metabolic stability', 'bioactivation', 'drug level',\n            'enzyme-mediated drug depletion', 'enzyme-mediated compound formation',\n            'phenotyping', 'activity of human recombinant cyp', 'activity of recombinant cyp',\n            'activity at cyp', 'enzyme-mediated drug metabolism'\n        ]\n\n        inactivator_keywords = [\n            'inactivator', 'inactivation of', 'mechanism based inactivation of', 'inactivators',\n            'metabolism dependent inactivation'\n        ]\n\n        activator_keywords = [\n            'assay for activators', 'activation of', 'activators of'\n        ]\n\n        inducer_keywords = [\n            'induction of', 'inducer', 'inducers', 'time-dependant induction'\n        ]\n\n        all_keywords = (inhibitor_keywords + ligand_keywords + inhibitor_substrate_keywords +\n                        inhibitor_activator_modulator_keywords + substrate_keywords +\n                        inactivator_keywords + activator_keywords + inducer_keywords)\n\n        keyword_to_label = {\n            **{keyword: 'Inhibitor' for keyword in inhibitor_keywords},\n            **{keyword: 'Inhibitor/Substrate' for keyword in inhibitor_substrate_keywords},\n            **{keyword: 'Inhibitor/Inducer/Modulator' for keyword in inhibitor_activator_modulator_keywords},\n            **{keyword: 'Substrate' for keyword in substrate_keywords},\n            **{keyword: 'Inactivator' for keyword in inactivator_keywords},\n            **{keyword: 'Activator' for keyword in activator_keywords},\n            **{keyword: 'Inducer' for keyword in inducer_keywords},\n            **{keyword: 'Ligand' for keyword in ligand_keywords},\n        }\n\n        def determine_active_label(assay_name):\n            # Determine the appropriate label based on the first keyword found in the assay name\n            assay_name_lower = assay_name.lower()\n            first_keyword = None\n            first_position = len(assay_name_lower)\n\n            for keyword in all_keywords:\n                position = assay_name_lower.find(keyword)\n                if 0 &lt;= position &lt; first_position:\n                    first_keyword = keyword\n                    first_position = position\n\n            if first_keyword:\n                return keyword_to_label[first_keyword]\n            return 'Inhibitor/Inducer/Modulator'\n\n        merged_df['activity'] = None\n\n        # Assign the 'Inactive' label where the activity outcome is inactive\n        inactive_mask = merged_df['activity_outcome'] == 'Inactive'\n        merged_df.loc[inactive_mask, 'activity'] = 'Inactive'\n\n        # Assign labels based on assay name keywords for active outcomes\n        active_mask = merged_df['activity_outcome'] == 'Active'\n        if active_mask.any():\n            merged_df.loc[active_mask, 'activity'] = merged_df.loc[active_mask, 'assay_name'].apply(determine_active_label)\n            merged_df.loc[active_mask &amp; merged_df['activity_name'].isin(['Km', 'Drug metabolism']), 'activity'] = 'Substrate'\n\n            substrate_pattern = r'(activity of.*oxidation)|(activity at cyp.*phenotyping)|(activity at human recombinant cyp.*formation)|(activity at recombinant cyp.*formation)'\n            merged_df.loc[active_mask &amp; merged_df['assay_name'].str.contains(substrate_pattern, case=False, regex=True), 'activity'] = 'Substrate'\n\n            ActIndMod_pattern = r'(effect on cyp)|(effect on human recombinant cyp)|(effect on recombinant cyp)|(effect on human cyp)'\n            merged_df.loc[active_mask &amp; merged_df['assay_name'].str.contains(ActIndMod_pattern, case=False, regex=True), 'activity'] = 'Inhibitor/Inducer/Modulator'\n\n            inducer_pattern = r'(effect on cyp.*induction)|(induction of.*)'\n            merged_df.loc[active_mask &amp; merged_df['assay_name'].str.contains(inducer_pattern, case=False, regex=True), 'activity'] = 'Inducer'\n\n            merged_df.loc[active_mask &amp; merged_df['activity_direction'].str.contains('decreasing', case=False), 'activity'] = 'Inhibitor'\n            merged_df.loc[active_mask &amp; merged_df['activity_direction'].str.contains('increasing', case=False), 'activity'] = 'Activator'\n            merged_df.loc[active_mask &amp; (merged_df['aid'] == 1215398), 'activity'] = 'Inactivator'\n\n        return merged_df\n</code></pre>"},{"location":"documentation/#chemgraphbuilder.relationship_data_processor.RelationshipDataProcessor.__init__","title":"<code>__init__(path)</code>","text":"<p>Initializes the RelationshipDataProcessor with the specified path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The directory path containing the CSV files.</p> required Source code in <code>chemgraphbuilder/relationship_data_processor.py</code> <pre><code>def __init__(self, path):\n    \"\"\"\n    Initializes the RelationshipDataProcessor with the specified path.\n\n    Args:\n        path (str): The directory path containing the CSV files.\n    \"\"\"\n    self.path = path\n    self.csv_files = glob.glob(os.path.join(path, \"AID_*.csv\"))\n    self.all_data_connected = self._load_all_data_connected('Data/AllDataConnected.csv')\n</code></pre>"},{"location":"documentation/#chemgraphbuilder.relationship_data_processor.RelationshipDataProcessor.most_frequent","title":"<code>most_frequent(row)</code>  <code>staticmethod</code>","text":"<p>Finds the most frequent value in a row, excluding NaN values.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Series</code> <p>A row from a DataFrame.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The most frequent value in the row.</p> Source code in <code>chemgraphbuilder/relationship_data_processor.py</code> <pre><code>@staticmethod\ndef most_frequent(row):\n    \"\"\"\n    Finds the most frequent value in a row, excluding NaN values.\n\n    Args:\n        row (pd.Series): A row from a DataFrame.\n\n    Returns:\n        str: The most frequent value in the row.\n    \"\"\"\n    values = row.dropna()\n    string_values = values[values.apply(lambda x: isinstance(x, str))]\n    return string_values.mode()[0] if not string_values.empty else None\n</code></pre>"},{"location":"documentation/#chemgraphbuilder.relationship_data_processor.RelationshipDataProcessor.process_files","title":"<code>process_files()</code>","text":"<p>Processes the CSV files by filtering, cleaning, and augmenting data.</p> <p>The processed data is saved to output files.</p> Source code in <code>chemgraphbuilder/relationship_data_processor.py</code> <pre><code>def process_files(self):\n    \"\"\"\n    Processes the CSV files by filtering, cleaning, and augmenting data.\n\n    The processed data is saved to output files.\n    \"\"\"\n    self._filter_and_clean_data()\n    logging.info(\"Data filtered, cleaned, and combined successfully.\")\n</code></pre>"},{"location":"documentation/#chemgraphbuilder.relationship_data_processor.RelationshipDataProcessor.propagate_phenotype","title":"<code>propagate_phenotype(group)</code>  <code>staticmethod</code>","text":"<p>Propagates the phenotype information within a group.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>DataFrame</code> <p>A DataFrame group.</p> required <p>Returns:</p> Type Description <p>pd.DataFrame: The updated group with propagated phenotype information.</p> Source code in <code>chemgraphbuilder/relationship_data_processor.py</code> <pre><code>@staticmethod\ndef propagate_phenotype(group):\n    \"\"\"\n    Propagates the phenotype information within a group.\n\n    Args:\n        group (pd.DataFrame): A DataFrame group.\n\n    Returns:\n        pd.DataFrame: The updated group with propagated phenotype information.\n    \"\"\"\n    phenotype_value = group['phenotype'].dropna().unique()\n    if len(phenotype_value) &gt; 0:\n        group['phenotype'] = phenotype_value[0]\n    return group\n</code></pre>"},{"location":"documentation/#8-add-graph-relationships","title":"8. Add Graph Relationships","text":"<p>Module to set up a data directory with a predefined structure.</p> <p>This module provides the DataFolderSetup class, which creates a directory structure for a data folder. The structure includes nodes and relationships folders with specified subfolders.</p> <p>Classes:</p> Name Description <code>DataFolderSetup</code> <p>Class to set up a data directory with a predefined structure.</p> <p>Functions:</p> Name Description <code>main</code> <p>Main function to set up the data directory.</p>"},{"location":"documentation/#chemgraphbuilder.setup_data_folder.SetupDataFolder","title":"<code>SetupDataFolder</code>","text":"<p>Class to set up a data directory with a predefined structure.</p> <p>Attributes:</p> Name Type Description <code>data_folder</code> <code>str</code> <p>The name of the data folder.</p> <code>base_path</code> <code>str</code> <p>The base path for the data directory.</p> <code>structure</code> <code>dict</code> <p>The structure of directories to create.</p> Source code in <code>chemgraphbuilder/setup_data_folder.py</code> <pre><code>class SetupDataFolder:\n    \"\"\"\n    Class to set up a data directory with a predefined structure.\n\n    Attributes:\n        data_folder (str): The name of the data folder.\n        base_path (str): The base path for the data directory.\n        structure (dict): The structure of directories to create.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initializes the DataFolderSetup with the data folder name and directory structure.\n        \"\"\"\n        self.data_folder = \"Data\"\n        self.base_path = os.path.join(os.getcwd(), self.data_folder)\n        self.structure = {\n            \"Nodes\": [\"Compound_Properties\"],\n            \"Relationships\": [\n                \"Assay_Compound_Relationship\",\n                \"Compound_Similarities\",\n                \"Cpd_Cpd_CoOccurrence\",\n                \"Cpd_Gene_CoOccurrence\"\n            ]\n        }\n\n    @staticmethod\n    def create_folder(path):\n        \"\"\"\n        Creates a folder if it does not already exist.\n\n        Args:\n            path (str): The path of the folder to create.\n        \"\"\"\n        if not os.path.exists(path):\n            os.makedirs(path)\n            print(f\"Created folder: {path}\")\n        else:\n            print(f\"Folder already exists: {path}\")\n\n    def setup(self):\n        \"\"\"\n        Sets up the data directory structure based on the predefined structure.\n        \"\"\"\n        # Create the base data directory\n        self.create_folder(self.base_path)\n\n        # Create the 'Nodes' directory and its subdirectories\n        nodes_path = os.path.join(self.base_path, \"Nodes\")\n        self.create_folder(nodes_path)\n        for folder in self.structure[\"Nodes\"]:\n            self.create_folder(os.path.join(nodes_path, folder))\n\n        # Create the 'Relationships' directory and its subdirectories\n        relationships_path = os.path.join(self.base_path, \"Relationships\")\n        self.create_folder(relationships_path)\n        for folder in self.structure[\"Relationships\"]:\n            self.create_folder(os.path.join(relationships_path, folder))\n</code></pre>"},{"location":"documentation/#chemgraphbuilder.setup_data_folder.SetupDataFolder.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the DataFolderSetup with the data folder name and directory structure.</p> Source code in <code>chemgraphbuilder/setup_data_folder.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initializes the DataFolderSetup with the data folder name and directory structure.\n    \"\"\"\n    self.data_folder = \"Data\"\n    self.base_path = os.path.join(os.getcwd(), self.data_folder)\n    self.structure = {\n        \"Nodes\": [\"Compound_Properties\"],\n        \"Relationships\": [\n            \"Assay_Compound_Relationship\",\n            \"Compound_Similarities\",\n            \"Cpd_Cpd_CoOccurrence\",\n            \"Cpd_Gene_CoOccurrence\"\n        ]\n    }\n</code></pre>"},{"location":"documentation/#chemgraphbuilder.setup_data_folder.SetupDataFolder.create_folder","title":"<code>create_folder(path)</code>  <code>staticmethod</code>","text":"<p>Creates a folder if it does not already exist.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path of the folder to create.</p> required Source code in <code>chemgraphbuilder/setup_data_folder.py</code> <pre><code>@staticmethod\ndef create_folder(path):\n    \"\"\"\n    Creates a folder if it does not already exist.\n\n    Args:\n        path (str): The path of the folder to create.\n    \"\"\"\n    if not os.path.exists(path):\n        os.makedirs(path)\n        print(f\"Created folder: {path}\")\n    else:\n        print(f\"Folder already exists: {path}\")\n</code></pre>"},{"location":"documentation/#chemgraphbuilder.setup_data_folder.SetupDataFolder.setup","title":"<code>setup()</code>","text":"<p>Sets up the data directory structure based on the predefined structure.</p> Source code in <code>chemgraphbuilder/setup_data_folder.py</code> <pre><code>def setup(self):\n    \"\"\"\n    Sets up the data directory structure based on the predefined structure.\n    \"\"\"\n    # Create the base data directory\n    self.create_folder(self.base_path)\n\n    # Create the 'Nodes' directory and its subdirectories\n    nodes_path = os.path.join(self.base_path, \"Nodes\")\n    self.create_folder(nodes_path)\n    for folder in self.structure[\"Nodes\"]:\n        self.create_folder(os.path.join(nodes_path, folder))\n\n    # Create the 'Relationships' directory and its subdirectories\n    relationships_path = os.path.join(self.base_path, \"Relationships\")\n    self.create_folder(relationships_path)\n    for folder in self.structure[\"Relationships\"]:\n        self.create_folder(os.path.join(relationships_path, folder))\n</code></pre>"},{"location":"documentation/#chemgraphbuilder.setup_data_folder.main","title":"<code>main()</code>","text":"<p>Main function to set up the data directory.</p> Source code in <code>chemgraphbuilder/setup_data_folder.py</code> <pre><code>def main():\n    \"\"\"\n    Main function to set up the data directory.\n    \"\"\"\n    data_folder_setup = SetupDataFolder()\n    data_folder_setup.setup()\n</code></pre>"},{"location":"graph_nodes_loader/","title":"You can check the documentation of the <code>GraphNodesLoader</code> class here.","text":"<p>Module to load data into a Neo4j graph database for different node types.</p> <p>This module provides the GraphDataLoader class, which allows loading data for specific node types into a Neo4j database. Users can provide the connection details and node label to load the data.</p> <p>Classes:</p> Name Description <code>GraphDataLoader</code> <p>Class to load data into a Neo4j graph database for different node types.</p> <p>Functions:</p> Name Description <code>main</code> <p>Main function to parse command-line arguments and load data for the specified node type.</p>"},{"location":"graph_nodes_loader/#chemgraphbuilder.graph_nodes_loader.GraphNodesLoader","title":"<code>GraphNodesLoader</code>","text":"<p>Class to load data into a Neo4j graph database for different node types.</p> <p>Attributes:</p> Name Type Description <code>driver</code> <p>The Neo4j driver instance.</p> <code>node_data_adder</code> <p>An instance of the AddGraphNodes class.</p> <code>label_mapping</code> <p>A dictionary mapping node labels to their unique properties and file paths.</p> Source code in <code>chemgraphbuilder/graph_nodes_loader.py</code> <pre><code>class GraphNodesLoader:\n    \"\"\"\n    Class to load data into a Neo4j graph database for different node types.\n\n    Attributes:\n        driver: The Neo4j driver instance.\n        node_data_adder: An instance of the AddGraphNodes class.\n        label_mapping: A dictionary mapping node labels to their unique properties and file paths.\n    \"\"\"\n\n    def __init__(self, uri, username, password):\n        \"\"\"\n        Initializes the GraphDataLoader with Neo4j connection details.\n\n        Args:\n            uri (str): The URI of the Neo4j database.\n            username (str): The username for the Neo4j database.\n            password (str): The password for the Neo4j database.\n        \"\"\"\n        self.driver = GraphDatabase.driver(uri, auth=(username, password))\n        self.logger = logging.getLogger(__name__)  # Define the logger\n        self.logger.info(\"GraphNodesLoader class initialized.\")\n        self.node_data_adder = AddGraphNodes(self.driver)\n        self.label_mapping = {\n            \"Compound\": {\n                \"unique_property\": \"CompoundID\",\n                \"file_path\": \"Data/Nodes/Compound_Properties_Processed.csv\"\n            },\n            \"BioAssay\": {\n                \"unique_property\": \"AssayID\",\n                \"file_path\": \"Data/Nodes/Assay_Properties_Processed.csv\"\n            },\n            \"Gene\": {\n                \"unique_property\": \"GeneID\",\n                \"file_path\": \"Data/Nodes/Gene_Properties_Processed.csv\"\n            },\n            \"Protein\": {\n                \"unique_property\": \"ProteinRefSeqAccession\",\n                \"file_path\": \"Data/Nodes/Protein_Properties_Processed.csv\"\n            }\n        }\n\n    def create_uniqueness_constraint(self, label, unique_property):\n        \"\"\"\n        Creates a uniqueness constraint for a given node label and property.\n\n        Args:\n            label (str): The label of the node.\n            unique_property (str): The property to enforce uniqueness on.\n        \"\"\"\n        self.node_data_adder.create_uniqueness_constraint(\n            self.driver, label=label, unique_property=unique_property\n        )\n\n    def process_and_add_nodes(self, file_path, label, unique_property):\n        \"\"\"\n        Processes and adds nodes from a CSV file to the Neo4j database.\n\n        Args:\n            file_path (str): The path to the CSV file containing node data.\n            label (str): The label of the node.\n            unique_property (str): The unique property of the node.\n        \"\"\"\n        self.node_data_adder.process_and_add_nodes(\n            file_path, label=label, unique_property=unique_property\n        )\n\n    def load_data_for_node_type(self, label):\n        \"\"\"\n        Loads data for a specific node type into the Neo4j database.\n\n        Args:\n            label (str): The label of the node.\n        \"\"\"\n        if label not in self.label_mapping:\n            self.logger.error(\"No mapping found for label: %s\", label)\n            return\n\n        unique_property = self.label_mapping[label][\"unique_property\"]\n        file_path = self.label_mapping[label][\"file_path\"]\n\n        self.create_uniqueness_constraint(label, unique_property)\n        self.process_and_add_nodes(file_path, label, unique_property)\n</code></pre>"},{"location":"graph_nodes_loader/#chemgraphbuilder.graph_nodes_loader.GraphNodesLoader.__init__","title":"<code>__init__(uri, username, password)</code>","text":"<p>Initializes the GraphDataLoader with Neo4j connection details.</p> <p>Parameters:</p> Name Type Description Default <code>uri</code> <code>str</code> <p>The URI of the Neo4j database.</p> required <code>username</code> <code>str</code> <p>The username for the Neo4j database.</p> required <code>password</code> <code>str</code> <p>The password for the Neo4j database.</p> required Source code in <code>chemgraphbuilder/graph_nodes_loader.py</code> <pre><code>def __init__(self, uri, username, password):\n    \"\"\"\n    Initializes the GraphDataLoader with Neo4j connection details.\n\n    Args:\n        uri (str): The URI of the Neo4j database.\n        username (str): The username for the Neo4j database.\n        password (str): The password for the Neo4j database.\n    \"\"\"\n    self.driver = GraphDatabase.driver(uri, auth=(username, password))\n    self.logger = logging.getLogger(__name__)  # Define the logger\n    self.logger.info(\"GraphNodesLoader class initialized.\")\n    self.node_data_adder = AddGraphNodes(self.driver)\n    self.label_mapping = {\n        \"Compound\": {\n            \"unique_property\": \"CompoundID\",\n            \"file_path\": \"Data/Nodes/Compound_Properties_Processed.csv\"\n        },\n        \"BioAssay\": {\n            \"unique_property\": \"AssayID\",\n            \"file_path\": \"Data/Nodes/Assay_Properties_Processed.csv\"\n        },\n        \"Gene\": {\n            \"unique_property\": \"GeneID\",\n            \"file_path\": \"Data/Nodes/Gene_Properties_Processed.csv\"\n        },\n        \"Protein\": {\n            \"unique_property\": \"ProteinRefSeqAccession\",\n            \"file_path\": \"Data/Nodes/Protein_Properties_Processed.csv\"\n        }\n    }\n</code></pre>"},{"location":"graph_nodes_loader/#chemgraphbuilder.graph_nodes_loader.GraphNodesLoader.create_uniqueness_constraint","title":"<code>create_uniqueness_constraint(label, unique_property)</code>","text":"<p>Creates a uniqueness constraint for a given node label and property.</p> <p>Parameters:</p> Name Type Description Default <code>label</code> <code>str</code> <p>The label of the node.</p> required <code>unique_property</code> <code>str</code> <p>The property to enforce uniqueness on.</p> required Source code in <code>chemgraphbuilder/graph_nodes_loader.py</code> <pre><code>def create_uniqueness_constraint(self, label, unique_property):\n    \"\"\"\n    Creates a uniqueness constraint for a given node label and property.\n\n    Args:\n        label (str): The label of the node.\n        unique_property (str): The property to enforce uniqueness on.\n    \"\"\"\n    self.node_data_adder.create_uniqueness_constraint(\n        self.driver, label=label, unique_property=unique_property\n    )\n</code></pre>"},{"location":"graph_nodes_loader/#chemgraphbuilder.graph_nodes_loader.GraphNodesLoader.load_data_for_node_type","title":"<code>load_data_for_node_type(label)</code>","text":"<p>Loads data for a specific node type into the Neo4j database.</p> <p>Parameters:</p> Name Type Description Default <code>label</code> <code>str</code> <p>The label of the node.</p> required Source code in <code>chemgraphbuilder/graph_nodes_loader.py</code> <pre><code>def load_data_for_node_type(self, label):\n    \"\"\"\n    Loads data for a specific node type into the Neo4j database.\n\n    Args:\n        label (str): The label of the node.\n    \"\"\"\n    if label not in self.label_mapping:\n        self.logger.error(\"No mapping found for label: %s\", label)\n        return\n\n    unique_property = self.label_mapping[label][\"unique_property\"]\n    file_path = self.label_mapping[label][\"file_path\"]\n\n    self.create_uniqueness_constraint(label, unique_property)\n    self.process_and_add_nodes(file_path, label, unique_property)\n</code></pre>"},{"location":"graph_nodes_loader/#chemgraphbuilder.graph_nodes_loader.GraphNodesLoader.process_and_add_nodes","title":"<code>process_and_add_nodes(file_path, label, unique_property)</code>","text":"<p>Processes and adds nodes from a CSV file to the Neo4j database.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the CSV file containing node data.</p> required <code>label</code> <code>str</code> <p>The label of the node.</p> required <code>unique_property</code> <code>str</code> <p>The unique property of the node.</p> required Source code in <code>chemgraphbuilder/graph_nodes_loader.py</code> <pre><code>def process_and_add_nodes(self, file_path, label, unique_property):\n    \"\"\"\n    Processes and adds nodes from a CSV file to the Neo4j database.\n\n    Args:\n        file_path (str): The path to the CSV file containing node data.\n        label (str): The label of the node.\n        unique_property (str): The unique property of the node.\n    \"\"\"\n    self.node_data_adder.process_and_add_nodes(\n        file_path, label=label, unique_property=unique_property\n    )\n</code></pre>"},{"location":"graph_nodes_loader/#chemgraphbuilder.graph_nodes_loader.main","title":"<code>main()</code>","text":"<p>Main function to parse command-line arguments and load data for the specified node type.</p> Source code in <code>chemgraphbuilder/graph_nodes_loader.py</code> <pre><code>def main():\n    \"\"\"\n    Main function to parse command-line arguments and load data for the specified node type.\n    \"\"\"\n    parser = argparse.ArgumentParser(description=\"Load data into\"\n                                                 \"Neo4j graph database.\")\n    parser.add_argument('--uri', required=True,\n                        help='URI for the Neo4j database')\n    parser.add_argument('--username', required=True,\n                        help='Username for the Neo4j database')\n    parser.add_argument('--password', required=True,\n                        help='Password for the Neo4j database')\n    parser.add_argument('--label', required=True,\n                        help='Label of the node')\n\n    args = parser.parse_args()\n\n    # Create an instance of GraphDataLoader and load data for the specified node type\n    graph_nodes_loader = GraphNodesLoader(args.uri, args.username, args.password)\n    graph_nodes_loader.load_data_for_node_type(args.label)\n</code></pre>"},{"location":"graph_relationships_loader/","title":"You can check the documentation of the <code>GraphRelationshipsLoader</code> class here.","text":"<p>GraphRelationshipsLoader class for loading graph relationships into a Neo4j database.</p>"},{"location":"graph_relationships_loader/#chemgraphbuilder.graph_relationships_loader.GraphRelationshipsLoader","title":"<code>GraphRelationshipsLoader</code>","text":"<p>Class for loading graph relationships into a Neo4j database using the AddGraphRelationships class.</p> <p>Attributes:</p> Name Type Description <code>uri</code> <code>str</code> <p>The URI for the Neo4j database.</p> <code>username</code> <code>str</code> <p>The username for the Neo4j database.</p> <code>password</code> <code>str</code> <p>The password for the Neo4j database.</p> <code>relationship_settings</code> <code>dict</code> <p>Predefined settings for different relationship types.</p> Source code in <code>chemgraphbuilder/graph_relationships_loader.py</code> <pre><code>class GraphRelationshipsLoader:\n    \"\"\"\n    Class for loading graph relationships into a Neo4j database using the AddGraphRelationships class.\n\n    Attributes:\n        uri (str): The URI for the Neo4j database.\n        username (str): The username for the Neo4j database.\n        password (str): The password for the Neo4j database.\n        relationship_settings (dict): Predefined settings for different relationship types.\n    \"\"\"\n\n    def __init__(self, uri, username, password):\n        \"\"\"\n        Initializes the GraphRelationshipsLoader with Neo4j connection details.\n\n        Args:\n            uri (str): The URI for the Neo4j database.\n            username (str): The username for the Neo4j database.\n            password (str): The password for the Neo4j database.\n        \"\"\"\n        self.driver = GraphDatabase.driver(uri, auth=(username, password))\n        self.logger = logging.getLogger(__name__)\n        self.logger.setLevel(logging.INFO)\n        handler = logging.StreamHandler()\n        handler.setLevel(logging.INFO)\n        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n        handler.setFormatter(formatter)\n        self.logger.addHandler(handler)\n        self.add_graph_relationships = AddGraphRelationships(self.driver)\n        self.logger.info(\"GraphRelationshipsLoader class initialized.\")\n\n        # Predefined settings for different relationship types\n        self.relationship_settings = {\n            \"Compound_Gene\": {\n                \"file_path\": \"Data/Relationships/Compound_Gene_Relationship.csv\",\n                \"source_label\": \"Compound\",\n                \"destination_label\": \"Gene\",\n                \"rel_type_column\": \"Activity\"\n            },\n            \"Assay_Compound\": {\n                \"file_path\": \"Data/Relationships/Assay_Compound_Relationship.csv\",\n                \"source_label\": \"BioAssay\",\n                \"destination_label\": \"Compound\",\n                \"rel_type_column\": None,\n                \"relationship_type\": \"EVALUATES\"\n            },\n            \"Assay_Gene\": {\n                \"file_path\": \"Data/Relationships/Assay_Gene_Relationship.csv\",\n                \"source_label\": \"BioAssay\",\n                \"destination_label\": \"Gene\",\n                \"rel_type_column\": None,\n                \"relationship_type\": \"STUDIES\"\n            },\n            \"Compound_Transformation\": {\n                \"file_path\": \"Data/Relationships/Compound_Transformation.csv\",\n                \"source_label\": \"Compound\",\n                \"destination_label\": \"Compound\",\n                \"rel_type_column\": None,\n                \"relationship_type\": \"IS_METABOLIZED_TO\"\n            },\n            \"Gene_Enzyme\": {\n                \"file_path\": \"Data/Relationships/Gene_Enzyme_Relationship.csv\",\n                \"source_label\": \"Gene\",\n                \"destination_label\": \"Protein\",\n                \"rel_type_column\": None,\n                \"relationship_type\": \"ENCODES\"\n            },\n            \"Compound_Similarities\": {\n                \"file_path\": \"Data/Relationships/Compound_Similarities\",\n                \"source_label\": \"Compound\",\n                \"destination_label\": \"Compound\",\n                \"rel_type_column\": None,\n                \"relationship_type\": \"IS_SIMILAR_TO\",\n                \"is_directory\": True\n            },\n            \"Cpd_Cpd_CoOccurence\": {\n                \"file_path\": \"Data/Relationships/Cpd_Cpd_CoOccurence\",\n                \"source_label\": \"Compound\",\n                \"destination_label\": \"Compound\",\n                \"rel_type_column\": None,\n                \"relationship_type\": \"CO_OCCURS_IN_LITERATURE\",\n                \"is_directory\": True\n            },\n            \"Cpd_Gene_CoOccurence\": {\n                \"file_path\": \"Data/Relationships/Cpd_Gene_CoOccurence\",\n                \"source_label\": \"Compound\",\n                \"destination_label\": \"Gene\",\n                \"rel_type_column\": None,\n                \"relationship_type\": \"CO_OCCURS_IN_LITERATURE\",\n                \"is_directory\": True\n            }\n        }\n\n    def close(self):\n        \"\"\"Closes the Neo4j database driver connection.\"\"\"\n        self.driver.close()\n\n    def add_relationships(self, relationship_type):\n        \"\"\"\n        Adds relationships to the Neo4j database based on the specified relationship type.\n\n        Args:\n            relationship_type (str): The type of the relationship.\n        \"\"\"\n        settings = self.relationship_settings.get(relationship_type)\n        if not settings:\n            self.logger.error(f\"Invalid relationship type: {relationship_type}\")\n            return\n\n        file_path = settings[\"file_path\"]\n        source_label = settings[\"source_label\"]\n        destination_label = settings[\"destination_label\"]\n        rel_type_column = settings.get(\"rel_type_column\")\n        is_directory = settings.get(\"is_directory\", False)\n\n        if is_directory:\n            self.add_graph_relationships.process_and_add_relationships_from_directory(file_path,\n                                                                                      settings[\"relationship_type\"],\n                                                                                      source_label,\n                                                                                      destination_label,\n                                                                                      rel_type_column)\n        else:\n            self.add_graph_relationships.process_and_add_relationships(file_path,\n                                                                       settings.get(\"relationship_type\"),\n                                                                       source_label,\n                                                                       destination_label,\n                                                                       rel_type_column)\n</code></pre>"},{"location":"graph_relationships_loader/#chemgraphbuilder.graph_relationships_loader.GraphRelationshipsLoader.__init__","title":"<code>__init__(uri, username, password)</code>","text":"<p>Initializes the GraphRelationshipsLoader with Neo4j connection details.</p> <p>Parameters:</p> Name Type Description Default <code>uri</code> <code>str</code> <p>The URI for the Neo4j database.</p> required <code>username</code> <code>str</code> <p>The username for the Neo4j database.</p> required <code>password</code> <code>str</code> <p>The password for the Neo4j database.</p> required Source code in <code>chemgraphbuilder/graph_relationships_loader.py</code> <pre><code>def __init__(self, uri, username, password):\n    \"\"\"\n    Initializes the GraphRelationshipsLoader with Neo4j connection details.\n\n    Args:\n        uri (str): The URI for the Neo4j database.\n        username (str): The username for the Neo4j database.\n        password (str): The password for the Neo4j database.\n    \"\"\"\n    self.driver = GraphDatabase.driver(uri, auth=(username, password))\n    self.logger = logging.getLogger(__name__)\n    self.logger.setLevel(logging.INFO)\n    handler = logging.StreamHandler()\n    handler.setLevel(logging.INFO)\n    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n    handler.setFormatter(formatter)\n    self.logger.addHandler(handler)\n    self.add_graph_relationships = AddGraphRelationships(self.driver)\n    self.logger.info(\"GraphRelationshipsLoader class initialized.\")\n\n    # Predefined settings for different relationship types\n    self.relationship_settings = {\n        \"Compound_Gene\": {\n            \"file_path\": \"Data/Relationships/Compound_Gene_Relationship.csv\",\n            \"source_label\": \"Compound\",\n            \"destination_label\": \"Gene\",\n            \"rel_type_column\": \"Activity\"\n        },\n        \"Assay_Compound\": {\n            \"file_path\": \"Data/Relationships/Assay_Compound_Relationship.csv\",\n            \"source_label\": \"BioAssay\",\n            \"destination_label\": \"Compound\",\n            \"rel_type_column\": None,\n            \"relationship_type\": \"EVALUATES\"\n        },\n        \"Assay_Gene\": {\n            \"file_path\": \"Data/Relationships/Assay_Gene_Relationship.csv\",\n            \"source_label\": \"BioAssay\",\n            \"destination_label\": \"Gene\",\n            \"rel_type_column\": None,\n            \"relationship_type\": \"STUDIES\"\n        },\n        \"Compound_Transformation\": {\n            \"file_path\": \"Data/Relationships/Compound_Transformation.csv\",\n            \"source_label\": \"Compound\",\n            \"destination_label\": \"Compound\",\n            \"rel_type_column\": None,\n            \"relationship_type\": \"IS_METABOLIZED_TO\"\n        },\n        \"Gene_Enzyme\": {\n            \"file_path\": \"Data/Relationships/Gene_Enzyme_Relationship.csv\",\n            \"source_label\": \"Gene\",\n            \"destination_label\": \"Protein\",\n            \"rel_type_column\": None,\n            \"relationship_type\": \"ENCODES\"\n        },\n        \"Compound_Similarities\": {\n            \"file_path\": \"Data/Relationships/Compound_Similarities\",\n            \"source_label\": \"Compound\",\n            \"destination_label\": \"Compound\",\n            \"rel_type_column\": None,\n            \"relationship_type\": \"IS_SIMILAR_TO\",\n            \"is_directory\": True\n        },\n        \"Cpd_Cpd_CoOccurence\": {\n            \"file_path\": \"Data/Relationships/Cpd_Cpd_CoOccurence\",\n            \"source_label\": \"Compound\",\n            \"destination_label\": \"Compound\",\n            \"rel_type_column\": None,\n            \"relationship_type\": \"CO_OCCURS_IN_LITERATURE\",\n            \"is_directory\": True\n        },\n        \"Cpd_Gene_CoOccurence\": {\n            \"file_path\": \"Data/Relationships/Cpd_Gene_CoOccurence\",\n            \"source_label\": \"Compound\",\n            \"destination_label\": \"Gene\",\n            \"rel_type_column\": None,\n            \"relationship_type\": \"CO_OCCURS_IN_LITERATURE\",\n            \"is_directory\": True\n        }\n    }\n</code></pre>"},{"location":"graph_relationships_loader/#chemgraphbuilder.graph_relationships_loader.GraphRelationshipsLoader.add_relationships","title":"<code>add_relationships(relationship_type)</code>","text":"<p>Adds relationships to the Neo4j database based on the specified relationship type.</p> <p>Parameters:</p> Name Type Description Default <code>relationship_type</code> <code>str</code> <p>The type of the relationship.</p> required Source code in <code>chemgraphbuilder/graph_relationships_loader.py</code> <pre><code>def add_relationships(self, relationship_type):\n    \"\"\"\n    Adds relationships to the Neo4j database based on the specified relationship type.\n\n    Args:\n        relationship_type (str): The type of the relationship.\n    \"\"\"\n    settings = self.relationship_settings.get(relationship_type)\n    if not settings:\n        self.logger.error(f\"Invalid relationship type: {relationship_type}\")\n        return\n\n    file_path = settings[\"file_path\"]\n    source_label = settings[\"source_label\"]\n    destination_label = settings[\"destination_label\"]\n    rel_type_column = settings.get(\"rel_type_column\")\n    is_directory = settings.get(\"is_directory\", False)\n\n    if is_directory:\n        self.add_graph_relationships.process_and_add_relationships_from_directory(file_path,\n                                                                                  settings[\"relationship_type\"],\n                                                                                  source_label,\n                                                                                  destination_label,\n                                                                                  rel_type_column)\n    else:\n        self.add_graph_relationships.process_and_add_relationships(file_path,\n                                                                   settings.get(\"relationship_type\"),\n                                                                   source_label,\n                                                                   destination_label,\n                                                                   rel_type_column)\n</code></pre>"},{"location":"graph_relationships_loader/#chemgraphbuilder.graph_relationships_loader.GraphRelationshipsLoader.close","title":"<code>close()</code>","text":"<p>Closes the Neo4j database driver connection.</p> Source code in <code>chemgraphbuilder/graph_relationships_loader.py</code> <pre><code>def close(self):\n    \"\"\"Closes the Neo4j database driver connection.\"\"\"\n    self.driver.close()\n</code></pre>"},{"location":"graph_relationships_loader/#chemgraphbuilder.graph_relationships_loader.main","title":"<code>main()</code>","text":"<p>Main function to handle command-line arguments and run the GraphRelationshipsLoader.</p> Source code in <code>chemgraphbuilder/graph_relationships_loader.py</code> <pre><code>def main():\n    \"\"\"Main function to handle command-line arguments and run the GraphRelationshipsLoader.\"\"\"\n    parser = argparse.ArgumentParser(description=\"Load graph relationships into\"\n                                                 \"a Neo4j database.\")\n    parser.add_argument(\"--uri\", type=str, required=True,\n                        help=\"URI for the Neo4j database.\")\n    parser.add_argument(\"--username\", type=str, required=True,\n                        help=\"Username for the Neo4j database.\")\n    parser.add_argument(\"--password\", type=str, required=True,\n                        help=\"Password for the Neo4j database.\")\n    parser.add_argument(\"--relationship_type\", type=str, required=True,\n                        help=\"Type of the relationship to add.\")\n    args = parser.parse_args()\n\n    loader = GraphRelationshipsLoader(args.uri, args.username, args.password)\n    loader.add_relationships(args.relationship_type)\n    loader.close()\n</code></pre>"},{"location":"node_collector_processor/","title":"Class Documentation","text":"<p>Here you can find detailed documentation for the <code>NodeCollectorProcessor</code> class in both Python and the command line:</p> <p>NodesCollectorProcessor Module</p> <p>This module provides the NodesCollectorProcessor class for collecting and processing data for different types of nodes using the NodePropertiesExtractor and NodeDataProcessor classes. The collected data is intended for loading into a Neo4j graph database. The module supports command-line interface (CLI) usage for ease of use.</p> <p>Classes:</p> Name Description <code>NodesCollectorProcessor</code> <p>A class to collect and process data for different types of nodes.</p> <p>Functions:</p> Name Description <code>main</code> <p>Main function to parse command-line arguments and collect data for the specified node type and enzyme list.</p>"},{"location":"node_collector_processor/#chemgraphbuilder.node_collector_processor.NodesCollectorProcessor","title":"<code>NodesCollectorProcessor</code>","text":"<p>A class to collect and process data for different types of nodes using NodePropertiesExtractor and NodeDataProcessor.</p> Source code in <code>chemgraphbuilder/node_collector_processor.py</code> <pre><code>class NodesCollectorProcessor:\n    \"\"\"\n    A class to collect and process data for different types of nodes using NodePropertiesExtractor and NodeDataProcessor.\n    \"\"\"\n\n    def __init__(self, node_type, enzyme_list, start_chunk=None):\n        \"\"\"\n        Initializes the NodesCollectorProcessor with the node type to collect data for, and the list of enzymes.\n\n        Args:\n            node_type (str): The type of node to collect data for (e.g., 'Compound', 'BioAssay', 'Gene', 'Protein').\n            enzyme_list (list of str): List of enzyme names for which assay data will be fetched from PubChem.\n            start_chunk (int, optional): The starting chunk index for processing. Default is None.\n        \"\"\"\n        self.node_type = node_type\n        self.extractor = NodePropertiesExtractor(enzyme_list=enzyme_list)\n        self.processor = NodeDataProcessor(data_dir=\"Data\")\n        self.start_chunk = start_chunk\n\n    def collect_and_process_data(self):\n        \"\"\"\n        Collects and processes data based on the node type and saves it to the appropriate file.\n        \"\"\"\n        data_file = 'Data/AllDataConnected.csv'\n\n        # Check if the data file exists before running the extractor\n        if not os.path.exists(data_file):\n            logging.info(f\"{data_file} does not exist. Running data extraction...\")\n            df = self.extractor.run()\n        else:\n            logging.info(f\"{data_file} already exists. Skipping main data extraction.\")\n\n        if self.node_type == 'Compound':\n            self.extractor.extract_compound_properties(main_data='Data/AllDataConnected.csv', start_chunk=self.start_chunk)\n            self.processor.preprocess_compounds()\n        elif self.node_type == 'BioAssay':\n            self.extractor.extract_assay_properties(main_data='Data/AllDataConnected.csv')\n            self.processor.preprocess_assays()\n        elif self.node_type == 'Gene':\n            self.extractor.extract_gene_properties(main_data='Data/AllDataConnected.csv')\n            self.processor.preprocess_genes()\n        elif self.node_type == 'Protein':\n            self.extractor.extract_protein_properties(main_data='Data/AllDataConnected.csv')\n            self.processor.preprocess_proteins()\n        else:\n            logging.error(f\"Unsupported node type: {self.node_type}\")\n</code></pre>"},{"location":"node_collector_processor/#chemgraphbuilder.node_collector_processor.NodesCollectorProcessor.__init__","title":"<code>__init__(node_type, enzyme_list, start_chunk=None)</code>","text":"<p>Initializes the NodesCollectorProcessor with the node type to collect data for, and the list of enzymes.</p> <p>Parameters:</p> Name Type Description Default <code>node_type</code> <code>str</code> <p>The type of node to collect data for (e.g., 'Compound', 'BioAssay', 'Gene', 'Protein').</p> required <code>enzyme_list</code> <code>list of str</code> <p>List of enzyme names for which assay data will be fetched from PubChem.</p> required <code>start_chunk</code> <code>int</code> <p>The starting chunk index for processing. Default is None.</p> <code>None</code> Source code in <code>chemgraphbuilder/node_collector_processor.py</code> <pre><code>def __init__(self, node_type, enzyme_list, start_chunk=None):\n    \"\"\"\n    Initializes the NodesCollectorProcessor with the node type to collect data for, and the list of enzymes.\n\n    Args:\n        node_type (str): The type of node to collect data for (e.g., 'Compound', 'BioAssay', 'Gene', 'Protein').\n        enzyme_list (list of str): List of enzyme names for which assay data will be fetched from PubChem.\n        start_chunk (int, optional): The starting chunk index for processing. Default is None.\n    \"\"\"\n    self.node_type = node_type\n    self.extractor = NodePropertiesExtractor(enzyme_list=enzyme_list)\n    self.processor = NodeDataProcessor(data_dir=\"Data\")\n    self.start_chunk = start_chunk\n</code></pre>"},{"location":"node_collector_processor/#chemgraphbuilder.node_collector_processor.NodesCollectorProcessor.collect_and_process_data","title":"<code>collect_and_process_data()</code>","text":"<p>Collects and processes data based on the node type and saves it to the appropriate file.</p> Source code in <code>chemgraphbuilder/node_collector_processor.py</code> <pre><code>def collect_and_process_data(self):\n    \"\"\"\n    Collects and processes data based on the node type and saves it to the appropriate file.\n    \"\"\"\n    data_file = 'Data/AllDataConnected.csv'\n\n    # Check if the data file exists before running the extractor\n    if not os.path.exists(data_file):\n        logging.info(f\"{data_file} does not exist. Running data extraction...\")\n        df = self.extractor.run()\n    else:\n        logging.info(f\"{data_file} already exists. Skipping main data extraction.\")\n\n    if self.node_type == 'Compound':\n        self.extractor.extract_compound_properties(main_data='Data/AllDataConnected.csv', start_chunk=self.start_chunk)\n        self.processor.preprocess_compounds()\n    elif self.node_type == 'BioAssay':\n        self.extractor.extract_assay_properties(main_data='Data/AllDataConnected.csv')\n        self.processor.preprocess_assays()\n    elif self.node_type == 'Gene':\n        self.extractor.extract_gene_properties(main_data='Data/AllDataConnected.csv')\n        self.processor.preprocess_genes()\n    elif self.node_type == 'Protein':\n        self.extractor.extract_protein_properties(main_data='Data/AllDataConnected.csv')\n        self.processor.preprocess_proteins()\n    else:\n        logging.error(f\"Unsupported node type: {self.node_type}\")\n</code></pre>"},{"location":"node_collector_processor/#chemgraphbuilder.node_collector_processor.main","title":"<code>main()</code>","text":"<p>Main function to parse command-line arguments and collect data for the specified node type and enzyme list.</p> Source code in <code>chemgraphbuilder/node_collector_processor.py</code> <pre><code>def main():\n    \"\"\"\n    Main function to parse command-line arguments and collect data for the specified node type and enzyme list.\n    \"\"\"\n    parser = argparse.ArgumentParser(description=\"Collect data for different types of nodes.\")\n    parser.add_argument('--node_type', type=str, required=True, choices=['Compound', 'BioAssay', 'Gene', 'Protein'], help='The type of node to collect data for')\n    parser.add_argument('--enzyme_list', type=str, required=True, help='Comma-separated list of enzyme names to fetch data for')\n    parser.add_argument('--start_chunk', type=int, default=None, help='The starting chunk index for processing Compound Data')\n\n    args = parser.parse_args()\n    enzyme_list = args.enzyme_list.split(',')\n\n    collector = NodesCollectorProcessor(node_type=args.node_type, enzyme_list=enzyme_list, start_chunk=args.start_chunk)\n    collector.collect_and_process_data()\n</code></pre>"},{"location":"relationship_collector_processor/","title":"Relationship Collector and Processor","text":"<p>This module collects and processes relationship data for different types of relationships using the <code>RelationshipPropertiesExtractor</code> and <code>RelationshipDataProcessor</code> classes.</p>"},{"location":"relationship_collector_processor/#you-can-check-the-documentation-of-the-relationshipscollectorprocessor-class-here","title":"You can check the documentation of the <code>RelationshipsCollectorProcessor</code> class here.","text":"<p>Module to collect and process relationship data for different types of relationships using RelationshipPropertiesExtractor and RelationshipDataProcessor.</p> <p>Classes:</p> Name Description <code>RelationshipsCollectorProcessor</code> <p>Class to collect and process relationship data for different types of relationships.</p> <p>Functions:</p> Name Description <code>main</code> <p>Main function to parse command-line arguments and collect relationship data for the specified type.</p>"},{"location":"relationship_collector_processor/#chemgraphbuilder.relationship_collector_processor.RelationshipsCollectorProcessor","title":"<code>RelationshipsCollectorProcessor</code>","text":"<p>A class to collect and process relationship data for different types of relationships using RelationshipPropertiesExtractor and RelationshipDataProcessor.</p> <p>Attributes:</p> Name Type Description <code>relationship_type</code> <code>str</code> <p>The type of relationship to collect data for.</p> <code>data_file</code> <code>str</code> <p>The path to the data file containing relationship data.</p> <code>extractor</code> <code>RelationshipPropertiesExtractor</code> <p>An instance of RelationshipPropertiesExtractor.</p> <code>processor</code> <code>RelationshipDataProcessor</code> <p>An instance of RelationshipDataProcessor.</p> Source code in <code>chemgraphbuilder/relationship_collector_processor.py</code> <pre><code>class RelationshipsCollectorProcessor:\n    \"\"\"\n    A class to collect and process relationship data for different types of relationships using RelationshipPropertiesExtractor and RelationshipDataProcessor.\n\n    Attributes:\n        relationship_type (str): The type of relationship to collect data for.\n        data_file (str): The path to the data file containing relationship data.\n        extractor (RelationshipPropertiesExtractor): An instance of RelationshipPropertiesExtractor.\n        processor (RelationshipDataProcessor): An instance of RelationshipDataProcessor.\n    \"\"\"\n\n    def __init__(self, relationship_type, start_chunk=0):\n        \"\"\"\n        Initializes the RelationshipsCollectorProcessor with the relationship type, data file, and start chunk index.\n\n        Args:\n            relationship_type (str): The type of relationship to collect data for (e.g., 'Assay_Compound', 'Assay_Enzyme', 'Gene_Enzyme', 'Compound_Enzyme', 'Compound_Similarity', 'Compound_Cooccurrence', 'Compound_Transformation').\n            start_chunk (int): The starting chunk index for processing data.\n        \"\"\"\n        self.relationship_type = relationship_type\n        self.data_file = \"Data/AllDataConnected.csv\"\n        self.start_chunk = start_chunk\n        self.extractor = RelationshipPropertiesExtractor()\n        self.processor = RelationshipDataProcessor(path=\"Data/Relationships/Assay_Compound_Relationship\")\n\n\n    def collect_relationship_data(self):\n        \"\"\"\n        Collects and processes relationship data based on the relationship type and saves it to the appropriate file.\n        \"\"\"\n        if self.relationship_type == 'Assay_Compound':\n            self.extractor.assay_compound_relationship(self.data_file, start_chunk=self.start_chunk)\n            self.processor.process_files()\n        elif self.relationship_type == 'Assay_Enzyme':\n            self.extractor.assay_enzyme_relationship(self.data_file)\n        elif self.relationship_type == 'Gene_Enzyme':\n            self.extractor.gene_enzyme_relationship(self.data_file)\n        elif self.relationship_type == 'Compound_Gene':\n            self.extractor.compound_gene_relationship(self.data_file)\n        elif self.relationship_type == 'Compound_Similarity':\n            self.extractor.compound_similarity_relationship(self.data_file, start_chunk=self.start_chunk)\n        elif self.relationship_type == 'Compound_Cooccurrence':\n            self.extractor.compound_cooccurrence(self.data_file, start_chunk=self.start_chunk)\n        elif self.relationship_type == 'Compound_Transformation':\n            self.extractor.compound_transformation(self.data_file)\n        else:\n            logging.error(f\"Unsupported relationship type: {self.relationship_type}\")\n</code></pre>"},{"location":"relationship_collector_processor/#chemgraphbuilder.relationship_collector_processor.RelationshipsCollectorProcessor.__init__","title":"<code>__init__(relationship_type, start_chunk=0)</code>","text":"<p>Initializes the RelationshipsCollectorProcessor with the relationship type, data file, and start chunk index.</p> <p>Parameters:</p> Name Type Description Default <code>relationship_type</code> <code>str</code> <p>The type of relationship to collect data for (e.g., 'Assay_Compound', 'Assay_Enzyme', 'Gene_Enzyme', 'Compound_Enzyme', 'Compound_Similarity', 'Compound_Cooccurrence', 'Compound_Transformation').</p> required <code>start_chunk</code> <code>int</code> <p>The starting chunk index for processing data.</p> <code>0</code> Source code in <code>chemgraphbuilder/relationship_collector_processor.py</code> <pre><code>def __init__(self, relationship_type, start_chunk=0):\n    \"\"\"\n    Initializes the RelationshipsCollectorProcessor with the relationship type, data file, and start chunk index.\n\n    Args:\n        relationship_type (str): The type of relationship to collect data for (e.g., 'Assay_Compound', 'Assay_Enzyme', 'Gene_Enzyme', 'Compound_Enzyme', 'Compound_Similarity', 'Compound_Cooccurrence', 'Compound_Transformation').\n        start_chunk (int): The starting chunk index for processing data.\n    \"\"\"\n    self.relationship_type = relationship_type\n    self.data_file = \"Data/AllDataConnected.csv\"\n    self.start_chunk = start_chunk\n    self.extractor = RelationshipPropertiesExtractor()\n    self.processor = RelationshipDataProcessor(path=\"Data/Relationships/Assay_Compound_Relationship\")\n</code></pre>"},{"location":"relationship_collector_processor/#chemgraphbuilder.relationship_collector_processor.RelationshipsCollectorProcessor.collect_relationship_data","title":"<code>collect_relationship_data()</code>","text":"<p>Collects and processes relationship data based on the relationship type and saves it to the appropriate file.</p> Source code in <code>chemgraphbuilder/relationship_collector_processor.py</code> <pre><code>def collect_relationship_data(self):\n    \"\"\"\n    Collects and processes relationship data based on the relationship type and saves it to the appropriate file.\n    \"\"\"\n    if self.relationship_type == 'Assay_Compound':\n        self.extractor.assay_compound_relationship(self.data_file, start_chunk=self.start_chunk)\n        self.processor.process_files()\n    elif self.relationship_type == 'Assay_Enzyme':\n        self.extractor.assay_enzyme_relationship(self.data_file)\n    elif self.relationship_type == 'Gene_Enzyme':\n        self.extractor.gene_enzyme_relationship(self.data_file)\n    elif self.relationship_type == 'Compound_Gene':\n        self.extractor.compound_gene_relationship(self.data_file)\n    elif self.relationship_type == 'Compound_Similarity':\n        self.extractor.compound_similarity_relationship(self.data_file, start_chunk=self.start_chunk)\n    elif self.relationship_type == 'Compound_Cooccurrence':\n        self.extractor.compound_cooccurrence(self.data_file, start_chunk=self.start_chunk)\n    elif self.relationship_type == 'Compound_Transformation':\n        self.extractor.compound_transformation(self.data_file)\n    else:\n        logging.error(f\"Unsupported relationship type: {self.relationship_type}\")\n</code></pre>"},{"location":"relationship_collector_processor/#chemgraphbuilder.relationship_collector_processor.main","title":"<code>main()</code>","text":"<p>Main function to parse command-line arguments and collect relationship data for the specified type.</p> Source code in <code>chemgraphbuilder/relationship_collector_processor.py</code> <pre><code>def main():\n    \"\"\"\n    Main function to parse command-line arguments and collect relationship data for the specified type.\n    \"\"\"\n    parser = argparse.ArgumentParser(description=\"Collect relationship data for different types of relationships.\")\n    parser.add_argument('--relationship_type',\n                        type=str,\n                        required=True,\n                        choices=['Assay_Compound', 'Assay_Enzyme', 'Gene_Enzyme',\n                                 'Compound_Gene', 'Compound_Similarity',\n                                 'Compound_Cooccurrence',\n                                 'Compound_Transformation'],\n                        help='The type of relationship to collect data for')\n    parser.add_argument('--start_chunk',\n                        type=int,\n                        default=0,\n                        help='The starting chunk index for processing data')\n\n    args = parser.parse_args()\n\n    collector = RelationshipsCollectorProcessor(relationship_type=args.relationship_type, start_chunk=args.start_chunk)\n    collector.collect_relationship_data()\n</code></pre>"}]}